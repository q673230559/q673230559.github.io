[{"title":"Markdown效果预览","url":"/2017_01_01_markdown_use/","content":"概要&amp;emsp;&amp;emsp;小书匠是一款专为markdown写作而设计的编辑器。\n//首行空两格&amp;emsp;&amp;emsp;\n\n主要功能\n专为markdown写作设计的文档编辑器，让用户心无旁骛的进行创作。//加粗**专为markdown写作设计的文档编辑器**\n多种编辑模式。单栏编辑，双栏编辑，三栏编辑，全屏写作，全屏阅读…想怎么切换，就怎么切换，就是这样随心所欲。\n多种编辑器实现。codemirror编辑器（提供vim,emacs按键，行专注等），ace编辑器（提供vim，emacs按键绑定，显示行号），轻量编辑器，CJK竖排编辑器\n多种主题选择。包括编辑器主题，预览区代码高亮主题，及预览区用户自定义css。\n丰富的语法支持。不仅提供了常用的commanmarkdown语法，还提供了许多有用的扩展语法，比如&#x3D;&#x3D;Latex公式&#x3D;&#x3D;，&#x3D;&#x3D;表格&#x3D;&#x3D;, &#x3D;&#x3D;目录&#x3D;&#x3D;， &#x3D;&#x3D;脚注&#x3D;&#x3D;, &#x3D;&#x3D;视频&#x3D;&#x3D;, &#x3D;&#x3D;音频&#x3D;&#x3D;, &#x3D;&#x3D;附件&#x3D;&#x3D;, &#x3D;&#x3D;checklist&#x3D;&#x3D;, &#x3D;&#x3D;流程图&#x3D;&#x3D;等。更多语法可查看&lt;小书匠语法使用手册&gt;\n代码块文字格式语法。语法可查看&lt;小书匠语法使用手册&gt;\n第三方同步。&#x3D;&#x3D;浏览器存储&#x3D;&#x3D;, &#x3D;&#x3D;本地文件系统存储&#x3D;&#x3D;, &#x3D;&#x3D;dropbox&#x3D;&#x3D;, &#x3D;&#x3D;evernote&#x3D;&#x3D;, &#x3D;&#x3D;印象笔记&#x3D;&#x3D;,&#x3D;&#x3D;有道笔记&#x3D;&#x3D;, &#x3D;&#x3D;为知笔记&#x3D;&#x3D;, &#x3D;&#x3D;github&#x3D;&#x3D;等多种存储方案，保证了用户数据的安全，也让用户在存储方案上有了更多的选择。\n支持evernote，印象笔记。提供双向操作，可以将文章保存到evernote&#x2F;印象笔记上，也可以从evernote&#x2F;印象笔记上导入数据。同时提供标签，附件，图片，待办等相关处理。\n强大的文件管理功能。文件信息，标签，附件，音频，视频，图片管理。\n发布功能。 支持将文章发布到博客平台上。\n邮件发送功能。\nsourceMap对照功能。方便在源markdown文件和生成的html文件上进行比较，特别适合markdown初学者使用，了解每一个markdown解析产生的结果，也适用于文章后期的校对上。\nppt。\nppt跨屏演示\npdf预览\ntypewriter scrolling\nautocomplete 和 snippets 功能\n\n离线版下载地址http://soft.xiaoshujiang.com\nWEB版访问地址http://markdown.xiaoshujiang.com\n元数据使用说明语法开关元数据项，可以到设置面板里的语法扩展标签页下查看对应的元数据标识．在元数据里true时，表示当前文档强制打开该语法，false时表示强制关闭该语法．如果没有对应的元数据，则使用全局设置里的语法开关.\npreview_previewType元数据，可用的值为normal和presentation．用于文章在打开时，控制是否需要系统切换对应的预览界面．如果文章里没有该项元数据，或者元数据值不正确，则系统默认使用normal预览界面．该元数据仅控制文章打开时初始化的界面，用户依然可以通过按钮在不同预览界面间切换．\n浏览器存储系统对创建的文章，都会在浏览器存储上进行保存。包括像evernote&#x2F;印象笔记&#x2F;github&#x2F;dropbox等导入的文章，也都会保存一份副本，并创建一个标识，表示跟哪些第三方存储关联。\n标题，标签文章标题的处理规则：如果文章内存在元数据title，则系统自动使用元数据内的title做为标题。如果文章未使用到元数据功能，用户可以通过维护文章信息按钮，修改标题。标签tags的规则也跟标题一样。\n附件文章使用./做为附件的引用标识。对于图片，音频，视频，附件等链接的处理，系统只处理以./开头的链接，并转换成附件真实的地址进行显示。用户可以通过工具栏的插入图片，插入音频，插入视频，插入附件等按钮上传附件。\nevernote&#x2F;印象笔记小书匠编辑器提供对evernote&#x2F;印象笔记的支持，下面的使用说明默认用户已经完成了evernote&#x2F;印象笔记的绑定操作，并将当前的工作平台切换到evernote&#x2F;印象笔记下。\n新建通过新建按钮后，创建的文章将自动关联到evernote&#x2F;印象笔记上(注:这里仅仅是在文章上创建一个关联的标识，只有当用户保存后，才能在服务器上查看到新的笔记)\n打开点击笔记，系统将自动把笔记导入，并将当前文章切换为导入的笔记内容。导入的文章自动与evernote&#x2F;印象笔记上的笔记关联，下次再点击该笔记时，将直接从浏览器存储上打开。用户可以通过切换存储平台浏览器存储，来删除该缓存的文件。导入的笔记如果本地没有缓存，系统将对服务器上的笔记进行判断，如果笔记是通过小书匠编辑器进行保存，并且文章在保存后没有被操作过，则系统自动使用保存时附带的markdown附件做为文章内容，重新导入。如果笔记已经被修改，或者笔记不是通过小书匠编辑器进行保存的，系统将自动将文章转换成markdown格式。\n保存对于新创建的文章，用户可以直接保存ctrl+s，系统将弹出一个选择笔记本的窗口，确认后，系统将保存当前文章到evernote&#x2F;印象笔记上。（在弹出窗口上选择笔记本时，如果用户选择了笔记，系统将覆盖该笔记）\n对于已经存在的文章，但还没有保存到evernote&#x2F;印象笔记，用户可以通过另存为ctrl+shift+s将当前文章保存到evernote&#x2F;印象笔记上。\n不管是保存，还是另存为，保存成功后，系统都将自动对当前文章与evernote&#x2F;印象笔记上的笔记进行关联。下次保存时ctrl+s系统将自动同步保存到evernote&#x2F;印象笔记上。\n删除系统不提供删除操作，用户需要自己到evernote&#x2F;印象笔记端删除，如果本地缓存了笔记，可以通过浏览器存储删除缓存。\n重命名直接修改元数据title，如果文章内未使用元数据功能，可通过浏览器存储里的修改文章信息进行修改\n标签管理系统自动通过每篇文章的元数据tags提取为笔记的标签。\n附件管理打开时，系统自动将笔记上的附件导入到文章对应的附件管理器上。保存时，系统将根据文章内对附件的引用，将附件保存到服务器上。这里的引用包括音频，视频，附件，图片。如果文章内使用到了流程图，序列图，公式，统计图等，系统将会把这些内容转换成图片进行保存。由于evernote&#x2F;印象笔记在部分终端不提供视频，音频的支持，查看保存的文章时，对应的音频，视频将以附件的形式存储。\n待办事项目前系统仅同步了待办事项。\ngithub&#x2F;dropbox新建参考evernote/印象笔记的新建\n打开参考evernote/印象笔记的打开不同的是，github&#x2F;dropbox只能打开扩展名为html，markdown，md，mkd以及无扩展名的文件。\n保存参考evernote/印象笔记的保存不同的是，github&#x2F;dropbox保存时，仅保存了markdown文章本身，并不会将markdown转换成html进行保存，也不会处理附件相关的内容。对于新文章的保存，github&#x2F;dropbox存储需要用户指定文件名及存储的位置。\n删除系统不提供删除操作\n重命名系统不提供重命名操作，只能通过另存为ctrl+shift+s，保存成新的文件。\n本地文件系统存储本地文件系统存储仅在离线版提供支持。\n新建参考evernote/印象笔记的新建\n打开参考github/dropbox的打开不同的是，本地文件系统存储在打开文件时，将会自动关联文章内的附件引用标识./，自动抓取同级目录下对应的附件资源。\n保存参考github/dropbox的保存不同的是，本地文件系统存储在保存时，不仅保存了markdown文章，还会处理附件相关的内容，将附件保存到同级目录下，请确保附件的名称不要重复，防止数据被覆盖丢失。\n删除右击相应的文章可进行删除操作\n重命名右击相应的文章可进行重命名操作\n发布小书匠编辑器离线版提供文章发布功能，用户可以将自己的文章发布到博客系统上。发布功能实现了博客的metaweblogAPI（newPost, editPost, newMediaObject）。使用该发布功能，需要博客系统提供对应的api接口，系统将转换成html的文章和图片自动提交到博客系统上。\n配置发布示例：博客链接地址：比如http://www.cnblogs.com/[用户名]/用户名：用户在该博客上的用户名密码：用户在该博客上的密码\n测试通过的博客地址：博客园：http://www.cnblogs.com/[用户名]/开源中国：http://my.oschina.net/[用户名]/blog\n邮件发送小书匠编辑器提供邮件发送功能，系统将对当前文章转换成html格式后进行发送，并对图片，视频等文件以附件的形式进行发送。\n导出小书匠编辑器提供多种格式的导出文件功能。&#x3D;&#x3D;html&#x3D;&#x3D;,&#x3D;&#x3D;markdown&#x3D;&#x3D;,&#x3D;&#x3D;html(inlinestyle)&#x3D;&#x3D;,&#x3D;&#x3D;word&#x3D;&#x3D;,&#x3D;&#x3D;zip&#x3D;&#x3D;,&#x3D;&#x3D;pdf&#x3D;&#x3D;。\nzip导出： 该导出将导出文章的所有信息，包括markdown,html,markdown文章内引用的所有附件，公式，流程图等对应的图片文件，以及方便再次导入时需要的标识数据文件。\npdf导出：目前pdf导出只能在chrome版浏览器上使用。\n导入小书匠编辑器提供markdown, html, zip三种导入功能，并且实现了文本文件直接拖动导入功能。\nzip导入：导入的zip文件必需是由小书匠编辑器导出的文件。\n其他web版实现了图片直接粘贴功能，用户不仅可以拖动图片上传，还可以直接复制粘贴图片。\n","categories":["其他资料"],"tags":["markdown","小书匠"]},{"title":"Youth(青春)","url":"/2017_01_06_youth/","content":"塞缪尔·厄儿曼既不是诗人，也不是作家，他一生就只写了这一首散文诗，而且是在他七十余岁高龄时写下来自勉的。后来偶然间由朋友不经意传出，立即受到了许多人喜爱，他们纷纷将其作为自己的人生格言，挂在墙上、放入衣兜、置入心灵深出，伴随自己一生，共同沐浴风雨阳光。塞缪尔·厄儿曼虽然已经去世快两个世纪了，但是他的这首《青春》仍然不减当年风采，依旧拥有震撼人心的力度，如诗如画的优美和深邃隽永的哲理。 青春是美好的，花样般的年华，白里透红的桃面，樱桃般的丹唇，弱柳扶风般的柔膝；勇敢的锐气，远大的理想，炽热的感情，青春是人一生中最美好的季节。\n青春塞缪尔\n青春不是年华，而是心境；青春不是桃面、丹唇、柔膝，而是深沉的意志，恢宏的想象，炙热的恋情；青春是生命的深泉在涌流。 \n青春气贯长虹，勇锐盖过怯弱，进取压倒苟安。如此锐气，二十后生而有之，六旬男子则更多见。年岁有加，并非垂老，理想丢弃，方堕暮年。 \n岁月悠悠，衰微只及肌肤；热忱抛却，颓废必致灵魂。忧烦，惶恐，丧失自信，定使心灵扭曲，意气如灰。 \n无论年届花甲，拟或二八芳龄，心中皆有生命之欢乐，奇迹之诱惑，孩童般天真久盛不衰。人人心中皆有一台天线，只要你从天上人间接受美好、希望、欢乐、勇气和力量的信号，你就青春永驻，风华常存。 \n一旦天线下降，锐气便被冰雪覆盖，玩世不恭、自暴自弃油然而生，即使年方二十，实已垂垂老矣；然则只要树起天线，捕捉乐观信号，你就有望在八十高龄告别尘寰时仍觉年轻。\nYOUTHSamuel Ullman \nYouth is not a time of life; it is a state of mind; it is not a matter of rosy cheeks, red lips and supple knees; it is a matter of the will, a quality of the imagination, a vigor of the emotions; it is the freshness of the deep springs of life. \nYouth means a tempera-mental predominance of courage over timidity, of the appetite for adventure over the love of ease. This often exists in a man of 60 more than a boy of 20.　Nobody grows old merely by a number of years.　We grow old by deserting our ideals. \nYears may wrinkle the skin, but to give up enthusiasm wrinkles the soul. Worry, fear, self-distrust bows the heart and turns the spring back to dust. \nWhether 60 or 16, there is in every human being’s heart the lure of wonder, the unfailing childlike appetite of what’s next and the joy of the game of living.　In the center of your heart and my heart there is a wireless station: so long as it receives messages of beauty, hope, cheer, courage and power from men and from the Infinite, so long are you young. \nWhen the aerials are down, and your spirit is covered with snows of cynicism and the ice of pessimism, then you are grown old, even at 20, but as long as your aerials are up, to catch waves of optimism, there is hope you may die young at 80.\n","categories":["哲学思考"],"tags":["励志","青春","塞缪尔","youth"]},{"title":"Markdown文件规范","url":"/2017_01_01_mdfile_standard/","content":"Markdown是一种可以使用普通文本编辑器编辑的标记语言，通过简单的标记语法，可以自动生成具有一定样式的文本。markdown语法简单明了，学习容易，而且功能比纯文本要强，因此很多人用他来写博客，比如WordPress和大型的CMS如Joomla、Drupal等都支持markdown。除此之外markdown还用于github中README.md用于编写说明文档。\n\n\n文件头格式规范---title: 诫子书date: 2017-04-15 16:20:00updated: 2017-04-15 16:20:00comments: falsetags: [励志]categories: 文言permalink: to_sondescription: xx---\ntitle\n文章标题\n\ndate\n文件编写日期\n\nupdated\n更新时间\n\ncomments\n是否开启评论true or false\n\ntags\ntomcat\nnginx\njava\nmarkdown\njs\najax\nhtml5\n前端\n青春\n励志\n读书\n激情\n…\n\ncategories\n文言\n美文\n诗歌\n文档\n技术\n摄影\n小说\n…\n\ndescription\n文章概述，如果不为空，则预览的时候显示后标签后面的内容（作用和正文中的分隔符作用差不多）\n\n正文格式规范&gt; &amp;emsp;&amp;emsp;其实非常喜欢这...# 标题一## 标题二### 标题三    XXXXXXXX## 标题二.1\n\n说明\n我们用引用符号&gt; 来规范文本格式，作为摘要部分，一般情况下，摘要要大于150个字，因为摘要的前150个字用于作为首页摘要简讯\n斜体*摘要：*注明摘要部分开始，这不是必需，而是规范。\n\n","categories":["其他资料"],"tags":["markdown"]},{"title":"以书为伴","url":"/2017_01_07_book/","content":"通常看一个读些什么书就可知道他的为人，就像看他同什么人交往就可知道他的为人一样，因为有人以人为伴，也有人以书为伴。无论是书友还是朋友，我们都应该以最好的为伴。\n好书就像是你最好的朋友。它始终不渝，过去如此，现在如此，将来也永远不变。它是最有耐心，最令人愉悦的伴侣。在我们穷愁潦倒，临危遭难时，它也不会抛弃我们，对我们总是一如既往地亲切。在我们年轻时，好书陶冶我们的性情，增长我们的知识；到我们年老时，它又给我们以慰藉和勉励。\n人们常常因为喜欢同一本书而结为知已，就像有时两个人因为敬慕同一个人而成为朋友一样。有句古谚说道：&quot;爱屋及屋。&quot;其实&quot;爱我及书&quot;这句话蕴涵更多的哲理。书是更为真诚而高尚的情谊纽带。人们可以通过共同喜爱的作家沟通思想，交流感情，彼此息息相通，并与自己喜欢的作家思想相通，情感相融。\n好书常如最精美的宝器，珍藏着人生的思想的精华，因为人生的境界主要就在于其思想的境界。因此，最好的书是金玉良言和崇高思想的宝库，这些良言和思想若铭记于心并多加珍视，就会成为我们忠实的伴侣和永恒的慰藉。\n书籍具有不朽的本质，是为人类努力创造的最为持久的成果。寺庙会倒坍，神像会朽烂，而书却经久长存。对于伟大的思想来说，时间是无关紧要的。多年前初次闪现于作者脑海的伟大思想今日依然清新如故。时间惟一的作用是淘汰不好的作品，因为只有真正的佳作才能经世长存。\n书籍介绍我们与最优秀的人为伍，使我们置身于历代伟人巨匠之间，如闻其声，如观其行，如见其人，同他们情感交融，悲喜与共，感同身受。我们觉得自己仿佛在作者所描绘的舞台上和他们一起粉墨登场。\n即使在人世间，伟大杰出的人物也永生不来。他们的精神被载入书册，传于四海。书是人生至今仍在聆听的智慧之声，永远充满着活力。\n","categories":["哲学思考"],"tags":["读书"]},{"title":"爱的深沉","url":"/2017_01_15_oak/","content":"第一篇：今天下午在言谈之中，忽然想起一首诗——舒婷的《致橡树》，致橡树 读后感。虽然这不是一首纯粹描述爱情的诗句，但是许多人却从中看出了爱情该有的态度。记得以前最喜欢其中的一句“我必须是你近旁的一株木棉，做为树的形象和你站在一起。”正是这种努力成长为树的信念，让我一度动容。自古以来，女性的形象似乎就被设定为了柔情似水、若柳扶风，长久以来受到压迫的旧社会的妇女也习惯了依附男人而生。拥有与失去似乎向来不是由女性向导，而只是被动地承受。\n也许真的存在那种耐心极好的男人，那就是极品了，不是你我轻易能够遇到的。男人的确喜欢温柔的女人，但是温柔不是软弱无力、不是悲泣啼哭，温柔的女人也可以拥有一个坚毅的灵魂。泪水也许会换来男人一时的柔情，但总怕终有一天，这个曾经给予你温暖的手也会因为疲倦而无力地垂下。我想我是不愿当这棵菟丝花的，那样只能努力地依附大树而生，贪婪地榨取大树的精力来得到生存，让大树的躯干无法得到伸展。我愿意用我的智慧来经营我的感情和生活，将自己塑造成一个温柔的女人和一个并肩的战友，拥有自己独特的和煦而坚定的微笑。当欣赏江山如画的时候，我愿意化为一汪春水，为这副风景增添一抹丽色;当遭遇风浪袭人的时候，我亦可以和你一起迎风而上 ，而不是委屈地退缩害怕、逃离。\n第二篇：《致橡树》是我非常喜欢的一首诗，记得上大学的时候，我把这首诗写在了日记里，对诗中的每一句都曾仔细的斟酌和品味，后来常常把它默记在心里。随着年龄的增长，对诗中所体现的那种至高无上的爱情更是感受至深。近几天，我在看央视播放的电视剧《相思树》，当这首诗被男女主人公深情朗诵的时候，让我又一次产生了强烈的思想共鸣，也让我想起了很多的往事，往事不可追，但却永远珍藏在内心深处，还有往事中的他伴我在记忆的空间里飞呀飞……\n致橡树舒婷我如果爱你——绝不像攀援的凌霄花，借你的高枝炫耀自己；我如果爱你——绝不学痴情的鸟儿，为绿荫重复单调的歌曲；也不止像泉源，常年送来清凉的慰藉；也不止像险峰，增加你的高度，衬托你的威仪。甚至日光，甚至春雨。不，这些都还不够！我必须是你近旁的一株木棉，作为树的形象和你站在一起。根，紧握在地下；叶，相触在云里。每一阵风过，我们都互相致意，但没有人，听懂我们的言语。你有你的铜枝铁干，像刀，像剑，也像戟；我有我红硕的花朵，像沉重的叹息，又像英勇的火炬。我们分担寒潮、风雷、霹雳；我们共享雾霭、流岚、虹霓。仿佛永远分离，却又终身相依。这才是伟大的爱情，坚贞就在这里：爱——不仅爱你伟岸的身躯，也爱你坚持的位置，足下的土地。\n","categories":["哲学思考"],"tags":["爱情","致橡树","舒婷","情书"]},{"title":"经典诫子书","url":"/2017_04_15_to_son/","content":"其实非常喜欢这篇文章，推荐给亲⊙o记得第一次读到这篇文章的时候在初中，当时很励志，以此鞭策自己，如今看来任然可以作为终身的信条O(∩_∩)O&#x2F;年轻一定要奋斗，严格要求自己，宁静致远，这样不会再年老的时候发出“悲守穷庐，将复何及”的感叹！而且要树立终身学习的观念，寻找适合自己的学习方法，每个人的学习能力存在差异可能不容易改变，但是时间的累积会让你获得意想不到的收获 ~ 朋友们加油吧。\n\n\n原文诸葛亮夫君子之行，静以修身，俭以养德。非淡泊无以明志，非宁静无以致远。夫学须静也，才须学也。非学无以广才，非志无以成学。淫慢则不能励精，险躁则不能治性。年与时驰，意与日去，遂成枯落，多不接世。悲守穷庐，将复何及？\n\n最喜欢“宁静致远”。\n\n创作背景这篇文章当作于蜀汉建兴十二年（元234年），是诸葛亮晚年写给他八岁的儿子诸葛瞻的一封家书。诸葛亮一生为国，鞠躬尽瘁，死而后已。他为了蜀汉国家事业日夜操劳，顾不上亲自教育儿子，于是写下这篇书信告诫诸葛瞻。\n作品鉴赏古代家训，大都浓缩了作者毕生的生活经历、人生体验和学术思想等方面内容，不仅他的子孙从中获益颇多，就是今人读来也大有可借鉴之处。三国时蜀汉丞相诸葛亮被后人誉为“智慧之化身”，他的《诫子书》也可谓是一篇充满智慧之语的家训，是古代家训中的名作。文章阐述修身养性、治学做人的深刻道理，读来发人深省。它也可以看作是诸葛亮对其一生的总结，后来更成为修身立志的名篇。\n《诫子书》的主旨是劝勉儿子勤学立志，修身养性要从淡泊宁静中下功夫，最忌怠惰险躁。文章概括了做人治学的经验，着重围绕一个“静”字加以论述，同时把失败归结为一个“躁”字，对比鲜明。\n在《诫子书》中，诸葛亮教育儿子，要“澹泊”自守，“宁静”自处，鼓励儿子勤学励志，从澹泊和宁静的自身修养上狠下功夫。他说，“夫学须静也，才须学也，非学无以广才，非志无以成学”。意思是说，不安定清静就不能为实现远大理想而长期刻苦学习，要学得真知必须使身心在宁静中研究探讨，人们的才能是从不断的学习中积累起来的；不下苦功学习就不能增长与发扬自己的才干；没有坚定不移的意志就不能使学业成功。《诸葛亮教育儿子切忌心浮气躁，举止荒唐。在书信的后半部分，他则以慈父的口吻谆谆教导儿子：少壮不努力，老大徒伤悲。这话看起来不过是老生常谈罢了，但它是慈父教诲儿子的，字字句句是心中真话，是他人生的总结，因而格外令人珍惜。\n这篇《诫子书》，不但讲明修身养性的途径和方法，也指明了立志与学习的关系；不但讲明了宁静淡泊的重要，也指明了放纵怠慢、偏激急躁的危害。诸葛亮不但在大的原则方面对其子严格要求，循循善诱，甚至在一些具体事情上也体现出对子女的细微关怀。在这篇《诫子书》中，有宁静的力量：“静以修身”，“非宁静无以致远”；有节俭的力量：“俭以养德”；有超脱的力量：“非澹泊无以明志”；有好学的力量：“夫学须静也，才须学也”；有励志的力量：“非学无以广才，非志无以成学”；有速度的力量：“淫慢则不能励精”；有性格的力量：“险躁则不能治性”；有惜时的力量：“年与时驰，意与岁去”；有想象的力量：“遂成枯落，多不接世，悲守穷庐，将复何及”；有简约的力量。这篇文章短短几十字，传递出的讯息，比起长篇大论，诫子效果好得多。\n文章短小精悍，言简意赅，文字清新雅致，不事雕琢，说理平易近人，这些都是这篇文章的特出之处。\n","categories":["哲学思考"],"tags":["诫子书"]},{"title":"阿里巴巴Java开发手册","url":"/2017_02_16_java_help_doc/","content":"2017年开春之际，诚意献上重磅大礼：阿里巴巴Java开发手册，首次公开阿里官方Java代码规范标准。这套Java统一规范标准将有助于提高行业编码规范化水平，帮助行业人员提高开发质量和效率、大大降低代码维护成本。你是否曾因Java代码规范版本纷杂而无所适从？\n你是否想过代码规范能将系统故障率降低20%？\n你是否曾因团队代码风格迥异而协同困难？\n你是否正在review一些原本可以避免的故障？\n\n\n你是否无法确定自己的代码足够健壮？ \n码出高效，码出质量！相比C++代码规范业界已经达成共识，Java代码规范业界比较混乱，我们期待这次发布的Java代码规范能够给业界带来一个标准，促使整体行业代码规范水平得到提高，最终能够帮助企业和开发者提升代码质量和降低代码故障率。\n阿里出品，质量保证！阿里Java技术团队一手打造出Dubbo、JStorm、Fastjson等诸多流行开源框架，部分已成为Apache基金会孵化项目；\n阿里在Java后端领域支撑起全球访问量最大的服务器集群；\nJava代码构建的阿里双11业务系统订单处理能力达到17.5万笔&#x2F;秒；\n到目前已累计数亿行高并发、高稳定性的最佳Java代码实践；\n……\n此次首度公开的Java开发手册正是出自这样的团队，近万名阿里Java技术精英的经验总结，并经历了多次大规模一线实战检验及完善，铸就了这本高含金量的阿里Java开发手册。该手册以Java开发者为中心视角，划分为编程规约、异常日志规约、MYSQL规约、工程规约、安全规约五大块，再根据内容特征，细分成若干二级子目录。根据约束力强弱和故障敏感性，规约依次分为强制、推荐、参考三大类。此套规范不仅能让代码一目了然， 更有助于加强团队分工与合作、真正提升效率。 \n无规矩不成方圆 无规范不能协作众所周知，制订交通法规表面上是要限制行车权，实际上是保障公众的人身安全。试想如果没有限速，没有红绿灯，没有规定靠右行驶，谁还敢上路行驶。 \n同理，对软件来说，适当的规范和标准绝不是消灭代码内容的创造性、优雅性，而是限制过度个性化，以一种普遍认可的方式一起做事，降低故障率，提升协作效率。开发手册详细列举如何开发更加高效，更加容错，更加有协作性，力求知其然，更知其不然，结合正反例，提高代码质量。比如，异常日志处理时的各种不规范行为；集合转换的各种坑；创建线程池出现的等待队列OOM等。 \n阿里技术资深大咖联袂推荐阿里高级研究员多隆：工程师对于代码，一定要“精益求精”，不论从性能，还是简洁优雅，都要具备“精益求精”的工匠精神，认真打磨自己的作品。 \n阿里研究员毕玄：一个优秀的工程师和一个普通工程师的区别，不是现在满天飞的架构图，他的功底就是体现在他写的每一行代码上。 \n阿里研究员玄难：代码是软件工程里面的产品设计、系统架构设计等工作的最后承载体，代码的质量决定了一切工作的成败。 \n阿里巴巴B2B事业群CTO李纯：好的软件产品离不开工程师高质量的代码及相互间顺畅的沟通与合作。简单，适用的代码规约背后所传递的是技术上的追求卓越、协同合作的精神，是每个技术团队不可缺失的重要利器。 \n阿里研究员、HipHop作者：赵海平（花名：福贝）：程序员是创造个性化作品的艺术家，但同时也是需要团队合作的工种。个性化应尽量表现在代码效率和算法方面，牺牲小我，成就大我。 \n拥抱规范，远离伤害！\n开发的同学们赶紧行动起来，遵守代码规范，你好，我好，大家好！ \n下载链接： http://pan.baidu.com/s/1o8bLJSm 密码: w9vw\n","categories":["开发手册"],"tags":["java","阿里"]},{"title":"誓刷红楼，品味经典","url":"/2017_05_11_dream__of_red/","content":"最近刷了一遍红楼，开始的目的是想借此了解一下清朝人们的生活状态，感受一下大观园富丽堂皇的生活，当然，也借此去找寻一下生活的意义；然而，迷上了红楼的文字艺术，辞藻的华丽，让人惊叹。即使是每一个丫头，佣人的名字艺术感也是非常强烈的，因是，在想，以后给某某取名，应多借鉴红楼。另外，关于清代人们的价值观，思想状态，也是有很多感受的，在这儿，就不细说，先列出优美的句子。\n\n\n品读佳句\n你证我证，心证意证。是无有证，斯可云证。无可云证，是立足境。无立足境，是方干净。\n\n彼此都想从对方得到感情的印证而频添烦恼；看来只有到了灭绝情谊，无需再验证时，方谈得上感情上的彻悟；到了万境归空，什么都无可验证之时，才是真正的立足之境。后一句是黛玉加的。证，在佛教用语中是领受、领悟道法之意，这里第一句中的证可以解释为印证。主要讲的是他们之间的感情纠葛。意谓彼此都想从对方心灵和表情达意中印证相互之间的感情；当然我们如果纯以禅理的角度讲可以理解为参禅的第一步。第二句“无有证”即无证，意谓无求于身外，不要证验，才谈得上参悟禅机，证得上层。第三句意谓到万境归空无证验可言时，才算找到了禅宗的境界。黛玉所续之句的意思是连禅境都放弃了才算是最彻底的。简单地说宝玉最终追求的是精神境界，而黛玉却连精神境界也一并不要了，从而达到空无纯明之境，“心体亦空，万缘俱寂”。这正好与禅宗三境界相对应。   \n\n\n菩提本无树，明镜亦非台。本来无一物，何处若尘埃。  \n\n假作真时真亦假，无为有处有还无。\n\n《太虚幻境》   \n\n\n世事洞明皆学问，人情练达即文章。\n\n处事哲学乎？  \n\n\n莫失莫忘，仙寿恒昌。不离不弃，芳龄永继。\n\n“莫失莫忘,仙寿恒昌”是_贾宝玉佩带的通灵宝玉上的字,“不离不弃,芳龄永继“是薛宝钗佩带的金锁;简言“莫失莫忘，不离不弃”。  一块玉一块金，此之谓“金玉良缘”。\n\n\n满纸荒唐言，一把辛酸泪。都云作者痴，谁解其中味？\n\n如花美眷，怎敌似水流年。\n\n花谢花飞花满天，红消香断有谁怜？游丝软系飘春榭，落絮轻沾扑绣帘。闺中女儿惜春暮，愁绪满怀无释处。手把花锄出绣帘，忍踏落花来复去？柳丝榆荚自芳菲，不管桃飘与李飞。桃李明年能再发，明年闺中知有谁？三月香巢已垒成，梁间燕子太无情！明年花发虽可啄，却不道人去梁空巢也倾。一年三百六十日，风刀霜剑严相逼。明媚鲜妍能几时？一朝飘泊难寻觅。花开易见落难寻，阶前闷杀葬花人。独倚花锄泪暗洒，洒上空枝见血痕。杜鹃无语正黄昏，荷锄归去掩重门。青灯照壁人初睡，冷雨敲窗被未温。怪奴底事倍伤神？半为怜春半恼春。怜春忽至恼忽去，至又无言去不闻。昨宵庭外悲歌发，知是花魂与鸟魂。花魂鸟魂总难留，鸟自无言花自羞。愿依胁下生双翼，随花飞到天尽头。天尽头,何处有香丘?未若锦囊收艳骨，一杯净土掩风流。 质本洁来还洁去，强于污淖陷渠沟。尔今死去依收葬，未卜依身何日丧? 依今葬花人笑痴，他年葬依知是谁?试看春残花渐落，便是红颜老死时。 一朝春尽红颜老，花落人亡两不知!  \n\n这是黛玉的《葬花吟》。最有名的我想是开头的“花谢花飞飞满天，红消香断有谁怜”跟结尾处“一朝春尽红颜老，花落人亡两不知”两句；翻译几句：花开易见落难寻，阶前闷杀葬花人。花开时容易被人发现，花落时却难寻觅，这真让我这个葬花人发愁啊!侬今葬花人笑痴，他年葬侬知是谁? 我今天葬花别人笑我傻，等我死时却又不知谁来埋葬我呢?试看春残花渐落，便是红颜老死时。看那春天将尽花也渐渐飘落，就是如花红颜衰老将死之时。一朝春尽红颜老，花落人亡两不知！等到哪一天春天彻底逝去，美人迟暮之后，花也落尽，人也去世，两边再无相知。  \n\n\n寒塘渡鹤影，冷月葬花魂。\n\n对仗炒鸡工整。  \n\n\n春恨秋悲皆自惹，花容月貌为谁妍。  \n\n滴不尽相思血泪抛红豆 ，开不完春柳春花满画楼，睡不稳纱窗风雨黄昏后 ，忘不了新愁与旧愁。\n\n厚地高天，堪叹古今情不尽；痴男怨女，可怜风月债难偿。\n\n太高人愈妒，过洁世同嫌。\n\n世人都晓神仙好，只有功名忘不了！古今将相在何方，荒冢一堆草没了！世人都晓神仙好，只有金银忘不了！终朝只恨聚无多，及到多时眼闭了！世人都晓神仙好，只有娇妻忘不了！君生日日说恩情，君死又随人去了！世人都晓神仙好，只有儿孙忘不了！痴心父母古来多，孝顺子孙谁见了！\n\n 《好了歌》诗歌内容隐射小说情节，表达了作者对现实的愤懑和失望。  \n\n\n茜纱窗下，公子无缘。黄土垅中，卿何薄命。\n\n意淫二字，惟心会而不可口传，可神通而不可语达。\n\n任凭弱水三千，我只取一瓢饮。\n\n弱水三千只取一瓢饮。\n\n\n纵然生得好皮囊，腹内原来草莽。\n\n这原始初中时候最喜欢的的一句，嘲讽下某某人\n\n\n半卷湘帘半掩门，碾冰为土玉为盆。偷来梨蕊三分白，借得梅花一缕魂。月窟仙人缝缟袂，秋闺怨女拭啼痕。娇羞默默同谁诉，倦倚西风夜已昏。\n\n女儿悲，青春已大守空闺。女儿愁，悔教夫婿觅封侯。女儿喜，对镜晨妆颜色美。女儿乐，秋千架上春衫薄。\n\n茶靡花开，末路之美，开到荼蘼花事了，尘烟过，知多少? \n\n悲伤的茶靡花–开到荼蘼花事了。  \n\n\n偶因一回顾，便为心上人。  \n\n瘦影自临春水照，卿须怜我我怜卿。\n\n孤芳自赏，憔悴的黛玉…\n\n\n\n人物别号\n林黛玉——潇湘妃子\n\n薛宝钗——蘅芜君\n\n李 纨——稻香老农\n\n史湘云——枕霞旧友\n\n贾宝玉——怡红公子\n\n贾探春——蕉下客\n\n贾迎春——菱洲\n\n贾惜春——藕榭\n\n\n人物名称\n十二金钗：林黛玉、薛宝钗、贾元春、贾迎春、贾探春、贾惜春、李纨、妙玉、史湘云、王熙凤、贾巧姐、秦可卿。 　　\n\n十二丫环：晴雯、麝月、袭人、鸳鸯、雪雁、紫鹃、碧痕、平儿、香菱、金钏、司棋、抱琴。  \n\n十二家人：赖大、焦大、王善保、周瑞、林之孝、乌进孝、包勇、吴贵、吴新登、邓好时、王柱儿、余信。   \n\n十二儿：庆儿、昭儿、兴儿、隆儿、坠儿、喜儿、寿儿、丰儿、住儿、小舍儿、李十儿、玉柱儿。  \n\n十二贾氏：贾敬、贾赦、贾政、贾宝玉、贾琏、贾珍、贾环、贾蓉、贾兰、贾芸、贾蔷、贾芹。   \n\n十二官：琪官、芳官、藕官、蕊官、药官、玉官、宝官、龄官、茄官、艾官、豆官、葵官。   \n\n七尼：妙玉、智能、智通、智善、圆信、大色空、净虚。  \n\n七彩：彩屏、彩儿、彩凤、彩霞、彩鸾、彩明、彩云。  \n\n四春：贾元春、贾迎春、贾探春、贾惜春。  \n\n四宝：贾宝玉、甄宝玉、薛宝钗、薛宝琴。  \n\n四薛：薛蟠、薛蝌、薛宝钗、薛宝琴。 　 \n\n四王：王夫人、王熙凤、王子腾、王仁。 　\n\n四尤：尤老娘、尤氏、尤二姐、尤三姐。   \n\n四草辈：贾蓉、贾兰、贾芸、贾芹。  \n\n四玉辈：贾珍、贾琏、贾环、贾瑞。 　\n\n四文辈：贾敬、贾赦、贾政、贾敏 。　\n\n四代辈：贾代儒、贾代化、贾代修、贾代善。 　\n\n四烈婢：晴雯、金钏、鸳鸯、司棋。 　\n\n四清客：詹光、单聘仁、程日兴、王作梅。 　\n\n四无辜：石呆子、张华、冯渊、张金哥。 　\n\n四小厮：茗烟、扫红、锄药、伴鹤。 　 \n\n四小：小鹊、小红、小蝉、小舍儿。 　　\n\n四婆子：刘姥姥、马道婆、宋嬷嬷、张妈妈。 　 \n\n四情友：秦锺、蒋玉菡、柳湘莲、东平王。 \n\n四壮客：乌进孝、冷子兴、山子野、方椿。  \n\n四宦官：载权、夏秉忠、周太监、裘世安。  \n\n文房四宝：抱琴、司棋、侍画、入画。  \n\n四珍宝：珍珠、琥珀、玻璃、翡翠。  \n\n一主三仆：史湘云–翠缕、笑儿、篆儿。贾探春–侍画、翠墨、小蝉。贾宝玉–茗烟、袭人、晴雯。林黛玉–紫鹃、雪雁、春纤。贾惜春–入画、彩屏、彩儿。贾迎春–彩凤、彩云、彩霞。\n\n\n","categories":["读书笔记"],"tags":["辞藻","佳句","红楼梦","红楼"]},{"title":"遥远的救世主","url":"/2017_05_12_distant_savior/","content":"不知道是谁推荐过《遥远的救世主》，也不知怎么就把它和《天道》相连，以为是两本书，总之就是记住了。后来知道《天道》是电视剧，众人捧的热烈，改编自这本书。很偶然的机会，图书馆里看到了它，就借来一观。五十万字，不是随意打发下午时光就可看完的。小说里佛学的光辉、哲学的深奥与“禅”的玄机不谈，“文化属性”也不说，这不是我如今的知识层面所能评判的，仅能在思想上被“灌输”而已，然后再顺便感慨几句晦涩难懂，大呼看不明白，作者有才。废话不多说，列些句子，慢慢品味。\n\n品读佳句\n一颗阴暗的心，永远托不起一张灿烂的脸。  \n\n人从根本上是面对两个问题，一，生存，得活下来；二，是要回答生命价值的问题，让心有个安处。\n\n有招有术的感情，招术里是什么不去论它了，没招没术的感情，剩下的就该是造物主给的那颗心了。  \n\n命题错误，答既有错；只要是需要证明的感情就有错误。\n\n\n当人一旦从危险里跳出来，他就不再去关注这个事物的危险了，他的目光就会全部落在这个事物的利益上。  \n\n神即道，道法自然，如来。  \n\n神就是道，道就是规律，规律如来，容不得你思议，按规律办事的人就是神。\n\n\n女人是形式逻辑的典范，是辩证逻辑的障碍，我无意摧残女人，也不想被女人摧残。  \n\n女人和男人的对话方式只有两个，要么躺着，要么站着。\n\n进了窄门，神立刻就会告诉你：我是不存在的，神就是你自己。但是，证到如此也并不究竟，神是什么?神即道，道法自然，如来。  \n\n文明对于不能以人字来界定的人无能为力。  \n\n所谓的神话竟是这么简单。原来能做到实事求是就是神话！原来能说老实话，能办老实事的人就是神！  \n\n衡量一种文化属性不是看它积淀的时间长短，而是看它与客观规律的距离远近。  \n\n一、天上掉馅饼的神话，实惠、破格，是为市井文化。二、最不道德的道德，明辨是非，是为哲人文化。三、不打碎点东西，不足以缘起主题，大智大爱，是为英雄文化。  \n\n无论做什么，市场都不是一块无限大的蛋糕。神话的实质就是强力作用的杀富济贫，这就可能产生两个问题，一是杀富是不是破坏性开采市场资源?二是让井底的人扒着井沿看了一眼再掉下去是不是让他患上精神绝症?  \n\n这就是圆融世故，不显山不露水，各得其所。可品性这东西，今天缺个角、明天裂道缝，也就离塌陷不远了。  \n\n生存法则很简单，就是忍人所不忍，能人所不能。忍是一条线，能是一条线，两者的间距就是生存机会。  \n\n这东西有点像禅，知之为不知，不知更非知。  \n\n强势文化就是遵循事物规律的文化，弱势文化就是依赖强者的道德期望破格获取的文化，也是期望救主的文化。强势文化在武学上被称为“秘笈”，而弱势文化由于易学、易懂、易用，成了流行品种。  \n\n比如说文化产业，文学、影视是扒拉灵魂的艺术，如果文学、影视的创作能破解更高思维空间的文化密码，那么它的功效就是启迪人的觉悟、震撼人的灵魂，这就是众生所需，就是功德、市场、名利，精神拯救的暴利与毒品麻醉的暴利完全等值，而且不必像贩毒那样耍花招，没有心理成本和法律风险。  \n\n股票的暴利并不产生于生产经营，而是产生于股票市场本身的投机性。它的运作动力是：把你口袋里的钱装到我口袋里去。它的规则是：把大多数羊的肉填到极少数狼的嘴里。私募基金是从狼嘴里夹肉，这就要求你得比狼更黑更狠，但是心理成本也更高，而且又多了一重股市之外的风险。所以，得适可而止。  \n\n《圣经》的教义如果不能经受逻辑学的检验，可能在实践上就会存在障碍。如果经受了逻辑学的检验，那表明神的思维即是人的思维，就会否定神性。换一种说法，神性如果附加上人性的期望值，神性就打了折扣。然而神性如果失去了人性的期望值，那么人还需要神吗?  \n\n基督教相信，太高的道德平台需要太高的教育、太深的觉悟和太复杂的炼造过程，是一道靠人性本能很难迈进的窄门。于是，基督教便有了神与人的约，有了神的关于天国与火湖、永生与死亡的应许，让凡夫俗子因为恐惧死亡和向往天堂而守约。这是智与善的魔术，非读懂的人不能理解。但《圣经》告诉世人了，要进窄门。  \n\n说魔说鬼都是个表述，本质是思维逻辑和价值观与普通人不同，所谓的地域之门也无非是价值观冲突所带来的精神痛苦。如果你是觉者，我尊敬你，向你学习，如果你是魔鬼，我鉴别你，弃你而去。  \n\n即便是呼之欲出，你也讲不出，因为一说就错，这就像法律不能单纯以推理定罪，得允许在可能与事实之间存续一个演化的过程。  \n\n红颜知己自古有之，这还得看男人是不是一杯好酒，自古又有几个男人能把自己酿到淡而又淡的名贵?这不适为之而可为的事情。  \n\n我把一个女人所能及的事都做了，包括我的廉耻和可能被你认为的淫荡，以后我就不遗憾了。  \n\n你是一块玉，但我不是匠人，你要求的，是一种雄性文化的魂，我不能因为你没说出来而装不知道。接受你，就接受了一种高度，我没有这个自信。  \n\n你让我用灵魂而不是文字去理解一个女人的圣洁。你这样做，是基于一种对应的人格，谢谢你能这样评价我。  \n\n你是那么的执著于孤独吗?我就眼看着让你走了，可心在问我，那我又该怎么去疼你?  \n\n所有的幸福、快乐、委屈，在这一刻都找到了接纳的地方。  \n\n顿悟天堂地狱的分别无二，证到极乐了。  \n\n视社会依次有三个层面：技术、制度和文化。小到一个人，大到一个国家一个民族，任何一种命运归根到底都是那种文化属性的产物。强势文化造就强者，弱势文化造就弱者，这是规律，也可以理解为天道，不以人的意志为转移。  \n\n俄罗斯是一个伟大的名族，历史上没有什么人能够战胜他们，但是再世界两大阵营五十多年意识形态的对抗里，他们却输在了他们还没有完全读懂的文化里，而美国，尊重客观规律的文化，赢得了靠飞机大炮赢得不了的胜利，以至于连联合国都成了一个失宠的王妃；在中国，有人动不动就拿民主指责共产党，但是他们根本就不知道，中国的政治文化也是传统文化的牺牲品，把沉积了几千年文化属性问题都记在一个只有几十年的政党的账上，这不公平，也不是真是的国情。\n\n丁对小丹说：不管是文化艺术还是生存艺术，有道无术，术尚可求也，有术无道，止于术；你的前途在于众生，众生没有真理真相，只有好恶，所以你就有了价值。觉悟天道，是名开天眼，你缺的就是这双眼睛，你需要的也是这双天眼，是一双剥离了政治，文化，传统，道德，宗教之分别的眼睛。然后再如实关照政治，文化，传统把文化道德颠倒了的真理真相再颠倒回来，不管随便你怎么写怎么拍，都是新意，深度，这就是钱，就是名利，成就，价值。\n\n丁为晓丹创造了一个神话，揭示了文化属性决定命运，使晓丹觉悟。\n\n\n\n资源共享小说下载链接： http://pan.baidu.com/s/1cmyvU2 密码：9620  \n电视剧《天道》： http://pan.baidu.com/s/1sloBCXf 密码：s0az\n","categories":["读书笔记"],"tags":["豆豆","遥远的救世主"]},{"title":"教育知识与能力知识总结","url":"/2018_07_01_educational_knowledge_and_ability/","content":"大纲-思维导图\n第一章 教育基础知识和基本原理1.生产力与教育的关系。⁕2.20世纪以后教育的特点。⁕3.个体身心发展的一般规律。⁕4.影响人身心发展的因素。⁕5.教育制度确立的依据。⁕6.确立我国教育目的的依据。⁕7.全面发展教育的构成及作用。⁕8.政治经济制度与教育的关系。⁕9.学校教育在人的身心发展中起主导作用的原因。10.现代教育制度的发展趋势。第二章 中学课程1.课程内容的文本表现形式。⁕2.课程改革的结构。⁕3.新课程改革背景下的评价观。⁕4.活动中心课程理论的观点&#x2F;简述活动课程的特点。5.学科中心课程论的观点。6.我国新一轮基础教育课程改革的具体目标有哪些？7.教材（教科书）编写的基本要求&#x2F;原则。8.综合时间活动课程的主要内容。第三章 中学教学1.我国现阶段的教学任务。⁕2.教学过程的结构&#x2F;基本阶段。⁕3.直观性教学原则的含义及贯彻要求。⁕4.循序渐进教学原则的含义及贯彻要求。⁕5.讲授法的概念和要求。⁕6.理论联系实际教学原则的含义及贯彻要求。⁕7.教学方法选用的依据。⁕8.教师备课的基本要求。9.教学过程的基本规律。10.班级授课制的优缺点。11.教学过程是一种特殊的认识过程。12.贯彻科学性和思想性相统一教学原则的基本要求。第四章 中学生学习心理1.如何培养中学生的注意力。⁕2.影响问题解决的主要因素。⁕3.影响遗忘进程的因素。⁕4.学生学习的特点。⁕5.有意义学习的实质及条件。⁕6.学习动机与学习效率的关系。⁕7.如何激发学生的学习动机。⁕8.如何培养学生的学习动机。⁕9.建构主义学习理论&#x2F;建构主义的知识观、学生观、学习观。10.有效促进学习迁移的教学。11.如何在教学中应用注意的规律。12.培养学生想象力的方法。13.提高问题解决能力的教学&#x2F;学生问题解决能力的培养。14.影响迁移的主要因素。15.操作（动作）技能的培养要素。16.心智技能的培养要求。17.学习策略的分类。18.中学生记忆发展的特点。19.马斯洛需求层理理论。20.加涅的学习结果分类。21.创造性思维&#x2F;发散性思维的特征。22.知觉的基本特征。23.短时记忆的特点。24.自我效能感理论及其功能。25.学习动机的定义和功能。第五章 中学生发展心理1.最近发展区及启示。⁕2.维果斯基的心理发展理论。⁕3.不通气质类型的教育措施。⁕4.影响人格形成的影响因素。⁕5.中学生情绪的特点和指导学生有效调节方法。⁕6.如何培养学生的能力。7.影响学生能力形成的因素。第六章 中学生心里辅导1.学校开展心理健康教育的途径。⁕2.简述学校心理辅导的原则。⁕第七章 中学德育1.促进中学生形成良好平的的方法。⁕2.德语过程的基本规律。⁕3.贯彻疏导原则的要求。⁕4.尊重与严格要求原则的贯彻要求。⁕5.知行统一原则的贯彻要求。⁕6.中学德育的途径。⁕7.皮亚杰道德发展理论阶段。⁕8.科尔伯格的道德发展理论。⁕9.榜样示范法的贯彻要求。⁕10.依靠积极与克服消极因素相结合原则的贯彻要求&#x2F;长善救失原则。11.说服教育法的要求。12.影响品德发展的因素。第八章 中学班级管理与教师心理1.班集体的发展阶段。⁕2.个别教育工作。⁕3.教师成长与发展的基本途径。⁕4.班主任工作的基本内容。5.班主任应具备的基本条件。6.建立教师的威信&#x2F;建立教师威信的途径。","categories":["读书笔记"],"tags":["教师资格","教育基础知识","教育原理"]},{"title":"禅宗历史和经典偈语","url":"/2018_07_10_buddhism/","content":"禅宗的起源禅宗是佛教的一个重要宗派，起源于中国。禅宗的核心思想是通过禅修实现觉醒，这种禅修通常包括坐禅、行禅、打坐等方式。禅宗强调直接体验，而不是通过文字或逻辑推理来理解佛法。\n禅宗的历史禅宗的历史可以追溯到公元前6世纪的印度。当时，佛陀创立了佛教，其中包括了禅修的方法。禅修是指通过冥想和内省来实现觉醒和解脱的修行方法。佛陀的禅修方法在中国得到了广泛传播，形成了禅宗。\n禅宗最初在中国南北朝时期（420-581年）得到了发展。在这个时期，禅宗的代表人物是南宗祖师慧思和北宗祖师神秀。他们都致力于将禅宗的思想和修行方法传播到更广泛的人群中。\n在唐朝（618-907年）时期，禅宗得到了更广泛的发展。禅宗的代表人物有慧能、法眼、道信、神秀等。他们的教诲和著作对禅宗的发展产生了深远的影响。慧能是禅宗中最为著名的人物之一，他的《坛经》和《入门四论》等著作被誉为禅宗经典之一。\n在五代十国时期（907-979年），禅宗进一步得到了发展。五代时期的禅宗代表人物有临济宗祖六祖惠能、曹洞宗祖道元等。临济宗和曹洞宗是禅宗的两个主要流派，它们都对后来的禅宗发展产生了深远的影响。\n在宋朝（960-1279年）时期，禅宗得到了更广泛的发展。在这个时期，禅宗的代表人物如智顗、大愚、道原等都取得了很大的成就。智顗是禅宗中最为杰出的人物之一，他的《华严经》和《法华经》等著作被誉为佛教经典之一。\n禅宗在中国的发展经历了多个阶段。每个阶段都有不同的代表人物和特点。这些人物和特点对禅宗的发展产生了深远的影响，成为禅宗文化的重要组成部分。\n禅宗的经典禅宗的经典文献禅宗的经典文献主要包括以下几部分：\n\n《楞严经》《楞严经》是禅宗最重要的经典之一，它强调了空性和缘起的理论，提出了“一切法无我”的观念，是禅宗中重要的参禅经典。\n《心经》《心经》是禅宗中最短、最精要的经典之一，它强调了空性和缘起的理论，提出了“般若波罗蜜多心经”的观念，是禅宗中重要的参禅经典。\n《华严经》《华严经》是禅宗中最为广泛流传的经典之一，它强调了诸法的互相依存和缘起的理论，提出了“一切法皆如梦幻泡影”的观念，是禅宗中重要的参禅经典。\n《法华经》《法华经》是禅宗中最为广泛流传的经典之一，它强调了佛性和菩萨道的理论，提出了“一切众生皆有佛性”的观念，是禅宗中重要的参禅经典。\n《坛经》《坛经》是禅宗中的经典之一，它强调了禅宗的实践方法和境界，提出了“见性成佛”的观念，是禅宗中重要的参禅经典。\n《金刚经》全称《金刚般若波罗蜜经》，一卷，印度大乘佛教般若系经典，后秦鸠摩罗什译。\n\n禅宗的经典文献涵盖了空性、缘起、佛性、菩萨道、禅修方法和境界等方面的内容，是禅宗行者进行参禅修行和领悟佛法的重要依据。\n禅宗顿悟，不立文字，教外别传，直指人心，见性成佛，禅宗有三部必看经文，第一部《金刚经》，第二部《坛经》，第三部《心经》。看了这三部经文之后，就会产生个人的观点与理解。这些都只是个人的主观看法，不是悟。\n禅宗的经典偈语禅宗的经典偈语是禅修者在修行中常用的语句，它们简短而富有哲理和启示性，可以帮助行者深入领悟佛法的精髓。\n下面是一些常见的禅宗经典偈语：\n\n身心是道场，念念不二。——出自《坛经》，强调禅修的重点在于身心的净化，要把身心当作道场，不断净化自己的念头。\n不立文字，教外别传。——出自《南宗顿教正脉纲要》，意味着禅宗强调的是直接体验，而非依靠文字传授。\n不思议境界，无所得而成佛。——出自《华严经》，表明了禅修的目的在于超越思维，放下执着，才能成佛。\n一花一世界，一叶一菩提。——出自《金刚经》，表明了万物皆有佛性，只要发掘自己内在的潜力，就能成佛。\n直指人心，见性成佛。——出自《法华经》，强调了禅修的核心在于直接指向人心，通过觉悟而成佛。\n不立文字，教外别传，直指人心，见性成佛。——出自《南宗顿教正脉纲要》，是禅宗的三句箴言，强调了禅修的重点和目的。\n万法归一，一念清净。——出自《楞严经》，表明了禅修的目的在于归一万法，通过一念清净而达到解脱。\n身心清净，自然成佛。——出自《法华经》，意味着禅修的目的在于净化身心，达到自然成佛的境界。\n三世诸佛，皆在此中。——出自《华严经》，表明了禅修的重点在于当下，只有当下的觉悟才能成就佛道。\n一念不生，万法无生。——出自《华严经》，表明了禅修的目的在于超越生死轮回，达到无生的境界。\n大道无门，千般万般皆是道。——出自《信心骨髓》，意味着禅修的道路是没有门槛的，只要心存正念，万事万物都是道。\n佛法本无文字，因心而立言语。——出自《华严经》，表明了禅宗强调的是直接体验，禅修的经验需要通过语言表达出来。\n禅宗不立文字，直指人心。——出自《南宗顿教正脉纲要》，表明了禅宗强调的是直接体验，禅修的经验需要通过直接指向人心来实现。\n无心即是道，无物即是禅。——出自《法华经》，表明了禅修的目的在于超越心物二元对立，达到无心无物的境界。\n一念贪嗔痴，万劫不复生。——出自《华严经》，表明了禅修的目的在于放下执着，达到无念无执的境界。\n一切法门，皆由心起。——出自《法华经》，强调了心的作用，禅修的关键在于觉察自己的心念。\n一念之差，天堂地狱。——出自《楞严经》，表明了禅修的重要性，一个念头的转变可以影响一个人的命运。\n禅宗无门，法外传灯。——出自《南宗顿教正脉纲要》，强调了禅修的无门无派，只要有心，就能得到法眼传承。\n心外无法，法外无心。——出自《坛经》，意味着禅修的目的在于超越心法二元对立，达到心法合一的境界。\n有法即失，无法可持。——出自《法华经》，表明了禅修的目的在于超越对法的执着，达到无法可持的境界。\n一切法皆空，无自性可得。——出自《般若波罗蜜多心经》，表明了禅修的目的在于超越对法的执着，达到法空自性的境界。\n一念清净，万法清净。——出自《华严经》，表明了禅修的目的在于通过清净自己的心念，达到清净万法的境界。\n心如止水，万象皆明。——出自《华严经》，表明了禅修的目的在于平静自己的心念，达到洞悉万象的境界。\n禅宗无上法门，直指人心。——出自《南宗顿教正脉纲要》，强调了禅修的重点在于直接指向人心，达到无上法门的境界。\n一念善恶，天人两界。——出自《华严经》，表明了禅修的重要性，一个念头的转变可以影响到自己的生死轮回。\n一念不生，万法无边。——出自《华严经》，表明了禅修的目的在于超越生死轮回，达到无边无界的境界。\n\n禅宗的流派禅宗是中国佛教的一个流派，发展出了五个主要的派别。以下是每个派别的简要介绍：\n\n潮州宗：又称南宗，起源于唐代。该派别注重禅修的实践，强调“顿悟”（即一瞬间的证悟），并提倡“无相禅”（即不依赖任何具体的对象进行禅修）。代表人物有南泉普贤、临济義玄等。\n唐宗：又称北宗，起源于唐代。该派别注重禅修的实践，强调“渐悟”（即逐步的证悟），并提倡“有相禅”（即通过禅修对象来证悟真理）。代表人物有神秀、华严智海等。\n法眼宗：起源于宋代。该派别注重禅修的实践，强调“顿悟”，并提倡“无门禅”（即不依赖任何门派或传承进行禅修）。代表人物有法眼、道信等。\n洛阳宗：起源于唐代。该派别注重禅修的实践，强调“渐悟”，并提倡“律严禅”（即通过对具体律法的遵守来进行禅修）。代表人物有珂罗版行者、道宣等。\n临济宗：起源于宋代。该派别注重禅修的实践，强调“顿悟”，并提倡“公案禅”（即通过对禅宗公案的研究来进行禅修）。代表人物有临济義玄、黄龙师隐等。\n\n《金刚经》的偈语\n诸法无我，如梦幻泡影。这句话体现了金刚经中的空性思想，即所有事物都是虚幻的，不存在固定的实体。\n一切有为法，如梦幻泡影，如露亦如电，应作如是观。这句话也是在强调一切法皆无常、无我、无相。\n色不异空，空不异色。色即是空，空即是色。这句话表明了色与空是相互依存的，没有色就没有空，没有空也就没有色。\n无我法界，生死相续。这句话意味着，只有放下自我，才能摆脱生死轮回的束缚。\n一切众生，皆有如来智慧。这句话表明了佛性存在于所有众生之中，只要发掘自己内在的潜力，就能成佛。\n一切法门，悉皆不二。这句话意味着，所有的修行方法都是相通的，只要找到适合自己的方法，就能达到成佛的目的。\n菩提本无树，明镜亦非台，本来无一物，何处惹尘埃。这句话表明了菩提本无所在，只有放下执念，才能觉悟。\n舍利子，色不异空，空不异色，色即是空，空即是色。这句话重复了第三条中的观点，强调了色与空的相互依存。\n舍利子，一切法无我，我亦无所得，得法者多劫修。这句话表明了修行的过程中，要放下执念，不执着于任何事物，才能获得真正的解脱。\n舍利子，若以色见我，以我见色，不见如来。这句话意味着，只有超越了对物质的执着，才能真正看到佛性的存在。\n\n","categories":["哲学思考"],"tags":["禅宗","偈语","金刚经","佛教"]},{"title":"儒释道思想和分与合","url":"/2021_06_20_3religion/","content":"人心无限，事物有限，以有限求无限不可得，矛盾出现了，怎么解决？\n儒、道、佛都给了方法。\n儒家：无所为而为何谓无所为？\n我们先看什么是有所为\n有所为而为：抱着目的去做事，说的现代点就是功利主义\n无所为即：本该做的，因该做的，读书不是为了功名，而是增长知识与涵养，尽孝不是为了名声，而是报答养育之恩，等等诸如此类\n还有一种无所为，是明知不可为而为，本该做的，但是做了利益有损。\n有所为和无所为在有利于我们的方面可以统一，那么在明知不可为的情况下就对立了，功利主义就妥协退缩了。做了我们本该做的事，却有损了我们的利益，错了吗？没有错。儒家讲的是仁者无敌。\n道家：无为而无不为无为：不做我们不该做的事，什么是我们不该做的事呢？这个不可言说，从自身实际出发，可小可大、可虚可实。\n无不为：去掉无为的就是无不为\n佛家：无心而为佛家讲众生平等，因果轮回。佛家是无神论，佛是觉醒的众生，众生是未觉醒的佛\n心，因无所住而生其心，人人皆有佛性，见性成佛 ，即心即佛。\n为：用为来消业，最高追求，摆脱轮回，是为涅槃\n无心而为，通俗来讲但行好事、莫问前程，这里的好事即心，即佛，\n三家汇合佛道儒三家，各自给了解决问题的方法，无所为而为、无为而无不为、无心而为，我们不难发现，三者都有为\n为便是做事的意思\n儒家让我们做该做的事，道家让我们除去不该做的事都要做，佛家让我们不抱有目的的做本该做的事。\n是不是有些惊愕，原本我以为儒家代表腐朽、道家代表消极、佛家就是迷信，原来小丑就是我自己。\n上面巴拉巴拉说了一大堆，如何应用自身呢？\n安心立命此心为何心？\n王阳明讲，心即理，自性具足，理，生命情感的真相，良知存于心，良知即天理，和孟子的人本性善、万物皆备于我一脉相承。\n那佛家的贪、痴、嗔是不是出于此心呢？\n王阳明讲的良知，佛家说的贪、痴、嗔皆来于心，\n如何安此心？儒家说是修心，佛家称为修行，现在我们讲：树立良好的道德观、价值观。\n焦虑、急躁，惶惶不可终日。此为心不安。\n心有所求而不得，心有所虑而不安，空空如也。\n此命为何命？佛家说因果轮回，此生是前世的果，要想来世有福报，今世就用无心而为来消前世的业，修来世的因。\n佛家讲无心而为，冥冥之中，自有天命。\n如果我们认同佛的因果轮回，那么嫉妒之心可消散全无，但是大家信命吗？\n你要说我信命，既然命运安排好了，该来的总会来，我坐吃等死就行了，此为大缪。\n佛家讲的命是和为是分不开的，为是因，命是果。因在前，果在后。\n你要说我不信命，人定胜天，想什么、干什么、就要得什么。这和佛家的因果关系有相似之处，不同的是佛家为是因，命是果，到了你这，想是因，为是果，随心所欲，但往往痴人说梦哉！\n信命也好，不信命也罢，通俗来讲大家都是想有个好的归宿，不同的是有人为己求归宿，有人为天下求归宿。\n如何达此命？\n孔子讲：仁，仁者无敌\n孟子讲：心，万物皆备于我，反身而诚，乐莫大焉\n朱熹讲：格物致知\n王阳明讲：知行合一\n教员说：从实践中来到实践中去\n安心立命:养成正确的道德观、价值观，在此基础上树立自己的理想抱负，运用适合自己的方法，去完成自己的抱负。\n与君共勉之。\n","categories":["哲学思考"],"tags":["佛教","道家","儒家"]},{"title":"读《GO语言实战》","url":"/2021_05_23_read_goinaction/","content":"Go语言是谷歌2007年开发的语言，开源社区活跃，比较流行的Docker等项目都是使用go语言开发的。Go语言凭借出色的多核多线程高性能模型大大收到后端开发者的喜爱，goroutines和channel的go语言的出色特性之一，除此之外Go语言还包含丰富的标准库、丰富的组件。基于对Go编程语言的好奇和喜爱，笔者开始了阅读学习这方面书籍，这本书推荐给大家。\n\n全书分为9个章节，讲了go要解决的问题、语法糖、几个标准库的介绍、单元测试几方面问题，行文思路比较简单，适合初学者阅读。\n\n目录\n\n","categories":["读书笔记"],"tags":["go"]},{"title":"读《深入理解java虚拟机3》","url":"/2021_06_21_read_jvm/","content":"必须拜读的java圣书。\n\n\n\n第一部分　走近Java第1章　走近Java 21.1　概述 21.2　Java技术体系 31.3　Java发展史 41.4　Java虚拟机家族 121.4.1　虚拟机始祖：Sun Classic&#x2F;Exact VM 121.4.2　武林盟主：HotSpot VM 131.4.3　小家碧玉：Mobile&#x2F;Embedded VM 141.4.4　天下第二：BEA JRockit&#x2F;IBM J9 VM 151.4.5　软硬合璧：BEA Liquid VM&#x2F;Azul VM 161.4.6　挑战者：Apache Harmony&#x2F;Google Android Dalvik VM 171.4.7　没有成功，但并非失败：Microsoft JVM及其他 181.4.8　百家争鸣 191.5　展望Java技术的未来 211.5.1　无语言倾向 211.5.2　新一代即时编译器 231.5.3　向Native迈进 241.5.4　灵活的胖子 261.5.5　语言语法持续增强 271.6　实战：自己编译JDK 291.6.1　获取源码 291.6.2　系统需求 311.6.3　构建编译环境 331.6.4　进行编译 341.6.5　在IDE工具中进行源码调试 361.7　本章小结 39第二部分　自动内存管理第2章　Java内存区域与内存溢出异常 422.1　概述 422.2　运行时数据区域 422.2.1　程序计数器 432.2.2　Java虚拟机栈 432.2.3　本地方法栈 442.2.4　Java堆 442.2.5　方法区 462.2.6　运行时常量池 472.2.7　直接内存 472.3　HotSpot虚拟机对象探秘 482.3.1　对象的创建 482.3.2　对象的内存布局 512.3.3　对象的访问定位 522.4　实战：OutOfMemoryError异常 532.4.1　Java堆溢出 542.4.2　虚拟机栈和本地方法栈溢出 562.4.3　方法区和运行时常量池溢出 612.4.4　本机直接内存溢出 652.5　本章小结 66第3章　垃圾收集器与内存分配策略 673.1　概述 673.2　对象已死？ 683.2.1　引用计数算法 683.2.2　可达性分析算法 703.2.3　再谈引用 713.2.4　生存还是死亡？ 723.2.5　回收方法区 743.3　垃圾收集算法 753.3.1　分代收集理论 753.3.2　标记-清除算法 773.3.3　标记-复制算法 783.3.4　标记-整理算法 793.4　HotSpot的算法细节实现 813.4.1　根节点枚举 813.4.2　安全点 823.4.3　安全区域 833.4.4　记忆集与卡表 843.4.5　写屏障 853.4.6　并发的可达性分析 873.5　经典垃圾收集器 893.5.1　Serial收集器 903.5.2　ParNew收集器 923.5.3　Parallel Scavenge收集器 933.5.4　Serial Old收集器 943.5.5　Parallel Old收集器 953.5.6　CMS收集器 963.5.7　Garbage First收集器 983.6　低延迟垃圾收集器 1043.6.1　Shenandoah收集器 1053.6.2　ZGC收集器 1123.7　选择合适的垃圾收集器 1213.7.1　Epsilon收集器 1213.7.2　收集器的权衡 1213.7.3　虚拟机及垃圾收集器日志 1223.7.4　垃圾收集器参数总结 1273.8　实战：内存分配与回收策略 1293.8.1　对象优先在Eden分配 1303.8.2　大对象直接进入老年代 1313.8.3　长期存活的对象将进入老年代 1323.8.4　动态对象年龄判定 1343.8.5　空间分配担保 1353.9　本章小结 137第4章　虚拟机性能监控、故障处理工具 1384.1　概述 1384.2　基础故障处理工具 1384.2.1　jps：虚拟机进程状况工具 1414.2.2　jstat：虚拟机统计信息监视工具 1424.2.3　jinfo：Java配置信息工具 1434.2.4　jmap：Java内存映像工具 1444.2.5　jhat：虚拟机堆转储快照分析工具 1454.2.6　jstack：Java堆栈跟踪工具 1464.2.7　基础工具总结 1484.3　可视化故障处理工具 1514.3.1　JHSDB：基于服务性代理的调试工具 1524.3.2　JConsole：Java监视与管理控制台 1574.3.3　VisualVM：多合-故障处理工具 1644.3.4　Java Mission Control：可持续在线的监控工具 1714.4　HotSpot虚拟机插件及工具 1754.5　本章小结 180第5章　调优案例分析与实战 1815.1　概述 1815.2　案例分析 1815.2.1　大内存硬件上的程序部署策略 1825.2.2　集群间同步导致的内存溢出 1845.2.3　堆外内存导致的溢出错误 1855.2.4　外部命令导致系统缓慢 1875.2.5　服务器虚拟机进程崩溃 1875.2.6　不恰当数据结构导致内存占用过大 1885.2.7　由Windows虚拟内存导致的长时间停顿 1895.2.8　由安全点导致长时间停顿 1905.3　实战：Eclipse运行速度调优 1925.3.1　调优前的程序运行状态 1935.3.2　升级JDK版本的性能变化及兼容问题 1965.3.3　编译时间和类加载时间的优化 2005.3.4　调整内存设置控制垃圾收集频率 2035.3.5　选择收集器降低延迟 2065.4　本章小结 209第三部分　虚拟机执行子系统第6章　类文件结构 2126.1　概述 2126.2　无关性的基石 2126.3　Class类文件的结构 2146.3.1　魔数与Class文件的版本 2156.3.2　常量池 2186.3.3　访问标志 2246.3.4　类索引、父类索引与接口索引集合 2256.3.5　字段表集合 2266.3.6　方法表集合 2296.3.7　属性表集合 2306.4　字节码指令简介 2516.4.1　字节码与数据类型 2516.4.2　加载和存储指令 2536.4.3　运算指令 2546.4.4　类型转换指令 2556.4.5　对象创建与访问指令 2566.4.6　操作数栈管理指令 2566.4.7　控制转移指令 2576.4.8　方法调用和返回指令 2576.4.9　异常处理指令 2586.4.10　同步指令 2586.5　公有设计，私有实现 2596.6　Class文件结构的发展 2606.7　本章小结 261第7章　虚拟机类加载机制 2627.1　概述 2627.2　类加载的时机 2637.3　类加载的过程 2677.3.1　加载 2677.3.2　验证 2687.3.3　准备 2717.3.4　解析 2727.3.5　初始化 2777.4　类加载器 2797.4.1　类与类加载器 2807.4.2　双亲委派模型 2817.4.3　破坏双亲委派模型 2857.5　Java模块化系统 2877.5.1　模块的兼容性 2887.5.2　模块化下的类加载器 2907.6　本章小结 292第8章　虚拟机字节码执行引擎 2938.1　概述 2938.2　运行时栈帧结构 2948.2.1　局部变量表 2948.2.2　操作数栈 2998.2.3　动态连接 3008.2.4　方法返回地址 3008.2.5　附加信息 3018.3　方法调用 3018.3.1　解析 3018.3.2　分派 3038.4　动态类型语言支持 3158.4.1　动态类型语言 3168.4.2　Java与动态类型 3178.4.3　java.lang.invoke包 3188.4.4　invokedynamic指令 3218.4.5　实战：掌控方法分派规则 3248.5　基于栈的字节码解释执行引擎 3268.5.1　解释执行 3278.5.2　基于栈的指令集与基于寄存器的指令集 3288.5.3　基于栈的解释器执行过程 3298.6　本章小结 334第9章　类加载及执行子系统的案例与实战 3359.1　概述 3359.2　案例分析 3359.2.1　Tomcat：正统的类加载器架构 3359.2.2　OSGi：灵活的类加载器架构 3389.2.3　字节码生成技术与动态代理的实现 3419.2.4　Backport工具：Java的时光机器 3459.3　实战：自己动手实现远程执行功能 3489.3.1　目标 3489.3.2　思路 3499.3.3　实现 3509.3.4　验证 3559.4　本章小结 356第四部分　程序编译与代码优化第10章　前端编译与优化 35810.1　概述 35810.2　Javac编译器 35910.2.1　Javac的源码与调试 35910.2.2　解析与填充符号表 36210.2.3　注解处理器 36310.2.4　语义分析与字节码生成 36410.3　Java语法糖的味道 36710.3.1　泛型 36710.3.2　自动装箱、拆箱与遍历循环 37510.3.3　条件编译 37710.4　实战：插入式注解处理器 37810.4.1　实战目标 37910.4.2　代码实现 37910.4.3　运行与测试 38510.4.4　其他应用案例 38610.5　本章小结 386第11章　后端编译与优化 38811.1　概述 38811.2　即时编译器 38911.2.1　解释器与编译器 38911.2.2　编译对象与触发条件 39211.2.3　编译过程 39711.2.4　实战：查看及分析即时编译结果 39811.3　提前编译器 40411.3.1　提前编译的优劣得失 40511.3.2　实战：Jaotc的提前编译 40811.4　编译器优化技术 41111.4.1　优化技术概览 41111.4.2　方法内联 41511.4.3　逃逸分析 41711.4.4　公共子表达式消除 42011.4.5　数组边界检查消除 42111.5　实战：深入理解Graal编译器 42311.5.1　历史背景 42311.5.2　构建编译调试环境 42411.5.3　JVMCI编译器接口 42611.5.4　代码中间表示 42911.5.5　代码优化与生成 43211.6　本章小结 436第五部分　高效并发第12章　Java内存模型与线程 43812.1　概述 43812.2　硬件的效率与一致性 43912.3　Java内存模型 44012.3.1　主内存与工作内存 44112.3.2　内存间交互操作 44212.3.3　对于volatile型变量的特殊规则 44412.3.4　针对long和double型变量的特殊规则 45012.3.5　原子性、可见性与有序性 45012.3.6　先行发生原则 45212.4　Java与线程 45512.4.1　线程的实现 45512.4.2　Java线程调度 45812.4.3　状态转换 46012.5　Java与协程 46112.5.1　内核线程的局限 46112.5.2　协程的复苏 46212.5.3　Java的解决方案 46412.6　本章小结 465第13章　线程安全与锁优化 46613.1　概述 46613.2　线程安全 46613.2.1　Java语言中的线程安全 46713.2.2　线程安全的实现方法 47113.3　锁优化 47913.3.1　自旋锁与自适应自旋 47913.3.2　锁消除 48013.3.3　锁粗化 48113.3.4　轻量级锁 48113.3.5　偏向锁 48313.4　本章小结 485\n","categories":["读书笔记"],"tags":["java","jvm"]},{"title":"操作系统与计算机网络总结","url":"/2021_10_16_os_net/","content":"操作系统知识点\n\n进程与线程上图进程与线程部分是一个非常重要的考察点。\n\n首先需要掌握进程与线程的区别和联系：\n\n进程是系统资源分配的最小单位，线程是程序执行的最小单位；\n进程使用独立的数据空间，而线程共享进程的数据空间。\n\n\n线程调度，简单了解线程的几种调度算法就可以了。比如时间片轮转调度、先来先服务调度、优先级调度、多级反馈队列调度以及高响应比优先调度。\n\n线程切换的步骤，主要是了解线程的上下文切换，明白线程切换的代价。关于线程的知识在后面的多线程课程中还会有详细讲解，这里先略过。\n\n在进程与线程部分还有一个比较常见的考察点，就是进程间通信，也就是IPC。这部分在面试中间件研发的相关职位时经常会考察。如上面知识点汇总图中所示，需要了解这6种进程通信方式的原理与适用场景。例如，进程间数据共享的场景可以使用共享内存；进程间数据交换的场景可以使用UnixSocket或者消息队列。\n\n最后协程部分，简单了解协程更轻量化，是在用户态进行调度，切换的代价比线程上下文切换要低很多就可以了，也可以了解Java的第三方协程框架，例如Kilim、Quasar等。\n\n\nLinux 常用命令大部分互联网公司的服务都是在Linux系统上运行的，因此Linux命令也是面试时的常考点，这部分其实主要考察的是候选人是否有线上问题的排查经验，重点学习AWK、top、netstat、grep等高频使用的工具\n还有一些知识点不常考，做适当了解，例如内存分页管理与Swap机制、任务队列与CPULoad等，这些知识在分析线上问题中十分有用。\n扩展知识最后是扩展知识点，例如内存屏障、指令乱序、分支预测、NUMA与CPU亲和性等，如果在面试时有机会谈到的话，会在知识深度上给面试官留下比较好的印象\n计算机网络知识点计算机网络也是非常重要的基础知识，服务之间通过不同的网络协议进行交互，例如HTTP协议、RPC协议等，在Java面试中网络知识被考到的几率非常大。网络知识点汇总如下图。\n\n\n首先你应该深刻理解网络的4&#x2F;7层模型，这是网络知识的基础。\n另外两个非常重要的网络协议就是HTTP和TCP了，这两个协议也是服务交互中使用最多的协议。先来看TCP协议，TCP协议中的三次握手建连与四次挥手断连是一个高频考点，后面会详细介绍。\nTCP的报文状态标志与链接状态，在排查网络问题时非常重要，必须要明白协议状态，才方便抓包分析。\n另一个知识点是Nagel算法和ACK延迟，需要了解产生的背景，是要解决小包问题，提高数据载荷比。知道对于延迟比较敏感且发送数据频率较低的场景可以关闭Nagel算法\n关于TCP的Keepalive，是一种长时间没有数据发送的场景下，TCP保持链接可用的机制，需要知道TCPKeepalive的开启和设置方式。\n最后一点，需要明白TCP是如何通过滑动窗口机制来实现流量控制的。\nHTTP协议部分\n需要掌握HTTP协议的规范，知道协议中的Method、Header、Cookies，需要了解常见状态码的含义，例如404、503、302等。\n另外还有HTTPS的交互流程。\n协议的了解可以在一定程度上体现对新技术的关注程度。可以关注：HTTP2多路复用、Stream流式交互、流量控制、服务端推送、头部压缩等新特性。\n\n除了HTTP和TCP外，UDP也是一个比较常见的传输层协议，UDP的特点是非链接、非可靠传输，但是效率非常高。\n最后可以对QUIC协议进行一些了解，QUIC已经被标准化为HTTP3协议。QUIC是基于UDP协议，但QUIC提供了类似TCP的可靠性保证和流量控制。QUIC可以有效避免HTTP2协议的前序包阻塞问题，能实现零RTT建连，提供FEC前向纠错能力。\n详解 TCP 协议特点TCP是传输层协议，对应OSI网络模型的第四层传输层，特点如下。\n\nTCP协议是基于链接的，也就是传输数据前需要先建立好链接，然后再进行传输。\nTCP链接一旦建立，就可以在链接上进行双向的通信\nTCP的传输是基于字节流而不是报文，将数据按字节大小进行编号，接收端通过ACK来确认收到的数据编号，通过这种机制，TCP协议能够保证接收数据的有序性和完整性，因此TCP能够提供可靠性传输。\nTCP还能提供流量控制能力，通过滑动窗口来控制数据的发送速率。滑动窗口的本质是动态缓冲区，接收端根据自己的处理能力，在TCP的Header中动态调整窗口大小，通过ACK应答包通知给发送端，发送端根据窗口大小调整发送的的速度。\n仅仅有了流量控制能力还不够，TCP协议还考虑到了网络问题可能会导致大量重传，进而导致网络情况进一步恶化，因此TCP协议还提供拥塞控制。TCP处理拥塞控制主要用到了慢启动、拥塞避免、拥塞发生、快速恢复四个算法，感兴趣的同学可以进一步了解。\n\n除了TCP协议的特点，还可以进一步了解TCP协议的报文状态、滑动窗口的工作流程、Keepalive的参数设置和Nagel算法的规则等一些细节。\n另外还有典型的TCP协议问题，例如特定场景下Nagel和ACK延迟机制配合使用可能会出现delay40ms超时后才回复ACK包的问题\n详解三次握手建连接下来看TCP建连的三次握手。TCP是基于链接的，所以在传输数据前需要先建立链接，TCP在传输上是双工传输，不区分Client端与Server端，为了便于理解，我们把主动发起建连请求的一端称作Client端，把被动建立链接的一端称作Server端。\n如下图，建连的时序是从上到下，左右两边的绿色字分别代表Client端与Server端当时的链接状态。\n\n\n首先建立链接前需要Server端先监听端口，因此Server端建立链接前的初始状态就是LISTEN状态，这时Client端准备建立链接，先发送一个SYN同步包，发送完同步包后，Client端的链接状态变成了SYN_SENT状态。Server端收到SYN后，同意建立链接，会向Client端回复一个ACK。\n由于TCP是双工传输，Server端也会同时向Client端发送一个SYN，申请Server向Client方向建立链接。发送完ACK和SYN后，Server端的链接状态就变成了SYN_RCVD。\nClient收到Server的ACK后，Client端的链接状态就变成了ESTABLISHED状态，同时，Client向Server端发送ACK，回复Server端的SYN请求\nServer端收到Client端的ACK后，Server端的链接状态也就变成了的ESTABLISHED状态，此时建连完成，双方随时可以进行数据传输。\n需要明白三次握手是为了建立双向的链接，需要记住Client端和Server端的链接状态变化。另外回答建连的问题时，可以提到SYN洪水攻击发生的原因，就是Server端收到Client端的SYN请求后，发送了ACK和SYN，但是Client端不进行回复，导致Server端大量的链接处在SYN_RCVD状态，进而影响其他正常请求的建连。可以设置tcp_synack_retries &#x3D; 0加快半链接的回收速度，或者调大tcp_max_syn_backlog来应对少量的SYN洪水攻击\n详解四次挥手断连再来看看TCP的断连，如下图所示。\n\n\n\nTCP链接的关闭，通信双方都可以先发起，我们暂且把先发起的一方看作Client，从图中看出，通信中Client和Server两端的链接都是ESTABLISHED状态，然后Client先主动发起了关闭链接请求，Client向Server发送了一个FIN包，表示Client端已经没有数据要发送了，然后Client进入了FIN_WAIT_1状态。\nServer端收到FIN后，返回ACK，然后进入CLOSE_WAIT状态。此时Server属于半关闭状态，因为此时Client向Server方向已经不会发送数据了，可是Server向Client端可能还有数据要发送。\n当Server端数据发送完毕后，Server端会向Client端发送FIN，表示Server端也没有数据要发送了，此时Server进入LAST_ACK状态，就等待Client的应答就可以关闭链接了。\nClient端收到Server端的FIN后，回复ACK，然后进入TIME_WAIT状态。TIME_WAIT状态下需要等待2倍的最大报文段生存时间，来保证链接的可靠关闭，之后才会进入CLOSED关闭状态。而Server端收到ACK后直接就进入CLOSED状态。\n这里可能会问为什么需要等待2倍最大报文段生存时间之后再关闭链接，原因有两个：\n\n保证TCP协议的全双工连接能够可靠关闭；\n保证这次连接的重复数据段从网络中消失，防止端口被重用时可能产生数据混淆。\n\n从这个交互流程可以看出，无论是建连还是断链，都是需要在两个方向上进行，只不过建连时，Server端的SYN和ACK合并为一次发送，而断链时，两个方向上数据发送停止的时间可能不同，所以不能合并发送FIN和ACK。这就是建连三次握手而断链需要四次的原因。\n另外回答断链的问题时，可以提到实际应用中有可能遇到大量Socket处在TIME_WAIT或者CLOSE_WAIT状态的问题。一般开启tcp_tw_reuse和tcp_tw_recycle能够加快TIME-WAIT的Sockets回收；而大量CLOSE_WAIT可能是被动关闭的一方存在代码bug，没有正确关闭链接导致的。\n","categories":["总结笔记"],"tags":["操作系统","计算机网络"]},{"title":"Java语言特性与设计模式总结","url":"/2021_10_17_java_feature/","content":"设计模式知识点设计模式的考察点，一般有两个：\n\n常用设计模式的实现；\n设计模式的使用场景。\n\n设计模式分为3大类型共23种：\n\n创建型：工厂方法模式、抽象工厂模式、单例模式、建造者模式、原型模式。\n结构型：适配器模式、装饰器模式、代理模式、外观模式、桥接模式、组合模式、享元模式。\n行为型：策略模式、模板方法模式、观察者模式、迭代子模式、责任链模式、命令模式、备忘录模式、状态模式、访问者模式、中介者模式、解释器模式。\n\n最常见的设计模式有：单例模式、工厂模式、代理模式、构造者模式、责任链模式、适配器模式、观察者模式等，如下图所示。\n\n\n对于设计模式，你应该明白不同的设计用来解决什么场景问题，对于常用的设计模式能够灵活运用。下面重点介绍几种常用的设计模式。\n单例模式首先是单例模式，这个模式在实际业务中会经常用到，也是设计模式中的主要考察点。这里介绍线程安全的单例模式实现方式。\n单例模式常见的实现方式有三种。\n\n第一种是静态初始化方式，也叫作饿汉方式。实现的思路就是在类初始化时完成单例实例的创建，因此不会产生并发问题，在这种方式下不管是否会使用到这个单例，都会创建这个单例。\n第二种实现方式是双重检查，也叫作懒汉方式，只有在真正用到这个单例实例的时候才会去创建，如果没有使用就不会创建。这个方式必然会面对多个线程同时使用实例时的并发问题。为了解决并发访问问题，通过synchronized或者lock进行双重检查，保证只有一个线程能够创建实例。这里要注意内存可见性引起的并发问题，必须使用volatile关键字修饰单例变量。\n第三种是单例注册表方式，Spring中Bean的单例模式就是通过单例注册表方式实现的。\n\n下面结合设计模式的实际应用，来介绍常用的设计模式，如下图所示。在面试时遇到类似问题，记得要将设计模式与实际业务场景进行结合，来体现对设计模式的理解和应用能力。\n\n\n工厂模式工厂模式是创建不同类型实例时常用的方式，例如Spring中的各种Bean是有不同Bean工厂类进行创建的。\n代理模式代理模式，主要用在不适合或者不能直接引用另一个对象的场景，可以通过代理模式对被代理对象的访问行为进行控制。Java的代理模式分为静态代理和动态代理。静态代理指在编译时就已经创建好了代理类，例如在源代码中编写的类；动态代理指在JVM运行过程中动态创建的代理类，使用动态代理的方法有JDK动态代理、CGLIB、Javassist等。面试时遇到这个问题可以举个动态代理的例子，比如在Motan RPC中，是使用JDK的动态代理，通过反射把远程请求进行封装，使服务看上去就像在使用本地的方法。\n\n责任链模式责任链模式有点像工厂的流水线，链上每一个节点完成对对象的某一种处理，例如Netty框架在处理消息时使用的Pipeline就是一种责任链模式。\n\n模式演进案例\n\n适配器模式适配器模式，类似于我们常见的转接头，把两种不匹配的对象来进行适配，也可以起到对两个不同的对象进行解藕的作用。例如我们常用的日志处理框架SLF4J，如果我们使用了SLF4J就可以跟Log4j或者Logback等具体的日志实现框架进行解藕。通过不同适配器将SLF4J与Log4j等实现框架进行适配，完成日志功能的使用\n\n适配器模式和代理模式的一个显著差别：代理模式是方法名相同，一般实现同接口，适配器模式是方法名不同，一般用组合方式嵌入适配对象。\n\n观察者模式观察者模式也被称作发布订阅模式，适用于一个对象的某个行为需要触发一系列事件的场景，例如gRPC中的Stream流式请求的处理就是通过观察者模式实现的。\n构造者模式构造者模式，适用于一个对象有很多复杂的属性，需要根据不同情况创建不同的具体对象，例如创建一个PB对象时使用的builder方式。\nJava语言特性知识点Java语言特性的知识点汇总如下图所示。\n\n\n常用集合类实现与Java并发工具包JUC是常见考点，JUC会在后面的多线程总结中进行详细讲解。\nJava的集合类中部分需要重点了解类的实现。例如，HashMap、TreeMap是如何实现的等。\n动态代理与反射是Java语言的特色，需要掌握动态代理与反射的使用场景，例如在ORM框架中会大量使用代理类。而RPC调用时会使用到反射机制调用实现类方法\nJava基础数据类型也常常会在面试中被问到，例如各种数据类型占用多大的内存空间、数据类型的自动转型与强制转型、基础数据类型与wrapper数据类型的自动装箱与拆箱等。\nJava对对象的引用分为强引用、软引用、弱引用、虚引用四种，这些引用在GC时的处理策略不同，强引用不会被GC回收；软引用内存空间不足时会被GC回收；弱引用则在每次GC时被回收；虚引用必须和引用队列联合使用，主要用于跟踪一个对象被垃圾回收的过程。\nJava的异常处理机制就是try-catch-finally机制，需要知道异常时在try catch中的处理流程；需要了解Error和Exception的区别。\n\nError和Exception同实现自Throwable异常分类区别\n\n最后Java的注解机制和SPI扩展机制可以作为扩展点适当了解。\n详解Map关于Java的基础知识重点讲解最常考察点HashMap和ConcurrentHashMap，以及Java的不同版本新技术特性，如下图所示。\n\n\n面试中，Map的实现这个题目能够考察到数据结构、Java基础实现以及对并发问题处理思路的掌握程度。\n\nHashMap\n\n先来看HashMap的实现，简单来说，Java的HashMap就是数组加链表实现的，数组中的每一项是一个链表。通过计算存入对象的HashCode，来计算对象在数组中要存入的位置，用链表来解决散列冲突，链表中的节点存储的是键值对。\n除了实现的方式，我们还需要知道填充因子的作用与Map扩容时的rehash机制，需要知道HashMap的容量都是2的幂次方，是因为可以通过按位与操作来计算余数，比求模要快。另外需要知道HashMap是非线程安全的，在多线程put的情况下，有可能在容量超过填充因子时进行rehash，因为HashMap为了避免尾部遍历，在链表插入元素时使用头插法，多线程的场景下有可能会产生死循环\n\n\nConcurrentHashMap\n\n从HashMap的非线程安全，面试官很自然地就会问到线程安全的ConcurrentHashMap。ConcurrentHashMap采用分段锁的思想来降低并发场景下的锁定发生频率，在JDK1.7与1.8中的实现差异非常大，1.7中使用Segment进行分段加锁，降低并发锁定；1.8中使用CAS自旋锁的乐观锁来提高性能，但是在并发度较高时性能会比较一般。另外1.8中的ConcurrentHashMap引入了红黑树来解决Hash冲突时链表顺序查找的问题。红黑树的启用条件与链表的长度和Map的总容量有关，默认是链表大于8且容量大于64时转为红黑树。这部分内容建议详细阅读源码进行学习。\n\n\n\n详解 Java 版本特性\n\nJava近些年一改以往的版本发布风格，发布频率提高了很多。目前大部分公司的生产环境使用的还是1.8版本，一少部分升级到1.9或1.10 版本，Java的1.8版本是一个长期支持的版本，最新发布的1.11版本也是一个长期支持的版本，1.11版本中已经包含了1.9、1.10 版本的功能，所以1.8和1.11版本是我们要重点关注的版本\n在1.8版本中Java增加了对lambda表达式的支持，使Java代码的编写可以更简洁，也更方便支持并行计算。并且提供了很多Stream流式处理的API。1.8版本还支持了方法引用的能力，可以进一步简化lambda表达式的写法。\n在1.8版本中，接口可以提供默认方法了，这样可以简化一些简单的抽象类。最后在1.8版本中对方法区进行调整，使用Metaspace替换掉了PermGen的永久代。Metaspace与PermGen之间最大的区别在于：Metaspace并不在虚拟机中，而是使用本地内存。替换的目的一方面是可以提升对元数据的管理同时提升GC效率，另一方面是方便后续HotSpot与JRockit合并。\n在1.9、1.10版本中的主要特性是增加了模块系统，将G1设为默认垃圾回收器、支持局部变量推断等功能。这些功能都已经包含在1.11版本中。\n1.11版本是Java最新的长期支持版本，也将会是未来一段时间的主要版本，1.11版本中提供的最激动人心的功能是ZGC这个新的垃圾回收器，ZGC为大内存堆设计，有着非常强悍的性能，能够实现10ms以下的GC暂停时间。关于ZGC会在下一课中进一步介绍。除此之外，1.11版本对字符串处理API进行了增强，提供了字符复制等功能。1.11版本还内置了HttpClient\n考察点从面试官角度出发，总结本课时对于计算机基础和Java语言特性的考察点如下：\n\n第一考察点就是对基本概念和基本原理的考察。要求对这两项的理解必须是正确的，清晰的。例如网络协议的4&#x2F;7层模型的概念，TCP协议流量控制的实现原理等。\n第二个考察点是常用工具、模型的实现方式和使用姿势，例如HashMap在JDK 1.8中的实现方式是怎样的？单例模式有几种实现方式？什么场景下该使用懒汉式单例实现，什么场景下该使用饿汉式单例实现等。\n第三个考察点是经常使用到的一些知识点，例如你常用的Linux命令有哪些，都用来解决什么问题？\n第四个考察点是实际应用中容易犯错的点，例如&#x3D;&#x3D;与equals的区别，例如对象的强引用使用不当可能导致内存泄露，主要考察候选人对于不同对象引用方式的作用和理解。\n第五个考察点是与面试方向相关的知识点。例如面试的岗位是中间件研发，面试时可能会涉及更多的存储、网络相关的知识的考察。\n\n加分项前面提到的考察点是面试通过的必要条件，回答出问题并不一定能保证通过面试，所以如何做到比其他竞争者更优秀，给面试官留下更好的印象，是成功的关键。你需要一些buff。这些加分能力不仅仅针对这一课的内容，后续课程也有一定的通用性。能将面试考察点与实际业务场景结合，或者与实际使用经验结合。\n这样能够更好的体现对知识点的理解，突出实践能力。例如，在回答 “你知道哪几种设计模式” 这个问题时，不但能说出几种设计模式，以及适合哪类场景，而且还能指出哪些著名的框架在处理什么问题时使用了哪种设计模式，或者自己在处理某个项目的什么场景时，使用了哪种设计模式，取得了什么效果，这样肯定会给面试官留下非常好的印象。\n\n用反例来描述在实际场景中，误用某些功能会带来的问题。\n\n例如，介绍反射机制时，除了介绍反射机制的实现方式、应用场景外，还可以提到大量使用反射会对性能产生影响，应避免滥用。\n\n\n知道与考察知识点相关的优化点。\n\n例如在介绍TCP建连与断连时，最好能够指出线上如果出现大量time_wait时，可以通过调整系统参数加快连接的回收与复用\n\n\n了解与知识点相关的最新技术趋势\n\n例如介绍ConcurrentHashMap的实现时，能够知道1.8版本的改进细节。或者在介绍HTTP时能够说出HTTP2和QUIC的特点与实现等\n回答面试问题时，在比较了解的前提下，尽量增加回答内容的深度。例如在介绍TCP的滑动窗口时，能讲到流量和拥塞控制，近一步能指出不同的解决拥塞的算法等\n这里要注意，面试官一般会沿着候选人的回答继续追问，如果对细节不太了解可能会适得其反。\n\n\n\n真题汇总\n\n解题思路如下。\n\n第一题：线程、进程的区别和联系，主要从资源占用、切换效率、通信方式等方面进行解答；\n第二题：线程的切换过程主要考察上下文切换，需要保存寄存器、栈等现场，需要由用户态切换到内核态。最后通过vmstat命令查看上下文切换的情况；\n第三题：常用的Linux命令可以参考前面操作系统汇总提到的命令；\n第四题、第五题，知识点详解中已经介绍过了，务必要掌握；\n第六题：大致包括DNS解析、TCP建连、HTTP请求、HTTP响应等，实际回答时，可以画个简单的交互图来说明。\n\n再汇总一些真题，包括基础概念，以及前面介绍过的知识点，如下图所示。\n","categories":["总结笔记"],"tags":["java","设计模式"]},{"title":"mybatis-sql拦截器打印sql执行时间","url":"/2021_08_06_mybatis-sql%E6%8B%A6%E6%88%AA%E5%99%A8%E6%89%93%E5%8D%B0sql%E6%89%A7%E8%A1%8C%E6%97%B6%E9%97%B4/","content":"集成mybatis，打印sql以及执行时间~\n\nmybatis拦截器import lombok.extern.slf4j.Slf4j;import org.apache.ibatis.cache.CacheKey;import org.apache.ibatis.executor.Executor;import org.apache.ibatis.mapping.BoundSql;import org.apache.ibatis.mapping.MappedStatement;import org.apache.ibatis.mapping.ParameterMapping;import org.apache.ibatis.plugin.*;import org.apache.ibatis.reflection.MetaObject;import org.apache.ibatis.session.Configuration;import org.apache.ibatis.session.ResultHandler;import org.apache.ibatis.session.RowBounds;import org.apache.ibatis.type.TypeHandlerRegistry;import org.springframework.util.CollectionUtils;import java.text.DateFormat;import java.util.Date;import java.util.List;import java.util.Locale;import java.util.Properties;import java.util.regex.Matcher;/** * @Author * @Date 2020/7/29 14:19 * @Version 版本号 * @Description mybatis执行sql耗时 */@Slf4j@Intercepts(&#123;@Signature(        type = Executor.class,        method = &quot;update&quot;,        args = &#123;MappedStatement.class, Object.class&#125;), @Signature(        type = Executor.class,        method = &quot;query&quot;,        args = &#123;MappedStatement.class, Object.class, RowBounds.class, ResultHandler.class&#125;), @Signature(        type = Executor.class,        method = &quot;query&quot;,        args = &#123;MappedStatement.class, Object.class, RowBounds.class, ResultHandler.class, CacheKey.class, BoundSql.class&#125;)&#125;)public class SqlStatementInterceptor implements Interceptor &#123;    @Override    public Object intercept(Invocation invocation) throws Throwable &#123;        long startTime = System.currentTimeMillis();        try &#123;            return invocation.proceed();        &#125; finally &#123;            long timeConsuming = System.currentTimeMillis() - startTime;            int level;            if (timeConsuming &lt; 100) &#123;                level = 0;            &#125; else if (timeConsuming &lt; 200) &#123;                level = 1;            &#125; else if (timeConsuming &lt; 300) &#123;                level = 2;            &#125; else if (timeConsuming &lt; 400) &#123;                level = 3;            &#125; else if (timeConsuming &lt; 500) &#123;                level = 4;            &#125; else &#123;                level = 5;            &#125;            Object[] args = invocation.getArgs();            MappedStatement ms = (MappedStatement) args[0];            Object parameter = null;            //获取参数，if语句成立，表示sql语句有参数，参数格式是map形式            if (invocation.getArgs().length &gt; 1) &#123;                parameter = invocation.getArgs()[1];            &#125;            String sqlId = ms.getId();// 获取到节点的id,即sql语句的id            BoundSql boundSql = ms.getBoundSql(parameter);  // BoundSql就是封装myBatis最终产生的sql类            Configuration configuration = ms.getConfiguration();  // 获取节点的配置            String sql = showSql(configuration, boundSql); // 获取到最终的sql语句            log.info(&quot;【执行SQL】: &#123;&#125;level &#123;&#125;ms &#123;&#125;&quot;, level, timeConsuming, sql);        &#125;    &#125;    @Override    public Object plugin(Object target) &#123;        return Plugin.wrap(target, this);    &#125;    @Override    public void setProperties(Properties properties) &#123;    &#125;    private String showSql(Configuration configuration, BoundSql boundSql) &#123;        // 获取参数        Object parameterObject = boundSql.getParameterObject();        List&lt;ParameterMapping&gt; parameterMappings = boundSql                .getParameterMappings();        // sql语句中多个空格都用一个空格代替        String sql = boundSql.getSql().replaceAll(&quot;[\\\\s]+&quot;, &quot; &quot;);        if (!CollectionUtils.isEmpty(parameterMappings) &amp;&amp; parameterObject != null) &#123;            // 获取类型处理器注册器，类型处理器的功能是进行java类型和数据库类型的转换　　　　　// 如果根据parameterObject.getClass(）可以找到对应的类型，则替换            TypeHandlerRegistry typeHandlerRegistry = configuration.getTypeHandlerRegistry();            if (typeHandlerRegistry.hasTypeHandler(parameterObject.getClass())) &#123;                sql = sql.replaceFirst(&quot;\\\\?&quot;, Matcher.quoteReplacement(getParameterValue(parameterObject)));            &#125; else &#123;                MetaObject metaObject = configuration.newMetaObject(parameterObject);                // MetaObject主要是封装了originalObject对象，提供了get和set的方法用于获取和设置originalObject的属性值,                // 主要支持对JavaBean、Collection、Map三种类型对象的操作                for (ParameterMapping parameterMapping : parameterMappings) &#123;                    String propertyName = parameterMapping.getProperty();                    if (metaObject.hasGetter(propertyName)) &#123;                        Object obj = metaObject.getValue(propertyName);                        sql = sql.replaceFirst(&quot;\\\\?&quot;, Matcher.quoteReplacement(getParameterValue(obj)));                    &#125; else if (boundSql.hasAdditionalParameter(propertyName)) &#123;                        Object obj = boundSql.getAdditionalParameter(propertyName);  // 该分支是动态sql                        sql = sql.replaceFirst(&quot;\\\\?&quot;, Matcher.quoteReplacement(getParameterValue(obj)));                    &#125; else &#123;                        sql = sql.replaceFirst(&quot;\\\\?&quot;, &quot;缺失&quot;);                    &#125;//打印出缺失，提醒该参数缺失并防止错位                &#125;            &#125;        &#125;        return sql;    &#125;    /**     * 如果参数是String，则添加单引号， 如果是日期，则转换为时间格式器并加单引号；     * 对参数是null和不是null的情况作了处理     */    private static String getParameterValue(Object obj) &#123;        String value = null;        if (obj instanceof String) &#123;            value = &quot;&#x27;&quot; + obj.toString() + &quot;&#x27;&quot;;        &#125; else if (obj instanceof Date) &#123;            Date date = (Date) obj;            DateFormat formatter = DateFormat.getDateTimeInstance(DateFormat.DEFAULT, DateFormat.DEFAULT, Locale.CHINA);            value = &quot;&#x27;&quot; + formatter.format(date) + &quot;&#x27;&quot;;        &#125; else &#123;            if (obj != null) &#123;                value = obj.toString();            &#125; else &#123;                value = &quot;&quot;;            &#125;        &#125;        return value;    &#125;&#125;\n配置文件import com.alibaba.druid.spring.boot.autoconfigure.DruidDataSourceBuilder;import com.baomidou.mybatisplus.extension.plugins.MybatisPlusInterceptor;import com.baomidou.mybatisplus.extension.spring.MybatisSqlSessionFactoryBean;import org.apache.ibatis.plugin.Interceptor;import org.apache.ibatis.session.SqlSessionFactory;import org.mybatis.spring.annotation.MapperScan;import org.springframework.boot.context.properties.ConfigurationProperties;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.core.io.support.PathMatchingResourcePatternResolver;import javax.sql.DataSource;@Configuration@MapperScan(basePackages = &quot;com.xx.staytime.mapper&quot;,sqlSessionFactoryRef = &quot;xxDatasourceSessionFactory&quot;)public class SxFactoryDatasourceConfig &#123;    @Bean(name = &quot;xxDataSource&quot;)    @ConfigurationProperties(prefix = &quot;xx.datasource&quot;)    public DataSource dataSource()&#123;        return DruidDataSourceBuilder.create().build();    &#125;    @Bean(name = &quot;xxDatasourceSessionFactory&quot;)    public SqlSessionFactory sqlSessionFactory(MybatisPlusInterceptor mybatisPlusInterceptor,SqlStatementInterceptor sqlStatementInterceptor) throws Exception &#123;        MybatisSqlSessionFactoryBean sessionFactoryBean = new MybatisSqlSessionFactoryBean();        sessionFactoryBean.setDataSource(dataSource());        sessionFactoryBean.setMapperLocations(new PathMatchingResourcePatternResolver().getResources(&quot;classpath:mapper/*.xml&quot;));        Interceptor[] plugins = &#123;mybatisPlusInterceptor,sqlStatementInterceptor&#125;;        sessionFactoryBean.setPlugins(plugins);        return sessionFactoryBean.getObject();    &#125;&#125;\n\neg.16:40:00.119 app [scheduling-1] INFO  com.x.conf.SqlStatementInterceptor - 【执行SQL】: 0level 2ms SELECT id,cross_record_syscode,entrance_name,vehicle_out,release_mode,cross_time,plate_no,responsible_user_name,responsible_user_tel,order_type,scr_id,wv_id,delete_status,updater FROM trucks_cross_records WHERE (cross_record_syscode = &#x27;5879c4b1746343daa49bceeaec583de2_5c8df9aeea91c_17d&#x27;) 16:40:00.121 app [scheduling-1] INFO  com.x.conf.SqlStatementInterceptor - 【执行SQL】: 0level 2ms SELECT id,cross_record_syscode,entrance_name,vehicle_out,release_mode,cross_time,plate_no,responsible_user_name,responsible_user_tel,order_type,scr_id,wv_id,delete_status,updater FROM trucks_cross_records WHERE (cross_record_syscode = &#x27;5879c4b1746343daa49bceeaec583de2_5c8df9aeea91c_17d&#x27;) 16:40:00.124 app [scheduling-1] INFO  com.x.conf.SqlStatementInterceptor - 【执行SQL】: 0level 1ms SELECT id,cross_record_syscode,entrance_name,vehicle_out,release_mode,cross_time,plate_no,responsible_user_name,responsible_user_tel,order_type,scr_id,wv_id,delete_status,updater FROM trucks_cross_records WHERE (cross_record_syscode = &#x27;5879c4b1746343daa49bceeaec583de2_5c8df9aeea91c_17d&#x27;) 16:40:00.126 app [scheduling-1] INFO  com.x.conf.SqlStatementInterceptor - 【执行SQL】: 0level 2ms SELECT id,cross_record_syscode,entrance_name,vehicle_out,release_mode,cross_time,plate_no,responsible_user_name,responsible_user_tel,order_type,scr_id,wv_id,delete_status,updater FROM trucks_cross_records WHERE (cross_record_syscode = &#x27;5879c4b1746343daa49bceeaec583de2_5c8df9aeea91c_17d&#x27;) 16:40:00.128 app [scheduling-1] INFO  com.x.conf.SqlStatementInterceptor - 【执行SQL】: 0level 2ms SELECT id,cross_record_syscode,entrance_name,vehicle_out,release_mode,cross_time,plate_no,responsible_user_name,responsible_user_tel,order_type,scr_id,wv_id,delete_status,updater FROM trucks_cross_records WHERE (cross_record_syscode = &#x27;5879c4b1746343daa49bceeaec583de2_5c8df9aeea91c_17d&#x27;)","categories":["应用笔记"],"tags":["java"]},{"title":"读《Effective Java》笔记","url":"/2021_12_01_effective_java/","content":"本书作者Joshua Bloch曾是Sun Microsystems的Java架构师，现在是Google的工程师。\n这是一本任何想要提高技能和编写更好代码的Java开发人员的必读之书。里面全是干货。\n“我很希望我10年前就能拥有这本书。有人可能认为我不需要任何Java方面的书籍，但是我需要这本书。”——Java之父James Gosling\n\n\n\n第一章 创建和销毁对象1. 用静态工厂方法代替构造器相对于构造器的优缺点：\n\n优点：  1.名称更容易理解。  2.可以使用缓存。例如单例模式，享元模式，静态工厂方法。  3.可以返回子类。  4.类可以不存在。例如jdbc驱动  缺点：  1.不能子类化。如果没有公共构造函数就不能被实例化。  2.不容易识别。类里面有多少工厂方法文档不会标注，不容易识别。\n\n常用名称：\n//一般有可能使用缓存，比如饿汉单例getInstance();//一般是新的对象newInstance();//Date.from(instant);from();//of();//valueOf();//createXxx();\n\n2. 遇到多个构造器参数时要考虑使用构建器使用建造者模式\n3. 用私有构造器或者枚举类型强化Singleton属性4. 通过私有构造器强化不可实例化能","categories":["总结笔记"],"tags":["java","effective java"]},{"title":"Java-SPI机制","url":"/2022_02_03_java_spi/","content":"随着应用程序越来越复杂，对于我们开发人员来说，如何实现高效的组件化和模块化已经成为了一个重要的问题。而 Java SPI（Service Provider Interface）机制，作为一种基于接口的服务发现机制，可以帮助我们更好地解决这个问题。这样会程序具有高度的灵活性、解耦、可扩展性！\nSPI概念与原理概念Java SPI（Service Provider Interface）是Java官方提供的一种服务发现机制，它允许在运行时动态地加载实现特定接口的类，而不需要在代码中显式地指定该类，从而实现解耦和灵活性。\n具体机制如下图：\n实现原理Java SPI 的实现原理基于 Java 类加载机制和反射机制。\n当使用 ServiceLoader.load(Class service) 方法加载服务时，会检查 META-INF&#x2F;services 目录下是否存在以接口全限定名命名的文件。如果存在，则读取文件内容，获取实现该接口的类的全限定名，并通过 Class.forName() 方法加载对应的类。\n在加载类之后，ServiceLoader 会通过反射机制创建对应类的实例，并将其缓存起来。\n这里涉及到一个懒加载迭代器的思想：\n当我们调用 ServiceLoader.load(Class service) 方法时，并不会立即将所有实现了该接口的类都加载进来，而是返回一个懒加载迭代器。\n只有在使用迭代器遍历时，才会按需加载对应的类并创建其实例。\n这种懒加载思想有以下两个好处：\n\n节省内存如果一次性将所有实现类全部加载进来，可能会导致内存占用过大，影响程序的性能。\n\n增强灵活性由于 ServiceLoader 是动态加载的，因此可以在程序运行时添加或删除实现类，而无需修改代码或重新编译。\n\n\n总的来说，Java SPI 的实现原理比较简单，利用了 Java 类加载和反射机制，提供了一种轻量级的插件化机制，可以很方便地扩展功能。\n优缺点\n优点\n\n\n松耦合性：SPI具有很好的松耦合性，应用程序可以在运行时动态加载实现类，而无需在编译时将实现类硬编码到代码中。\n扩展性：通过SPI，应用程序可以为同一个接口定义多个实现类。这使得应用程序更容易扩展和适应变化。\n易于使用：使用SPI，应用程序只需要定义接口并指定实现类的类名，即可轻松地使用新的服务提供者。\n\n\n缺点\n\n\n配置较麻烦：SPI需要在META-INF&#x2F;services目录下创建配置文件，并将实现类的类名写入其中。这使得配置相对较为繁琐。\n安全性不足：SPI提供者必须将其实现类名称写入到配置文件中，因此如果未正确配置，则可能存在安全风险。\n性能损失：每次查找服务提供者都需要重新读取配置文件，这可能会增加启动时间和内存开销。\n\n应用场景Java SPI机制是一种服务提供者发现的机制，适用于需要在多个实现中选择一个进行使用的场景。\n常见的应用场景包括：\n我们上面对Java SPI的缺点说了一下，我们来说一下：Spring的SPI机制相对于Java原生的SPI机制进行了改造和扩展，主要体现在以下几个方面：\n\n支持多个实现类：Spring的SPI机制允许为同一个接口定义多个实现类，而Java原生的SPI机制只支持单个实现类。这使得在应用程序中使用Spring的SPI机制更加灵活和可扩展。\n\n支持自动装配：Spring的SPI机制支持自动装配，可以通过将实现类标记为Spring组件（例如@Component），从而实现自动装配和依赖注入。这在一定程度上简化了应用程序中服务提供者的配置和管理。\n\n支持动态替换：Spring的SPI机制支持动态替换服务提供者，可以通过修改配置文件或者其他方式来切换服务提供者。而Java原生的SPI机制只能在启动时加载一次服务提供者，并且无法在运行时动态替换。\n\n提供了更多扩展点：Spring的SPI机制提供了很多扩展点，例如BeanPostProcessor、BeanFactoryPostProcessor等，可以在服务提供者初始化和创建过程中进行自定义操作。\n\n\n其他框架也是对Java SPI进行改造和扩展增强，从而更好的提供服务！\n使用步骤\n定义接口：首先需要定义一个接口，所有实现该接口的类都将被注册为服务提供者。\n\n创建实现类：创建一个或多个实现接口的类，这些类将作为服务提供者。\n\n配置文件：在 META-INF&#x2F;services 目录下创建一个以接口全限定名命名的文件，文件内容为实现该接口的类的全限定名，每个类名占一行。\n\n加载使用服务：使用 java.util.ServiceLoader 类的静态方法 load(Class service) 加载服务，默认情况下会加载 classpath 中所有符合条件的提供者。调用 ServiceLoader 实例的 iterator() 方法获取迭代器，遍历迭代器即可获取所有实现了该接口的类的实例。\n\n\n使用 Java SPI 时，需要注意以下几点：\n\n接口必须是公共的，且只能包含抽象方法。\n\n实现类必须有一个无参构造函数。\n\n配置文件中指定的类必须是实现了相应接口的非抽象类。\n\n配置文件必须放在 META-INF&#x2F;services 目录下。\n\n配置文件的文件名必须为接口的全限定名。\n\n\n实践案例组织模块\n接口层//MLogger.javapackage h.xd.spi;public interface MLogger &#123;    void info(String msg);    void debug(String msg);&#125;\nspi实现层//MLogback.javapackage h.xd.spi.impl;import h.xd.spi.MLogger;public class MLogback implements MLogger &#123;    @Override    public void info(String msg) &#123;        System.err.println(&quot;MLogback info &quot; + msg);    &#125;    @Override    public void debug(String msg) &#123;        System.err.println(&quot;MLogback debug &quot; + msg);    &#125;&#125;\n//h.xd.spi.MLoggerh.xd.spi.impl.MLogback\n客户端层\n\n// SpiLogger.javapackage com.h.xd.spi;import h.xd.spi.MLogger;import java.util.ArrayList;import java.util.List;import java.util.ServiceLoader;public class SpiLogger implements MLogger &#123;    private final MLogger logger;    public SpiLogger() &#123;        ServiceLoader&lt;MLogger&gt; load = ServiceLoader.load(MLogger.class);        List&lt;MLogger&gt; mLoggerList = new ArrayList&lt;&gt;();        for(MLogger mLogger: load)&#123;            mLoggerList.add(mLogger);        &#125;        if(!mLoggerList.isEmpty())&#123;            this.logger = mLoggerList.get(0);        &#125;else&#123;            this.logger = null;        &#125;    &#125;    @Override    public void info(String msg) &#123;        if(logger != null)&#123;            logger.info(msg);        &#125;else &#123;            System.out.println(&quot;info无SPI&quot;);        &#125;    &#125;    @Override    public void debug(String msg) &#123;        if(logger != null)&#123;            logger.debug(msg);        &#125;else &#123;            System.out.println(&quot;info无SPI&quot;);        &#125;    &#125;&#125;\n\n//Main.javapackage com.h.xd;public class Main &#123;        public static void main(String[] args) &#123;        SpiLogger spiLogger = new SpiLogger();        spiLogger.debug(&quot;hello&quot;);    &#125;&#125;","categories":["应用笔记"],"tags":["java","java-SPI"]},{"title":"Go channel和select","url":"/2022_06_02_go_channel/","content":"channel是指定类型的值的线程安全队列, channel的最大用途是goroutines之间进行通信。\ngoroutines通信时使用ch&lt;-value将值写入channel,使用value&lt;-ch从channel中接收值。\nchannel基本使用方法package mainimport (\t&quot;fmt&quot;\t&quot;math/rand&quot;\t&quot;time&quot;)func genInts(chInts chan int) &#123;\tchInts &lt;- rand.Intn(1000)&#125;func main() &#123;\trand.Seed(time.Now().UnixNano())\tchInts := make(chan int)\tfor i := 0; i &lt; 2; i++ &#123;\t\tgo genInts(chInts)\t&#125;\tfmt.Printf(&quot;n: %d\\n&quot;, &lt;-chInts)\tfmt.Printf(&quot;n: %d\\n&quot;, &lt;-chInts)&#125;// n: 578// n: 424\n\n启动两个协程，相chInts 通道放随机数\n然后获取打印该随机数两次\n使用range从channel中读取数据当从channel中读取多个值时,通常会使用range:\npackage mainimport (\t&quot;fmt&quot;)func foo(ch chan int) &#123;\tch &lt;- 1\tch &lt;- 2\tclose(ch)&#125;func main() &#123;\tch := make(chan int)\tgo foo(ch)\tfor n := range ch &#123;\t\tfmt.Println(n)\t&#125;\tfmt.Println(&quot;channel is now closed&quot;)&#125;// 1// 2// channel is now closed\n\nchannel关闭,循环就会结束\n使用工作池时，这是常见的模式:\n\n为所有工作创建一个channel\n启动工作\n工作使用v:&#x3D;range chan来提取要处理的任务\n在对所有作业进行排队之后，关闭channel，以便goroutine处理channel中的所有作业\n\n使用select从channel中超时读取从channel中读取数据时,有时候希望限制等待的时间\n使用select可以达到目的:\npackage mainimport (\t&quot;fmt&quot;\t&quot;time&quot;)func main() &#123;\ttimeStart := time.Now()\tchResult := make(chan int, 1)\tgo func() &#123;\t\ttime.Sleep(10 * time.Second)\t\tchResult &lt;- 5\t\tfmt.Printf(&quot;Worker finished&quot;)\t&#125;()\tselect &#123;\tcase res := &lt;-chResult:\t\tfmt.Printf(&quot;Got %d from worker\\n&quot;, res)\tcase &lt;-time.After(100 * time.Millisecond):\t\tfmt.Printf(&quot;Timed out before worker finished\\n&quot;)\t&#125;\tfmt.Printf(&quot;cost %f s&quot;, time.Since(timeStart).Seconds())&#125;// Timed out before worker finished// cost 0.115807 s\n\n向chResult 放入值需要等1秒，  select的时候， 先等到 100毫秒的信号，故输出结果。\n关闭channel使用close(chan)关闭channel关闭channel的主要目的是通知worker goroutine他们的工作已经完成并且可以结束。保证了goroutines不会泄露\npackage mainimport (\t&quot;fmt&quot;\t&quot;time&quot;)func main() &#123;\tch := make(chan string)\tgo func() &#123;\t\tfor s := range ch &#123;\t\t\tfmt.Printf(&quot;received from channel: %s\\n&quot;, s)\t\t&#125;\t\tfmt.Print(&quot;range loop finished because ch was closed\\n&quot;)\t&#125;()\tch &lt;- &quot;foo&quot;\ttime.Sleep(1 * time.Second)\tclose(ch)\ttime.Sleep(1 * time.Second)&#125;// received from channel: foo// range loop finished because ch was closed\n\n从已关闭的channel中读取数据会立即返回零值package mainimport (\t&quot;fmt&quot;)func main() &#123;\tch := make(chan string)\tclose(ch)\tv := &lt;-ch\tfmt.Printf(&quot;Receive from closed channel immediately returns zero value of the type: %#v\\n&quot;, v)&#125;// Receive from closed channel immediately returns zero value of the type: &quot;&quot;\n\n\n判断channel是否关闭package mainimport (\t&quot;fmt&quot;)func main() &#123;\tch := make(chan int)\tgo func() &#123;\t\tch &lt;- 1\t\tclose(ch)\t&#125;()\tv, isOpen := &lt;-ch\tfmt.Printf(&quot;received %d, is channel open: %v\\n&quot;, v, isOpen)\tv, isClosed := &lt;-ch\tfmt.Printf(&quot;received %d, is channel open: %v\\n&quot;, v, isClosed)&#125;// received 1, is channel open: true// received 0, is channel open: false\n\n重复关闭channel会引发panicpackage mainfunc main() &#123;\tch := make(chan string)\tclose(ch)\tclose(ch)&#125;// panic: close of closed channel\n\n发送数据到关闭的channel引发panicpackage mainfunc main() &#123;\tch := make(chan int)\tclose(ch)\tch &lt;- 5 // panics&#125;// panic: send on closed channel\n\n\n是否缓冲发送和接收goroutines块，除非发送goroutine具有要发送的值，并且接收goroutine已准备好接收。\n对每个接收&#x2F;发送操作坚持同步可能会导致不必要的速度降低。\n想象一个场景，一个工人生产，而另一个工人消费。\n如果产生一个值要花一秒钟，消耗也要花一秒钟，则要花2秒的时间来产生和消耗一个值。\n如果生产者可以在channel中排队，则不必等待消费者为每个值做好准备。\n这是缓冲channel的好处。\n通过允许生产者独立于消费者进行生产，我们可以加快某些场景:\npackage mainimport (\t&quot;fmt&quot;\t&quot;time&quot;)func producer(ch chan int) &#123;\tfor i := 0; i &lt; 5; i++ &#123;\t\tif i%2 == 0 &#123;\t\t\ttime.Sleep(10 * time.Millisecond)\t\t&#125; else &#123;\t\t\ttime.Sleep(1 * time.Millisecond)\t\t&#125;\t\tch &lt;- i\t&#125;&#125;func consumer(ch chan int) &#123;\ttotal := 0\tfor i := 0; i &lt; 5; i++ &#123;\t\tif i%2 == 1 &#123;\t\t\ttime.Sleep(10 * time.Millisecond)\t\t&#125; else &#123;\t\t\ttime.Sleep(1 * time.Millisecond)\t\t&#125;\t\ttotal += &lt;-ch\t&#125;&#125;func unbuffered() &#123;\ttimeStart := time.Now()\tch := make(chan int)\tgo producer(ch)\tconsumer(ch)\tfmt.Printf(&quot;Unbuffered version took %s\\n&quot;, time.Since(timeStart))&#125;func buffered() &#123;\ttimeStart := time.Now()\tch := make(chan int, 5)\tgo producer(ch)\tconsumer(ch)\tfmt.Printf(&quot;Buffered version took %s\\n&quot;, time.Since(timeStart))&#125;func main() &#123;\tunbuffered()\tbuffered()&#125;// Unbuffered version took 96.2924ms// Buffered version took 78.0755ms\n\n使用select非阻塞接收可以使用select语句的默认部分进行非阻塞等待。\npackage mainimport (\t&quot;fmt&quot;\t&quot;time&quot;)func main() &#123;\tch := make(chan int, 1)end:\tfor &#123;\t\tselect &#123;\t\tcase n := &lt;-ch:\t\t\tfmt.Printf(&quot;Received %d from a channel\\n&quot;, n)\t\t\tbreak end\t\tdefault:\t\t\tfmt.Print(&quot;Channel is empty\\n&quot;)\t\t\tch &lt;- 8\t\t&#125;\t\t// wait for channel to be filled with values\t\t// don&#x27;t use time.Sleep() like that in production code\t\ttime.Sleep(20 * time.Millisecond)\t&#125;&#125;// Channel is empty// Received 8 from a channel\n\n在for循环的第一次迭代中，由于channel为空，因此select立即以default子句结束。\n我们将值发送到该通道，以便下一个选择将从通道中获取该值。\nchan struct{}信号事件有时不想通过channel发送值，而仅将其用作信号事件的一种方式。\n信令通道通常用来通知goroutine结束:\nstruct{} 不占用内存空间，作为信号节省内存\npackage mainimport (\t&quot;fmt&quot;)func worker(ch chan int, chQuit chan struct&#123;&#125;) &#123;\tfor &#123;\t\tselect &#123;\t\tcase v := &lt;-ch:\t\t\tfmt.Printf(&quot;Got value %d\\n&quot;, v)\t\tcase &lt;-chQuit:\t\t\tfmt.Printf(&quot;Signalled on quit channel. Finishing\\n&quot;)\t\t\tchQuit &lt;- struct&#123;&#125;&#123;&#125;\t\t\treturn\t\t&#125;\t&#125;&#125;func main() &#123;\tch, chQuit := make(chan int), make(chan struct&#123;&#125;)\tgo worker(ch, chQuit)\tch &lt;- 3\tchQuit &lt;- struct&#123;&#125;&#123;&#125;\t// wait to be signalled back by the worker\t&lt;-chQuit&#125;// Got value 3// Signalled on quit channel. Finishing\n\n检查通道是否有可用数据如果通道中没有数据，则在通道上接收会阻塞。\n如果您不想阻止怎么办？\n您可能很想在接收之前检查通道是否有数据。\n您无法在Go中执行此操作，因为它可能无法正常运行。 在您检查可用性的时间和您收到数据的时间之间，其他一些goroutine可能会获取该值。\n如果要避免无限等待，可以使用select添加超时或进行非阻塞等待。\n发送数据到nil channel将永久阻塞package mainfunc main() &#123;\tvar ch chan bool\tch &lt;- true // deadlocks because ch is nil&#125;// fatal error: all goroutines are asleep - deadlock!\n\n通道的未初始化值是nil，因此上述程序会永远阻塞。\n从nil channel接收数据将永久阻塞package mainimport &quot;fmt&quot;func main() &#123;\tvar ch chan bool\tfmt.Printf(&quot;Value received from ch is: %v\\n&quot;, &lt;-ch) // deadlock because c is nil&#125;// fatal error: all goroutines are asleep - deadlock!\n\n发送数据到关闭的channel引发panicpackage mainimport (\t&quot;fmt&quot;\t&quot;time&quot;)func main() &#123;\tvar ch = make(chan int, 100)\tgo func() &#123;\t\tch &lt;- 1\t\ttime.Sleep(time.Second)\t\tclose(ch)\t\tch &lt;- 1\t&#125;()\tfor i := range ch &#123;\t\tfmt.Printf(&quot;i: %d\\n&quot;, i)\t&#125;&#125;// i: 1// panic: send on closed channel\n\n您应该对程序进行架构设计，以使一个发送方控制频道的生存期。\n该规则强调:如果只有一个频道发送者，那么确保您永远不会写入封闭的频道没有问题。\n如果您有多个发件人，这将变得很困难:如果一个发件人关闭了一个频道，那么其他发件人应该不会崩溃吗？\n无需尝试解决上述问题的方法，而是重新设计代码，以使只有一个发送方可以控制通道的生存期。\n从已关闭的channel接收数据会立即返回零值package mainimport &quot;fmt&quot;func main() &#123;    // show    ch := make(chan int, 2)    ch &lt;- 1    ch &lt;- 2    close(ch)    for i := 0; i &lt; 3; i++ &#123;        fmt.Printf(&quot;%d &quot;, &lt;-ch) // -&gt; 1 2 0    &#125;    // show end&#125;// 1 2 0\n\n很容易补救:\npackage mainimport &quot;fmt&quot;func main() &#123;    ch := make(chan int, 2)    ch &lt;- 1    ch &lt;- 2    close(ch)    // show    for &#123;        v, ok := &lt;-ch        if !ok &#123;            break        &#125;        fmt.Printf(&quot;%d &quot;, v) // -&gt; 1 2    &#125;    // show end&#125;\n\n更好更惯用的方法:\npackage mainimport &quot;fmt&quot;func main() &#123;    ch := make(chan int, 2)    ch &lt;- 1    ch &lt;- 2    close(ch)    // show    for v := range ch &#123;        fmt.Printf(&quot;%d &quot;, v) // -&gt; 1 2    &#125;    // show end&#125;\n\n关闭channel以表明Goroutine已结束有时我们需要等到goroutine完成。\n来自已关闭channel的接收会立即返回，可以通过共享done channel来在goroutine之间进行协调。\npackage mainimport &quot;fmt&quot;// showfunc checkState(ch chan struct&#123;&#125;) &#123;\tselect &#123;\tcase &lt;-ch:\t\tfmt.Printf(&quot;channel is closed\\n&quot;)\tdefault:\t\tfmt.Printf(&quot;channel is not closed\\n&quot;)\t&#125;&#125;// show endfunc main() &#123;\t// show\tch := make(chan struct&#123;&#125;)\tcheckState(ch)\tclose(ch)\tcheckState(ch)\t// show end&#125;// channel is not closed// channel is closed","categories":["应用笔记"],"tags":["go","go-channel","go-select"]},{"title":"Go实现端口扫描","url":"/2023_12_27_go_scan_port/","content":"端口扫描基本原理\n向目标主机的某个端口，发送建立链接的请求，如果对方开放了这个端口，就会响应；如果没有没开放，则不会响应。\n\n根据这个原理，向一些常用的端口逐个建立链接，就能知道对方开放了哪些端口。\n\n\n端口扫描方法TelnetWindows系统自带的 Telnet 命令，可以用来探测目标主机的端口是否开放。\n格式：\ntelnet IP 端口\nNmapnmap -sV -p 1-65535 206.119.105.9\n\nMasscanmasscan -p 0-65535 206.119.105.9\n几种扫描工具的原理和区别\nTelnet 使用完整的三次握手建立链接，常用于单个端口的测试。\nMasscan 只发送SYN包，如果对方返回 ACK+SYN 就说明端口开放。\nNmap 默认使用SYN扫描，可以通过修改参数来修改扫描的方式。\n\n端口扫描分类完全链接扫描使用TCP三次握手建立一次完整的链接，从系统调用 connect()开始，端口开放则建立链接，端口不开放则返回-1。\n\n半链接扫描就是我们常说的SYN扫描，只建立TCP的前两次链接，发送一个SYN后，就停止建立链接，等待对方的响应。如果返回一个ACK，就说明端口开放；如果返回一个RESET，就说明端口没开放。\n\ngo实现端口扫描在Go中,我们通常使用net.Dial进行TCP连接。它就两种情况成功:返回conn，失败:err !&#x3D; nil。\n单线程版本// 扫描440到450的端口打开情况package mainimport (\t&quot;fmt&quot;\t&quot;net&quot;)func main() &#123;\tvar ip = &quot;127.0.0.1&quot;\tfor i := 440; i &lt;= 450; i++ &#123;\t\tvar address = fmt.Sprintf(&quot;%s:%d&quot;, ip, i)\t\tconn, err := net.Dial(&quot;tcp&quot;, address)\t\tif err != nil &#123;\t\t\tfmt.Println(address, &quot;是关闭的&quot;)\t\t\tcontinue\t\t&#125;\t\tconn.Close()\t\tfmt.Println(address, &quot;打开&quot;)\t&#125;&#125;\n\n多线程版本package mainimport (\t&quot;fmt&quot;\t&quot;net&quot;\t&quot;sync&quot;\t&quot;time&quot;)func main() &#123;\tvar begin = time.Now()\t//wg\tvar wg sync.WaitGroup\t//ip\tvar ip = &quot;127.0.0.1&quot;\t//var ip = &quot;192.168.43.34&quot;\t//循环\tfor j := 21; j &lt;= 1000; j++ &#123;\t\t//添加wg\t\twg.Add(1)\t\tgo func(i int) &#123;\t\t\t//释放wg\t\t\tdefer wg.Done()\t\t\tvar address = fmt.Sprintf(&quot;%s:%d&quot;, ip, i)\t\t\t//conn, err := net.DialTimeout(&quot;tcp&quot;, address, time.Second*10)\t\t\tconn, err := net.Dial(&quot;tcp&quot;, address)\t\t\tif err != nil &#123;\t\t\t\t//fmt.Println(address, &quot;是关闭的&quot;, err)\t\t\t\treturn\t\t\t&#125;\t\t\tconn.Close()\t\t\tfmt.Println(address, &quot;打开&quot;)\t\t&#125;(j)\t&#125;\t//等待wg\twg.Wait()\tvar elapseTime = time.Now().Sub(begin)\tfmt.Println(&quot;耗时:&quot;, elapseTime)&#125;\n\n线程池版本package main//线程池方式import (\t&quot;fmt&quot;\t&quot;net&quot;\t&quot;sync&quot;\t&quot;time&quot;\t&quot;github.com/loveleshsharma/gohive&quot;)//wgvar wg sync.WaitGroup//地址管道,100容量var addressChan = make(chan string, 100)type Worker struct &#123;&#125;func (s Worker) Run() &#123;\tworker()&#125;//工人func worker() &#123;\t//函数结束释放连接\tdefer wg.Done()\tfor &#123;\t\taddress, ok := &lt;-addressChan\t\tif !ok &#123;\t\t\tbreak\t\t&#125;\t\t//fmt.Println(&quot;address:&quot;, address)\t\tconn, err := net.Dial(&quot;tcp&quot;, address)\t\t//conn, err := net.DialTimeout(&quot;tcp&quot;, address, 10)\t\tif err != nil &#123;\t\t\t//fmt.Println(&quot;close:&quot;, address, err)\t\t\tcontinue\t\t&#125;\t\tconn.Close()\t\tfmt.Println(&quot;open:&quot;, address)\t&#125;&#125;func main() &#123;\tvar begin = time.Now()\t//ip\tvar ip = &quot;127.0.0.1&quot;\t//线程池大小\tvar pool_size = 10000\tvar pool = gohive.NewFixedPool(pool_size)\t//拼接ip:端口\t//启动一个线程,用于生成ip:port,并且存放到地址管道种\tgo func() &#123;\t\tfor port := 1; port &lt;= 60000; port++ &#123;\t\t\tvar address = fmt.Sprintf(&quot;%s:%d&quot;, ip, port)\t\t\t//将address添加到地址管道\t\t\t//fmt.Println(&quot;&lt;-:&quot;,address)\t\t\taddressChan &lt;- address\t\t&#125;\t\t//发送完关闭 addressChan 管道\t\tclose(addressChan)\t&#125;()\t//启动pool_size工人,处理addressChan种的每个地址\tfor work := 0; work &lt; pool_size; work++ &#123;\t\twg.Add(1)\t\tpool.Submit(Worker&#123;&#125;)\t&#125;\t//等待结束\twg.Wait()\t//计算时间\tvar elapseTime = time.Now().Sub(begin)\tfmt.Println(&quot;耗时:&quot;, elapseTime)&#125;\n\n","categories":["应用笔记"],"tags":["go","端口扫描"]},{"title":"Docker安装hexo编译环境","url":"/2023_12_26_docker_hexo/","content":"hexo编译打包静态页面需要依赖node，npm等环境，为了不污染本地环境，考虑用docker拉取ubuntu来搭建hexo编译环境，用来编译博客，以下记录详细过程。\n// 拉取镜像，运行镜像，并进入docker pull ubuntu:22.04docker run -it -p 4000:4000 -v $PWD/data:/home -p 22:22 --name ubuntu -d ubuntu:22.04docker exec -it ubuntu bash\n\n//更新包，安装nodejs, 安装 npm  安装vimapt-get updateapt-get install nodejsroot@5890065e0c87:/home/hexo-blog# node -vv12.22.9apt-get install npmroot@5890065e0c87:/home/hexo-blog# npm -v8.5.1root@5890065e0c87:/home/hexo-blog# npm installapt install vim\n\n// hexo 找不到命令，写一下环境变量vim ~/.bashrc# 在最后一行加上export PATH=$PATH:/home/hexo-blog/node_modules/hexo/bin#重新加载当前用户的 bashrc 文件source ~/.bashrc\n\n// 启动博客服务root@5890065e0c87:/home/hexo-blog# hexo sINFO  Validating configINFO  Start processingINFO  Hexo is running at http://localhost:4000 . Press Ctrl+C to stop.","categories":["应用笔记"],"tags":["docker","hexo"]},{"title":"Java 8 ~ Java 21 的新特性","url":"/2023_12_30_java/","content":"学习必须往深处挖，挖的越深，基础越扎实！\nJava 现在发布的版本很快，每年两个，但是真正会被大规模使用的是 3 年一个的 LTS 版本。\n每 3 年发布一个 LTS（Long-Term Support），长期维护版本。意味着只有Java 8 ，Java 11， Java 17，Java 21 才可能被大规模使用。\n每年发布两个正式版本，分别是 3 月份和 9 月份。\n在 Java 版本中，一个特性的发布都会经历孵化阶段、预览阶段和正式版本。其中孵化和预览可能会跨越多个 Java 版本。所以在介绍 Java 新特性时采用如下这种策略：\n\n每个版本的新特性，都会做一个简单的概述。\n单独出文介绍跟编码相关的新特性，一些如 JVM、性能优化的新特性不单独出文介绍。\n孵化阶段的新特性不出文介绍。\n首次引入为预览特性、新特性增强、首次引入的正式特性，单独出文做详细介绍。\n影响比较大的新特性如果在现阶段没有转正的新特性不单独出文介绍，单独出文的重大特性一般都在 Java 21 版本之前已转为正式特性，例如：\n虚拟线程，Java 19 引入的，在 Java 21 转正，所以在 Java 19 单独出文做详细介绍\n作用域值，Java 20 引入的，但是在 Java 21 还处于预览阶段，所以不做介绍，等将来转正后会详细介绍\n\n\n\nJava 9 新特性JEP 261: 模块系统JEP 269: 集合工厂方法：新增只读集合和工厂方法JEP 222：REPL 工具：JSheel 命令JEP 213：接口支持私有方法Stream API 增强Optional 的增强改进 try-with-resourcesJEP 102：Process APIJEP 264：平台日志 API 和 服务JEP 266: 反应式流（Reactive Streams）JEP 224: HTML5 JavadocJEP 238: 多版本兼容 JAR 文件JEP 277：改进的弃用注解 @DeprecatedJEP 213：改进钻石操作符(Diamond Operator)增强 CompletableFutureJava 10 新特性JEP 286：局部变量类型推断JEP 304：统一的垃圾回收接口JEP 307：并行全垃圾回收器 G1JEP 310：应用程序类数据共享JEP 312：线程-局部变量管控JEP 313：移除 Native-Header 自动生成工具JEP 314：额外的 Unicode 语言标签扩展JEP 316：备用存储装置上的堆分配JEP 317：基于 Java 的 实验性 JIT 编译器JEP 319：根证书认证JEP 322：基于时间的版本发布模式新增 APIJava 11 新特性JEP 181: 基于嵌套的访问控制新增 String APIJEP 321：全新的 HTTP 客户端 APIJEP 323：局部变量类型推断的增强JEP 318：Epsilon—低开销垃圾回收器ZGC：可伸缩低延迟垃圾收集器JEP 335：废弃 Nashorn JavaScript 引擎增加 Files APIOptional API 增强JEP 328：飞行记录器（Flight Recorder）JEP 330：运行单文件源码程序JEP 320：删除 Java EE 和 corba 模块Java 12 新特性JEP 189：Shenandoah 垃圾收集器（预览特性）JEP 325：Switch 表达式（预览特性）JEP 334：JVM 常量 APIJEP 230：微基准测试套件（JMH）的支持新增 String API新增 Files API新增 NumberFormat API新增 Collectors APIJEP 340：移除多余ARM64实现JEP 341：默认CDS归档JEP 344：G1的可中断 mixed GCJava 13 新特性JEP 354：增强 Switch 表达式（第二次预览）JEP 355：文本块（预览特性）JEP 353：重构 Socket APIJEP 350：动态 CDSJEP 351：增强 ZGC 释放未使用内存Java 14 新特性JEP 361：表达式（正式特性）JEP 368：增强文本块（第二次预览）JEP 359：Records (预览)JEP 305：模式匹配的 instanceof（预览）JEP 358：改进 NullPointerExceptions 提示信息JEP 343：打包工具（孵化）JEP 345：NUMA-Aware 内存分配JEP 349：JFR Event StreamingJEP 364：macOS 上的 ZGC（实验性）JEP 365：Windows 上的 ZGC（实验性）JEP 366：弃用 ParallelScavenge + SerialOld GC 组合JEP 367：删除 Pack200 工具和 APIJEP 363：删除 CMS 垃圾收集器JEP 370：外部存储器访问 API（孵化器版）Java 15 新特性JEP 339：Edwards-Curve 数字签名算法 (EdDSA)JEP 360：密封的类和接口（预览）：Java 15 新特性—密封的类与接口JEP 371：隐藏类 Hidden Classes：Java 15 新特性—隐藏类JEP 372：移除 Nashorn JavaScript 引擎JEP 373：重新实现 DatagramSocket APIJEP 374：禁用偏向锁定JEP 375：模式匹配的 instanceof（第二次预览）JEP 377：ZGC—可伸缩低延迟垃圾收集器（正式特性）JEP 378：文本块（正式特性）JEP 379：Shenandoah—低暂停时间垃圾收集器（正式特性）JEP 381：移除 Solaris 和 SPARC 支持JEP 383：外部存储器访问 API （二次孵化器版）JEP 384：Record (第二次预览)JEP 385：废除 RMI 激活Java 16 新特性JEP 338：向量 API（孵化器）JEP 347：启用 C++14 语言特性JEP 357：将JDK的源代码库从Mercurial迁移到GitJEP 369：将JDK的源代码库托管到GitHubJEP 376：ZGC 并发线程处理JEP 380：Unix-Domain 套接字通道JEP 386：AlpineLinux 移植JEP 387：弹性元空间JEP 388：Windows&#x2F;AArch64 移植JEP 389：外部函数与内存 API（孵化器）JEP 390：对基于值的类发出警告JEP 392：打包工具（正式版）JEP 393：外部存储器访问 API（第三次孵化）JEP 394：instanceof 模式匹配（正式特性）JEP 395：Records (正式特性)JEP 396：默认强封装 JDK 内部元素JEP 397：密封类（第二预览）\nJava 17 新特性JEP 356：增强型伪随机数生成器：Java 17 新特性—增强型伪随机数生成器JEP 382：新的 macOS 渲染管线JEP 391：macOS&#x2F;AArch64 端口JEP 398：移除 Applet APIJEP 406：模式匹配的 Swith 表达式（预览）：Java 17 新特性—模式匹配的 Swith 表达式JEP 407：删除 RMI 激活JEP 409：密封类（正式特性）JEP 410：移除实验性的 AOT 和 JIT 编译JEP 411：废弃安全管理器JEP 412：外部函数与内存 API（第二次孵化）JEP 414：向量 API（第二次孵化）\nJava 18 新特性JEP 400：默认UTF-8编码JEP 408：简易Web服务器JEP 413：支持在 Java API 文档中加入代码片段JEP 416：用方法句柄重新实现核心反射JEP 417：向量 API（第三孵化器）JEP 418：互联网地址解析 SPIJEP 419：外部函数和内存 API（第三孵化器）JEP 420：模式匹配 Switch 表达式（预览）JEP 421：弃用 Finalization 功能\nJava 19 新特性JEP 405：Record模式（预览）：Java 19 新特性—Record模式JEP 422：JDK移植到Linux&#x2F;RISC-VJEP 424：外部函数和内存API（预览）JEP 425：虚拟线程（预览）：Java 19 新特性—虚拟线程JEP 426：向量API（第四次孵化）JEP 427：模式匹配的 Switch（第三次预览）JEP 428：结构化并发（孵化功能）\nJava 20 新特性JEP 429：作用域值（第一次孵化）JEP 432：Record 模式（第二次预览）JEP 433：模式匹配的 Switch 表达式（第四次预览）JEP 434：外部函数与内存 API（第二次预览）JEP 436：虚拟线程（第二次预览）JEP 437：结构化并发（第二次孵化）JEP 438：向量 API（第五次孵化）\nJava 21 新特性JEP 430：字符串模板 （预览）：Java 21 新特性—字符串模板JEP 431：有序集合：Java 21 新特性—有序集合JEP 439：分代 ZGCJEP 440：Record 模式JEP 441：switch 模式匹配JEP 442：外部函数和内存 API （第三次预览）JEP 443：未命名模式和变量 （预览）：Java 21 新特性—未命名模式和变量JEP 444：虚拟线程（正式特性）JEP 445：未命名类和 main 方法 （预览）：Java 21 新特性—未命名类和 main 方法JEP 446：作用域值 （预览）JEP 448：向量 API（第六次孵化）JEP 449：弃用 Windows 32 位 x86 端口JEP 451：准备禁止动态加载代理JEP 452：密钥封装机制 API  安全库JEP 453：结构化并发（预览）\n","categories":["总结笔记"],"tags":["java","java8","java9","java10"]},{"title":"Springboot自定义指标并使用Prometheus监控预警","url":"/2022_06_18_springboot_actuator/","content":"spring套件为我们提供了很多starter，其中spring-boot-starter-actuator支持指标采集，本文介绍使用Prometheus监控Spring Boot提供的默认指标，以及自定义业务指标，并使用Prometheus进行监控并报警，同时在 Grafana 进行展现\n","categories":["应用笔记"],"tags":["springboot","prometheus"]},{"title":"JAVA线程总结","url":"/2017_07_03_java_thread/","content":"什么是线程？线程是进程的一个实体，是CPU调度和分派的基本单位，它是比进程更小的能独立运行的基本单位。线程自己基本上不拥有系统资源，只拥有一点在运行中必不可少的资源(如程序计数器，一组寄存器和栈)，但是它可与同属一个进程的其他的线程共享进程所拥有的全部资源。\n有时候我们说线程是轻量级进程。就象进程一样，线程在程序中是独立的、并发的执行路径，每个线程有它自己的堆栈、自己的程序计数器和自己的局部变量。但是，与分隔的进程相比，进程中的线程之间的隔离程度要小。它们共享内存、文件句柄和其它每个进程应有的状态。\nJava语言是第一个在语言本身中显式地包含线程的主流编程语言，它没有把线程化看作是底层操作系统的工具。\n为什么要用线程？\n响应更快的 UI（如GUI中事件线程）\n利用多处理器系统（单处理器多线程本质是处理器的短时间间隔复用，多处理器多线程就可以高效使用多处理器系统）\n简化建模（比如五大常用算法中的分治法，求多维数组的排序问题，多线程可以简化建模）\n异步或后台处理（这个很常见，比如轮询套接字，异步响应请求，servlet请求等等）\n\n一个简单线程：一个计时线程案例\n/*** 一个线程案例： （计时功能）* 主线程开启一个打印素数的线程之后，主线程自己休息10秒，* 10秒之后通过改变finished状态值，break新线程。*/public class CalculatePrimes extends Thread&#123;    public static final int MAX_PRIMES = 1000000;    public static final int TEN_SECONDS = 10000;    public volatile boolean finished = false;    public void run()&#123;        int[] primes = new int[MAX_PRIMES];        int count = 0;        for (int i=2; count&lt;MAX_PRIMES; i++)&#123;            // Check to see if the timer has expired            if (finished) &#123;            break;            &#125;            boolean prime = true;            for (int j=0; j&lt;count; j++)&#123;                if (i % primes[j] == 0)&#123;                    prime = false;                    break;                &#125;            &#125;            if (prime)&#123;                primes[count++] = i;                System.out.println(&quot;Found prime: &quot; + i);            &#125;        &#125;    &#125;    public static void main(String[] args)&#123;        CalculatePrimes calculator = new CalculatePrimes();        calculator.start();        try &#123;            Thread.sleep(TEN_SECONDS);        &#125;        catch (InterruptedException e)&#123;            // fall through        &#125;        calculator.finished = true;    &#125;&#125;\n\n线程的使用方法生命周期5状态说起\n新建（new Thread）当创建Thread类的一个实例（对象）时，此线程进入新建状态（未被启动）。例如：Thread  t1&#x3D;new Thread();\n就绪（runnable）线程已经被启动，正在等待被分配给CPU时间片，也就是说此时线程正在就绪队列中排队等候得到CPU资源。例如：t1.start();\n运行（running）线程获得CPU资源正在执行任务（run()方法），此时除非此线程自动放弃CPU资源或者有优先级更高的线程进入，线程将一直运行到结束。\n死亡（dead）当线程执行完毕或被其它线程杀死，线程就进入死亡状态，这时线程不可能再进入就绪状态等待执行。自然终止：正常运行run()方法后终止异常终止：调用stop()方法让一个线程终止运行\n堵塞（blocked）由于某种原因导致正在运行的线程让出CPU并暂停自己的执行，即进入堵塞状态。正在睡眠：用sleep(longt)方法可使线程进入睡眠方式。一个睡眠着的线程在指定的时间过去可进入就绪状态。正在等待：调用wait()方法。（调用motify()方法回到就绪状态）被另一个线程所阻塞：调用suspend()方法。（调用resume()方法恢复）\n\n三种创建线程的方法1.继承Thread类创建线程类（1）定义Thread类的子类，并重写该类的run方法，该run方法的方法体就代表了线程要完成的任务。因此把run()方法称为执行体。（2）创建Thread子类的实例，即创建了线程对象。（3）调用线程对象的start()方法来启动该线程。\npublic class FirstThreadTest extends Thread&#123;    int i = 0;    //重写run方法，run方法的方法体就是现场执行体    public void run()&#123;        for(;i&lt;100;i++)&#123;        System.out.println(getName()+&quot;  &quot;+i);        &#125;    &#125;    public static void main(String[] args)&#123;        for(int i = 0;i&lt; 100;i++)&#123;            System.out.println(Thread.currentThread().getName()+&quot;  : &quot;+i);            if(i==20)&#123;                new FirstThreadTest().start();                new FirstThreadTest().start();            &#125;        &#125;    &#125;&#125;\n2.通过Runnable接口创建线程类（1）定义runnable接口的实现类，并重写该接口的run()方法，该run()方法的方法体同样是该线程的线程执行体。（2）创建 Runnable实现类的实例，并依此实例作为Thread的target来创建Thread，该Thread对象才是真正的线程对象。（3）调用线程对象的start()方法来启动该线程。\npublic class RunnableThreadTest implements Runnable&#123;    private int i;    public void run()&#123;        for(i = 0;i &lt;100;i++)&#123;            System.out.println(Thread.currentThread().getName()+&quot; &quot;+i);        &#125;    &#125;    public static void main(String[] args)&#123;        for(int i = 0;i &lt; 100;i++)&#123;            System.out.println(Thread.currentThread().getName()+&quot; &quot;+i);            if(i==20)&#123;                RunnableThreadTest rtt = new RunnableThreadTest();                new Thread(rtt,&quot;新线程1&quot;).start();                new Thread(rtt,&quot;新线程2&quot;).start();            &#125;        &#125;    &#125;&#125;\n3.通过Callable和Future创建线程（1）创建Callable接口的实现类，并实现call()方法，该call()方法将作为线程执行体，并且有返回值。（2）创建Callable实现类的实例，使用FutureTask类来包装Callable对象，该FutureTask对象封装了该Callable对象的call()方法的返回值。（3）使用FutureTask对象作为Thread对象的target创建并启动新线程。（4）调用FutureTask对象的get()方法来获得子线程执行结束后的返回值。\nimport java.util.concurrent.Callable;import java.util.concurrent.ExecutionException;import java.util.concurrent.FutureTask;public class CallableThreadTest implements Callable&lt;Integer&gt;&#123;    @Override    public Integer call() throws Exception&#123;        int i = 0;        for(;i&lt;100;i++)&#123;            System.out.println(Thread.currentThread().getName()+&quot; &quot;+i);        &#125;        return i;    &#125;    public static void main(String[] args)&#123;        CallableThreadTest ctt = new CallableThreadTest();        FutureTask&lt;Integer&gt; ft = new FutureTask&lt;&gt;(ctt);        for(int i = 0;i &lt; 100;i++)&#123;            System.out.println(Thread.currentThread().getName()+&quot; 的循环变量i的值&quot;+i);            if(i==20)&#123;                new Thread(ft,&quot;有返回值的线程&quot;).start();            &#125;        &#125;        try&#123;            System.out.println(&quot;子线程的返回值：&quot;+ft.get());        &#125; catch (InterruptedException e)&#123;            e.printStackTrace();        &#125; catch (ExecutionException e)&#123;            e.printStackTrace();        &#125;    &#125;&#125;\n采用实现Runnable、Callable接口的方式创见多线程时，优势是：线程类只是实现了Runnable接口或Callable接口，还可以继承其他类。在这种方式下，多个线程可以共享同一个target对象，所以非常适合多个相同线程来处理同一份资源的情况，从而可以将CPU、代码和数据分开，形成清晰的模型，较好地体现了面向对象的思想。劣势是：编程稍微复杂，如果要访问当前线程，则必须使用Thread.currentThread()方法。\n使用继承Thread类的方式创建多线程时优势是：编写简单，如果需要访问当前线程，则无需使用Thread.currentThread()方法，直接使用this即可获得当前线程。劣势是：线程类已经继承了Thread类，所以不能再继承其他父类。\n一些常见API在介绍生命周期的时候，我们已经接触了一些常见API,建议直接看API文档JDK7-API-java.lang.Thread\n/**常用API**///几毫秒加几纳秒之后调用线程将阻塞，直到目标线程完成为止.调用线程继续。void join();void join(long millis);void join(long millis,int nanos);//继承Object.导致线程进入等待状态，直到它被其他线程通过notify()或者notifyAll唤醒。该方法只能在同步方法中调用。如果当前线程不是锁的持有者，该方法抛出一个IllegalMonitorStateException异常。void wait();void wait(long millis);void wait(long millis,int nanos);//随机选择一个在该对象上调用wait方法的线程，解除其阻塞状态。该方法只能在同步方法或同步块内部调用。如果当前线程不是锁的持有者，该方法抛出一个IllegalMonitorStateException异常。void notify();//解除所有那些在该对象上调用wait方法的线程的阻塞状态。该方法只能在同步方法或同步块内部调用。如果当前线程不是锁的持有者，该方法抛出一个IllegalMonitorStateException异常。void notifyAll();/**wait、notify和notifyAll方法是Object类的final native方法。所以这些方法不能被子类重写，Object类是所有类的超类，因此在程序中有以下三种形式调用wait等方法。wait();//方式1：this.wait();//方式2：super.wait();//方式3**///将使当前线程进入等待状态，直到过了一段指定时间，或者直到另一个线程对当前线程的Thread对象调用了Thread.interrupt()，从而中断了线程。//当过了指定时间后，线程又将变成可运行的，并且回到调度程序的可运行线程队列中。//如果线程是由对 Thread.interrupt() 的调用而中断的，那么休眠的线程会抛出InterruptedException，这样线程就知道它是由中断唤醒的，就不必查看计时器是否过期。static void sleep(long millis);static void sleep(long millis, int nanos);//中断发起调用的线程。void interrupt();//就像sleep() 一样，但它并不引起休眠，而只是暂停当前线程片刻，这样其它线程就可以运行了。//在大多数实现中，当较高优先级的线程调用Thread.yield() 时，较低优先级的线程就不会运行。static void yield();//指明某个线程是守护程序线程。void setDaemon (boolean on)//启动线程void start();//结束线程void stop();\n\n关于守护线程大概是受到操作系统中守护进程的设计思路，在设计java线程的时候也同样的也有守护线程机制。\njava的线程分为两类：User Thread(用户线程)、Daemon Thread(守护线程)，其实User Thread线程和Daemon Thread守护线程本质上来说去没啥区别的，唯一的区别之处就在虚拟机的离开：如果User Thread全部撤离，那么Daemon Thread也就没啥线程好服务的了，所以虚拟机也就退出了。\nJava语言机制是构建在JVM的基础之上的，java内部的守护线程也存在与JVM中，比如GC线程。\n守护线程并非虚拟机内部可以提供，用户也可以自行的设定守护线程，方法：public final void setDaemon(boolean on) ；但是有几点需要注意：\n\nthread.setDaemon(true)必须在thread.start()之前设置，否则会跑出一个IllegalThreadStateException异常。你不能把正在运行的常规线程设置为守护线程。  （备注：这点与守护进程有着明显的区别，守护进程是创建后，让进程摆脱原会话的控制+让进程摆脱原进程组的控制+让进程摆脱原控制终端的控制；所以说寄托于虚拟机的语言机制跟系统级语言有着本质上面的区别）\n\n在Daemon线程中产生的新线程也是Daemon的。   （这一点又是有着本质的区别了：守护进程fork()出来的子进程不再是守护进程，尽管它把父进程的进程相关信息复制过去了，但是子进程的进程的父进程不是init进程，所谓的守护进程本质上说就是“父进程挂掉，init收养，然后文件0,1,2都是&#x2F;dev&#x2F;null，当前目录到&#x2F;”）\n\n不是所有的应用都可以分配给Daemon线程来进行服务，比如读写操作或者计算逻辑。因为在Daemon Thread还没来的及进行操作时，虚拟机可能已经退出了。\n\n\n例子：\nimport java.io.*;class TestRunnable implements Runnable&#123;    public void run()&#123;        try&#123;            Thread.sleep(1000);//守护线程阻塞1秒后运行            File f=new File(&quot;daemon.txt&quot;);            FileOutputStream os=new FileOutputStream(f,true);            os.write(&quot;daemon&quot;.getBytes());        &#125;catch(IOException e1)&#123;            e1.printStackTrace();        &#125;catch(InterruptedException e2)&#123;            e2.printStackTrace();        &#125;    &#125;&#125;public class TestDemo2&#123;    public static void main(String[] args) throws InterruptedException&#123;        Runnable tr=new TestRunnable();        Thread thread=new Thread(tr);        //thread.setDaemon(true);        thread.setDaemon(true); //设置守护线程        thread.start(); //开始执行分进程    &#125;&#125;\n运行结果：文件daemon.txt中没有”daemon”字符串。\n但是如果把thread.setDaemon(true); 注释掉，文件daemon.txt是可以被写入daemon字符串的。\nJVM判断程序是否执行结束的标准是所有的前台执线程行完毕了，而不管后台线程（守护线程）的状态，因此，在使用守护线程的时候一定要注意这个问题。\n举个例子，web服务器中的Servlet容器启动时后台初始化一个服务线程，即调度线程，负责处理http请求，然后每个请求过来调度线程从线程池中取出一个工作者线程来处理该请求，从而实现并发控制的目的。\n同步的问题为了确保可以在线程之间以受控方式共享数据，Java语言提供了两个关键字：synchronized 和 volatile。\nVolatile 只适合于控制对基本变量（整数、布尔变量等）的单个实例的访问。当一个变量被声明成volatile，任何对该变量的写操作都会绕过高速缓存，直接写入主内存，而任何对该变量的读取也都绕过高速缓存，直接取自主内存。这表示所有线程在任何时候看到的 volatile 变量值都相同，这保证了变量的一致性，但是如果要保护比较大的代码段还需要用Synchronized。\nSynchronized 同步：\n同步使用监控器（monitor）或锁的概念，以协调对特定代码块的访问。\n每个 Java 对象都有一个相关的锁。同一时间只能有一个线程持有 Java 锁。当线程进入synchronized代码块时，线程会阻塞并等待，直到锁可用，当它可用时，就会获得这个锁，然后执行代码块。当控制退出受保护的代码块时，即到达了代码块末尾或者抛出了没有在 synchronized 块中捕获的异常时，它就会释放该锁。\n这样，每次只有一个线程可以执行受给定监控器保护的代码块。从其它线程的角度看，该代码块可以看作是原子的，它要么全部执行，要么根本不执行。\n例子：\npublic class SyncExample &#123;    private static lockObject = new Object();    private static class Thread1 extends Thread &#123;        public void run() &#123;            synchronized (lockObject) &#123;                x = 0;                y = 0;                System.out.println(x);            &#125;        &#125;    &#125;    private static class Thread2 extends Thread &#123;        public void run() &#123;            synchronized (lockObject) &#123;                x = 1;                y = 1;                System.out.println(y);            &#125;        &#125;    &#125;    public static void main(String[] args) &#123;        new Thread1().run();        new Thread2().run();    &#125;&#125;\n使用 synchronized 块可以让您将一组相关更新作为一个集合来执行，而不必担心其它线程中断或看到计算的中间结果。以下示例代码将打印“10”或“01”。如果没有同步，它还会打印“1 1” 或“0 0”。\n以上是synchronized 块的原理，除此之外还可以同步一个方法：\npublic class Point &#123;    private x;private y;    public synchronized void setXY(int x, int y) &#123;    this.x = x;    this.y = y;    &#125;&#125;\n对于普通的 synchronized 方法，这个锁是一个对象，将针对它调用方法。对于静态 synchronized方法，这个锁是本对象，在该对象中声明了方法。仅仅因为 setXY() 被声明成 synchronized 并不表示两个不同的线程不能同时执行 setXY()，只要它们调用不同的 Point 实例的 setXY() 就可同时执行。对于一个 Point 实例，一次只能有一个线程执行 setXY()。\n示例：简单的线程安全的高速缓存：\npublic class SimpleCache&#123;    private final Map cache = new HashMap();    public Object load(String objectName)&#123;    // load the object somehow    &#125;    public void clearCache()&#123;        synchronized (cache)&#123;        cache.clear();        &#125;    &#125;    public Object getObject(String objectName) &#123;        synchronized (cache)&#123;            Object o = cache.get(objectName);            if (o == null)&#123;                o = load(objectName);                cache.put(objectName, o);            &#125;        &#125;    return o;    &#125;&#125;\n以上代码，使用 HashMap 为对象装入器提供了一个简单的高速缓存。load()方法知道怎样按对象的键装入对象。在一次装入对象之后，该对象就被存储到高速缓存中，这样以后的访问就会从高速缓存中检索它，而不是每次都全部地装入它。对共享高速缓存的每个访问都受到synchronized 块保护。由于它被正确同步，所以多个线程可以同时调用 getObject 和clearCache 方法，而没有数据损坏的风险。\n同步？不同步？什么时候必须同步？\n\n需要保证在多线程中，一部分数据是一致的即用于一致性的同步\n递增共享计数器（多线程共用一个计数器类或方法），本质还是一致性\nfinal字段是线程友好的，不必担心同步问题\n\n什么时候不需要同步？\n\n由静态初始化器（在静态字段上或 static{} 块中的初始化器）初始化数据时，JVM隐性的帮我们同步了\n访问final变量时\n死锁\n性能考虑\n\n同步准则？\n\n使代码块保持简短。Synchronized块应该简短,在保证相关数据操作的完整性的同时，尽量简短。把不随线程变化的预处理和后处理移出 synchronized 块\n不要阻塞。不要在 synchronized块或方法中调用可能引起阻塞的方法，如InputStream.read()\n在持有锁的时候，不要对其它对象调用方法。这听起来可能有些极端，但它消除了最常见的死锁源头。\n\n其他一些案例使用java.util.TimerTask解决计数器的问题这是上文的案例，我们可以不让主线程休眠，方法如下：\n/*** 一个线程案例： （计时功能）* 主线程开启一个打印素数的线程之后，主线程自己休息10秒，* 10秒之后通过改变finished状态值，break新线程。*/public class CalculatePrimes extends Thread&#123;    public static final int MAX_PRIMES = 1000000;    public static final int TEN_SECONDS = 10000;    public volatile boolean finished = false;    public void run()&#123;        int[] primes = new int[MAX_PRIMES];        int count = 0;        for (int i=2; count&lt;MAX_PRIMES; i++)&#123;            // Check to see if the timer has expired            if (finished) &#123;            break;            &#125;            boolean prime = true;            for (int j=0; j&lt;count; j++)&#123;                if (i % primes[j] == 0)&#123;                    prime = false;                    break;                &#125;            &#125;            if (prime)&#123;                primes[count++] = i;                System.out.println(&quot;Found prime: &quot; + i);            &#125;        &#125;    &#125;    public static void main(String[] args)&#123;        Timer timer = new Timer();        final CalculatePrimes calculator = new CalculatePrimes();        calculator.start();        timer.schedule(            new TimerTask() &#123;                public void run()&#123;                    calculator.finished = true;                &#125;            &#125;, TEN_SECONDS);    &#125;&#125;\nservlet 和 JavaServer Pages 技术实现 RMI 对象","categories":["总结笔记"],"tags":["java","多线程"]},{"title":"读《爱有8种习惯》","url":"/2021_05_01_read_8habitsoflove/","content":"我发现爱而不会的现象在身边发生的太多，究其本质，是主动爱的人总是站在自己的立场去爱（关心），其结果往往是自我感觉良好，亦或者是连自我感觉都不良好，轻者单身孤苦伶仃，重者众叛亲离（言过其实O(∩_∩)O）。\n\n\n爱的8种习惯分别是慷慨、静默、求真、坦诚、游戏、宽恕、慈悲，和社群。\n慷慨的习惯人类的精神就像大海，既需要河水流入，也需要河水流出，这样才能孕育生命，产生能量。\n当爱从我们心中流出时，更多的爱会流入我们的心中。\n当我恩向爱敞开心扉时，我们不仅将这种传递给了他人，也能接受到其他人给予的爱。\n流出决定了流入。我们给予得越多，我们的人生就越有活力，精神就越强大，生命就越深刻。\n每个慷慨的举动都是在给予祝福。\n在祝福他人时，某种力量就被释放出来了这种力量能够穿透并挫败源于恐惧的抗拒。\n从外表看，它或许没有立竿见影的效果，\n但是，被祝福着的内心却被触动，变化已然发生。\n如何践行慷慨\n\n每日制作感恩清单，列出今天需要感恩的五件事，与他人分享。\n列出让你感到害怕的人际关系。贬低或不赞同或失望。以祝福之心尝试着拜访他们。\n每一个会面结束之时，让每个人都表达自己的感激和遗憾。\n尽可能地捐赠行善。如果你将收入的10%捐赠，剩余的90%就将派上更大的用场。\n不要将自己的善意只限定在每年的特殊时刻，随时向周围的人表达你的爱。\n\n心怀感恩之情。多想一想过去或现在让你心怀感恩的人们。引导自己多做善举。\n静默的习惯恐惧常常以忙乱和疲劳为食\n生活的压力，紧要的事情，基本上都是以自我为中心的，\n因为涉及许多我们必须承担和履行的责任。\n而当我们进入静默中，我们就能找到自我，超越自我，更轻松承受这些日常生活的压力。\n取而代之的，是无言的信心。\n由于现在我们和心灵深处的爱之源泉联系起来了，我们会有足够的本领来解决出现的问题。\n静默既是身体的体验，也是心灵和灵性的状态。\n当我们的肌肉在静默中放松下来时，我们身体内在状态也会发生变化，从紧张，激动变得清醒，清凉。\n我们的呼吸节奏也会变得平静而从容。\n我们的心灵，也初见下沉，到充满动荡，混乱的人生睡眠的下方，所有的自我特征在深处汇聚，与我们的灵魂融合，我们的灵魂是爱的居所，是我们内心中的至爱者。\n如何修习静默\n\n反复尝试，找到属于自己体验静默的方式。凝望大海，或静坐，或绘画创作。\n安静舒适地让思绪尽情徜徉。回想人生中不同的时光和往事。\n每天至少给自己十分钟时间来修习静默，到真正变成安全根基的习惯。\n尝试在每天不同时刻练习，以确定最佳练习时间。\n\n求真的习惯生活不是戏剧，我们不会收到能够帮助我们做出抉择和摆脱困境的剧本和文件，\n然而求真的习惯能够帮我们认识到，哪些言语，举止，行为涉及到我们的名字。\n真理不会追随我们，而我们必须追随真理，真理并不听从给我们的计划，相反它有时给到我们冲击和阻力，但是如果我们真诚地相信自己在追随真理，那么我们就能相信，我们也能驾驭随后可能出现的任何问题，这恰恰是信仰的本质。\n求真过程中，最棘手的问题就在于，真理是个观点问题，它不是一个事实问题。\n在聆听真理对我们的呼唤时，我们首先必须履行自身责任，而不是坚持让其他人用完全相同的方式来回答这个问题。有时我们需要接受分歧，知道怎样最好接纳分歧。这可能意味着离弃我们所珍爱的人，也可能意味着接纳真理观念不同于自己的人。\n真理不是成套的想法，不能被完全包含在一个人的头脑，言语，概念或想法当中，它过于活泼广阔，不受人类控制。真理是不断发展的，是我们对自身和世界运转方式的一系列洞见，理解，启示和顿悟。真理总是让我们能够更好地理解自己和他人。\n如何实践真理\n\n每日修习静默\n以简短开放的句子表述思考的内容。\n记住实践爱的习惯的最终目标是意识到至爱者多么深切地真爱着你和所有其他人。\n识别思考中的冲突是来自于内在的，家人的，朋友的，或是爱人的，外在的。\n思考比起背叛真实的自我，他人的谴责和阻力是否更强大更可怕\n思考信心，平安，喜乐，忍耐，恩慈，自律，宽容，大度在其中的位置和影响力\n结交有类似经验的友人聆听他们的故事。\n意识到没有真理能够让你完全脱离恐惧。\n了解到践行真理有时似乎是危险的，令人敬而远之的，然而当它引导我们觉悟上升到新的层次时，最终构建的会是平安，坚不可摧的堡垒。接纳那个爱的自我。\n\n坦诚的习惯坦诚的做法实际上是关爱之举，关心自己和他人，以及彼此的交情。坦诚也是充满爱与信念的举动，使人际关系拥有更坚实的基础，与此同时，勇敢而坦诚的对话能够让彼此关系更丰富，更深刻，更持久，经受各种挑战。有些时候，你可能觉得坦诚是徒然的，事与愿违的，但是必须记住，坦诚的习惯，是一个过程，是一种生活方式，不是终点。\n在坦诚时如何表达可能会让对方失控，愤怒或产生戒备心理的看法，是一种挑战，也是技巧。当我们与他人意见分歧时，坦诚会让我们接纳并珍惜这段宝贵而持久的人际关系，如果我们能够分享不同的看法并尊重彼此，这是我们能够给予他人的最大赞美。\n通常情况下，在坦诚的交流中，互动的成功与否可能会取决于很多外在因素，例如时间和地点，在不带偏见的场所，双方都冷静的时候，这种勇敢的谈话能够更有成效。但是即使我们采取了所有能够想到的预防措施，心态开明并有宽宏大量的精神，交流仍然可能导致感情受到伤害。我们没有办法来决定或引导别人如何回应我们的行为。这时候怀着爱的态度来真诚践行坦诚的习惯，本身就已经足够。\n如何实践坦诚\n\n在准备实践坦诚时，先自问一些先觉性问题以确定事实是需要我们去践行爱而非恐惧。\n在坦诚对待他人之前，要先修习静默。\n清醒地意识到自己的动机。是存在的建设性的，积极的或者是报复性的，消极的。\n坦诚过程中需要在存在的层面上高度尊重他人，批评就事论事不针对人。\n坦诚之前找出对方存在和行为中的可取之处。列出清单。\n无法坦诚时，不强迫。只有不执着于结果时，坦诚才充满了爱意。\n坦诚前谨慎奠定沟通基础。询问对方是否有足够的时间，心情，以及明确温和表示重要性。\n坦诚前表明彼此配合的重要性。\n沟通时，以自身，对方，彼此之间的关系三个角度来时刻梳理和自省。\n需要给对方留出时间，来对你的话作出回应。11.充满自信并保持静默。消除戒备心理。你已深思熟虑，你的意图是好的。要忠诚于那个渴望爱和被爱的自我。竭尽全力，就可以无怨无悔。\n\n游戏的习惯焦虑会让我们无法跟随生活的节奏，让我们无法接触充满爱的自我。每一种爱的习惯都需要我们认真参与，但游戏要求我们区分这种认真和致命的严肃。严肃会让我们以为自己所担忧的某个问题是整个宇宙唯一迫切的问题。把自己太当回事，就会无法从创造性的角度看待问题和寻求解决问题的办法。除了让我们害怕的利害关系之外，我们会丧失对其他所有事物的兴趣。这是胆怯。而当我们出于爱而行动，就更容易让游戏进入生活，并将受益于随之而来的无忧无虑和灵活变通。\n游戏的习惯能真正改变我们大脑的化学物质，让我们得到释放，发挥我们的想象力，变得更有创造性，有建设性，快乐。游戏使我们可以给予自己以及他人最有爱心的礼物之一。通过它，我们变得更有创意，神清气爽，不沉闷呆板。其他人可以更轻松接近我们，也使我们的观点也变得更深刻。\n如何实践游戏\n\n以学习者的心态来面对生活就可以更自然地实践游戏的精神。\n留意并尊重身体所传达的讯息。了解身上恐惧与爱的力量对比。\n紧张陌生的场合，可以考虑融入游戏元素放松心情。但是要专注于自己。\n练习游戏的精神。学习一两个笑话，学习幽默的心态。\n花时间与孩子相处，观察他们并向他们学习。\n犯下错误的时候，幽默承认。没有人可以完美到任何时候都不犯错。\n想想可以通过哪些方法把游戏融入到工作之中。你的日常就会有更多的喜悦和创造力。\n有意识地多花时间和幽默诙谐的朋友相处。\n\n宽恕的习惯宽恕的习惯更多设计受难者，而不是引起痛苦的人，事和情况。我们常常很难接受宽恕的概念，因为我们相信冒犯我们的人才是问题的根源。我们怎么能原谅肇事逃逸的司机，虐待我们的配偶，或是疏忽的外科医生呢？宽恕的习惯给予我们的礼物就是，它避免让施害者的罪遮盖了受害者的自由和权利。最终，宽恕更多涉及受害者的痊愈和自由。而不是施害者。\n当我们经历宽恕时，有爱的强大能量被释放出来，穿透全身。在我们实现宽恕之前，那些现实和想象中的伤害无比痛苦悲伤混乱地纠缠我们的身心。但当我们最终原谅的时候，淤塞物边会融化，生命之河又可以重新流淌。这种能量变成了爱，光明和清醒，我们的灵魂就好像从囚禁中解脱。\n通过宽恕的习惯，我们会面对人生旅程中及其艰巨的事情，在经历重大伤痛以后，回归自由，完整，心灵开放的自我。我们关注那些让我们遭受蔑视，羞辱，不公，暴力或威胁的时刻。这些伤害让我们内心恐惧，害怕不公得不到纠正。这种伤害根于我们的内心，成为痛苦，愤怒和怨恨的根源，让我们陷入充满恐惧的自我之中。这种生活会让我们心力交瘁，让我们遭受的痛苦在心中持续下去。我们需要思考，我们是继续停留在过去，还是利用宽恕的力量来继续生活，走向自由和爱。我们陷越深，就越可能诱使他人也陷入留存在我们心里的伤害之中。至爱者希望我们是条河，而不是个死谭。\n宽恕真的仅仅是某种自我治愈和自强自立的行为。\n而当实在无法彻底原谅那些伤害我们的人时，只要做出宽恕时可取的决定便可以。尽管我们无法宽恕，但是我们可以选择宽恕。至爱者的强大力量将满足你宽恕别人的愿望，帮助你踏上治疗之旅。\n如何实践宽恕\n\n修习静默是宽恕的前提条件。这个过程可能会很困难，你需要持之以恒地坚持。\n列一个宽恕清单。把清单像杂志一样保存在安全的地方。最终目标是不再需要这张清单。\n从名单中挑选一个人，用自己的心灵之眼观察TA。想想自己沐浴在治疗之光中。\n制定一份宽恕声明，再三念给自己听。关键是要放下自己的怨恨。不诅咒他人。\n记住你们可以不必再是朋友，也不必再以任何形式进行交往。放下蚕食心灵的旧关系。\n寻找能够帮助你保护自己的合适人选。这个过程关乎自我治愈和解脱，你必须有安全感。\n让爱的力量战胜恐惧的力量。\n\n慈悲的习惯当我们实践慈悲的习惯，就选择了为他人映照出他们自身再也无法看到的东西。所有人心中都有神圣的至爱者之光，每个人，无论我们看起来多么平凡，出色或古怪，都远远不是我们在生活中所做事情的总和。如果相信所有人本质上都是善良的我们必然会做出这样的推论，我们是为毫不人道或伤天害理的行为，体现了作恶者的痛苦和疾病。\n虽然慈悲并非最难理解的爱的习惯，但是却可能是最难以接受并付诸实践的习惯。它要求我们处于这样的信念而行动，即每个人本质上都是善良的。对于许多人来说，这似乎是不可能完成的。\n宽恕将人引向慈悲。在宽恕中我们重点关注别人对我们做过的事情。而在慈悲中，我们专注于别人可能对我们和其他人所做的事情。当我们能超越他人的伤害性行为，洞察到潜藏在阴影深处的善良时，这将有助于我们揭示那种善良，激发那个人重新觉悟到充满爱的自我。在实践慈悲的习惯时，我们是在馈赠他人。这并不意味着我们要纵容令人发指的行为或淡化这些罪行的严重性。在公正的社会必须有法治问责。这表明，我们知道肇事者也在承受痛苦，而且相信他们内心最深处仍然有善良和光明。他们并没有丧失全部的良知。在非常复杂的情况下，我们通过扩展慈悲心，帮助那个人再次扎到那个爱与被爱的自我。当我们这么做时，是在正式痊愈的可能性。\n虽然我们有时候可能很难产生慈悲心，当我们受到伤害或感到恐惧时尤其如此，但我们可以学会将其纳入我们的日常活动之中。不需要立刻就做到慈悲，重要的是，我们要逐渐前进。只要我们希望将慈悲的能量给予别人，克服那个充满恐惧的自我阻力和狡辩，我们就已经踏上了慈悲的征程。\n如何实践慈悲\n\n花时间静默，回忆你接受他人慈悲的时刻。\n如果无法做到，就向至爱者或宇宙的精神或更高的力量敞开自己的心灵。请求感受到从出生之处就具有的美善。\n参加能够导致自我祝福的活动。寻求承受者从我的祝福的礼物中再次绽放的状态。\n参加静修。由静修老师指导。\n当逐渐相信的时候，就可以将他给予别人了，从最亲近的人开始。慢慢扩大范围。\n第一阶段慈悲后，可以逐渐挑战那些我们认为不配，残忍，很坏的人。达到真正慈悲。\n始终相信每个人都对自己人生家庭充满希望和梦想。\n\n社群的习惯当与他人隔绝时，我们只不过是一粒微笑的沙子。轻易就能被吹走。在我们选择的社区中，我们成了岩石。能够勇敢面对大海的怒涛。直接影响一个人的事物，会间接影响到所有人。人不能离群索居，群体能够克服其他习惯无法克服的恐惧—最核心的恐惧，即：我们本质上都孤独地活在这个世界上。\n我们需要社群给我们勇气，激励我们去改变，要求我们承担责任，不管这种社群是我们采用什么形式找到或创建的，我们不能独自渡过难关。也不想独自度过美好时光。与他人分享我们的痛苦和喜悦有助于让我们远离恐惧本性中的黑暗，接纳我们心中始终存在的爱与被爱的自我。社交有助于我们敞开心灵。\n然而团队并不意味着盲从，真正的团队能够包容意见，甚至鼓励不同意见和个性存在。当拥有独立思考的人组成齐心协力的团体，欣赏和肯定相互之间的分歧并且不会因此相互排斥时，就形成了健康的社群。\n我们终生都需要远离与世隔绝和从众思维。走向温暖和力量的群社。\n如何实践社群\n\n自觉选择与那些充满爱和正面能量的人共处，而不是那些排斥不同意见或喜好说教的人共处。\n留意家庭，生意场，社交网络中那些喜欢将人分为三六九等并且加以褒贬的人。\n真诚寻找社群成员，并了解社群在他们的生活中所扮演的角色。\n以梦想或兴趣爱好的方式寻找社群。\n练习静默。\n在社区活动中慷慨地对待别人。\n尊重每个人的差异性，让每个人都可以分享至爱者的光明。\n当我们向丰盈之爱敞开心扉之时，我们的生命就会变得更加美好，世界也将变得更为公平公正。让我们放下积怨，放下对自己或他人的责备，通过赞美并分享已然安住在我们心中的爱，让自己摆脱深受其苦的恐惧，愤怒和悲伤。在内心找到勇敢而充满创造力的声音，告别索然无味的沉闷生活，开始充满活力的欢快生活。\n\n","categories":["读书笔记"],"tags":["沟通","爱"]},{"title":"JVM知识点汇总","url":"/2021_10_15_jvm/","content":"JVM 知识点汇总\n\n\n内存模型：程序计数器、方法区、堆、栈、本地方法栈的作用，保存哪些数据。\n类加载：双亲委派的加载机制，以及常用类加载器分别加载哪种类型的类。\nGC：分代回收的思想和依据，以及不同垃圾回收算法实现的思路、适合的场景。 \n性能调优：常用的 JVM 优化参数的作用，参数调优的依据，常用的 JVM 分析工具能分析哪类问题，以及使用方法。\n执行模式：解释、编译、混合模式的优缺点，Java7 提供的分层编译技术。需要知道 JIT 即时编译技术和 OSR（栈上替换），知道C1、C2编译器针对的场景，其中 C2针对 Server模式，优化更激进。在新技术方面可以了解Java10 提供的由 Java 实现的 Graal 编译器。\n编译优化：前端编译器javac 的编译过程、AST 抽象语法树、编译期优化和运行期优化。编译优化的常用技术包括公共子表达式的消除、方法内联、逃逸分析、栈上分配、同步消除等。明白了这些才能写出对编译器友好的代码。\n\nJVM 内存模型JVM 内存模型主要指运行时的数据区，包括 5 个部分，如下图所示。\n   \n\n\n\n栈也叫方法栈，是线程私有的，线程在执行每个方法时都会同时创建一个栈帧，用来存储局部变量表、操作栈、动态链接、方法出口等信息。调用方法时执行入栈，方法返回时执行出栈。\n本地方法栈与栈类似，也是用来保存线程执行方法时的信息，不同的是，执行 Java 方法使用栈，而执行 native 方法使用本地方法栈。\n程序计数器保存着当前线程所执行的字节码位置，每个线程工作时都有一个独立的计数器。程序计数器为执行Java方法服务，执行 native 方法时，程序计数器为空。\n栈、本地方法栈、程序计数器这三个部分都是线程独占的。\n堆是 JVM 管理的内存中最大的一块，堆被所有线程共享，目的是为了存放对象实例，几乎所有的对象实例都在这里分配。当堆内存没有可用的空间时，会抛出 OOM 异常。\n根据对象存活的周期不同，JVM 把堆内存进行分代管理，由垃圾回收器来进行对象的回收管理。\n方法区也是各个线程共享的内存区域，又叫非堆区。用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据，JDK 1.7 中的永久代和JDK 1.8 中的 Metaspace 都是方法区的一种实现。\n试回答这个问题时，要答出两个要点：一个是各部分的功能，另一个是哪些线程共享，哪些独占。\n\nJMM 内存可见性JMM 是 Java 内存模型，与 JVM 内存模型是两回事，JMM 的主要目标是定义程序中变量的访问规则，如下图所示，所有的共享变量都存储在主内存中共享。每个线程有自己的工作内存，工作内存中保存的是主内存中变量的副本，线程对变量的读写等操作必须在自己的工作内存中进行，而不能直接读写主内存中的变量。\n\n\n\n在多线程进行数据交互时，例如线程 A 给一个共享变量赋值后，由线程 B 来读取这个值，A 修改完变量是修改在自己的工作区内存中，B 是不可见的，只有从 A 的工作区写回主内存，B 再从主内存读取自己的工作区才能进行进一步的操作。由于指令重排序的存在，这个写—读的顺序有可能被打乱。因此 JMM 需要提供原子性、可见性、有序性的保证。\n\n详解JMM保证如下图所示，来看JMM如何保证原子性、可见性，有序性。\n \n\n原子性JMM 保证对除 long 和 double 外的基础数据类型的读写操作是原子性的。另外关键字 synchronized 也可以提供原子性保证。synchronized 的原子性是通过 Java 的两个高级的字节码指令 monitorenter 和 monitorexit 来保证的。\n可见性JMM可见性的保证，一个是通过 synchronized，另外一个就是 volatile。volatile 强制变量的赋值会同步刷新回主内存，强制变量的读取会从主内存重新加载，保证不同的线程总是能够看到该变量的最新值。\n有序性对有序性的保证，主要通过 volatile 和一系列 happens-before 原则。volatile 的另一个作用就是阻止指令重排序，这样就可以保证变量读写的有序性。\nhappens-before原则包括一系列规则，如：\n程序顺序原则，即一个线程内必须保证语义串行性；\n锁规则，即对同一个锁的解锁一定发生在再次加锁之前；\nhappens-before原则的传递性、线程启动、中断、终止规则等。\n\n类加载机制类的加载指将编译好的 Class 类文件中的字节码读入内存中，将其放在方法区内并创建对应的 Class 对象。类的加载分为加载、链接、初始化，其中链接又包括验证、准备、解析三步。如下图所示。\n  \n\n\n加载是文件到内存的过程。通过类的完全限定名查找此类字节码文件，并利用字节码文件创建一个 Class 对象。\n验证是对类文件内容验证。目的在于确保 Class 文件符合当前虚拟机要求，不会危害虚拟机自身安全。主要包括四种：文件格式验证，元数据验证，字节码验证，符号引用验证。\n准备阶段是进行内存分配。为类变量也就是类中由 static 修饰的变量分配内存，并且设置初始值。这里要注意，初始值是 0 或者null，而不是代码中设置的具体值，代码中设置的值是在初始化阶段完成的。另外这里也不包含用 final 修饰的静态变量，因为 final 在编译的时候就会分配。\n解析主要是解析字段、接口、方法。主要是将常量池中的符号引用替换为直接引用的过程。直接引用就是直接指向目标的指针、相对偏移量等。\n初始化，主要完成静态块执行与静态变量的赋值。这是类加载最后阶段，若被加载类的父类没有初始化，则先对父类进行初始化。\n只有对类主动使用时，才会进行初始化，初始化的触发条件包括在创建类的实例时、访问类的静态方法或者静态变量时、Class.forName() 反射类时、或者某个子类被初始化时。\n如上图所示，浅绿的两个部分表示类的生命周期，就是从类的加载到类实例的创建与使用，再到类对象不再被使用时可以被 GC 卸载回收。这里要注意一点，由 Java 虚拟机自带的三种类加载器加载的类在虚拟机的整个生命周期中是不会被卸载的，只有用户自定义的类加载器所加载的类才可以被卸载。\n\n详解类加载器\n\n\n如上图所示，Java 自带的三种类加载器分别是：BootStrap 启动类加载器、扩展类加载器和应用加载器（也叫系统加载器）。图右边的桔黄色文字表示各类加载器对应的加载目录。启动类加载器加载 java home 中 lib 目录下的类，扩展加载器负责加载 ext 目录下的类，应用加载器加载 classpath 指定目录下的类。除此之外，可以自定义类加载器。\n\n的类加载使用双亲委派模式，即一个类加载器在加载类时，先把这个请求委托给自己的父类加载器去执行，如果父类加载器还存在父类加载器，就继续向上委托，直到顶层的启动类加载器，如上图中蓝色向上的箭头。如果父类加载器能够完成类加载，就成功返回，如果父类加载器无法完成加载，那么子加载器才会尝试自己去加载。如图中的桔黄色向下的箭头。\n\n这种双亲委派模式的好处，可以避免类的重复加载，另外也避免了 Java 的核心 API 被篡改。\n\n\n详解分代回收Java 的堆内存被分代管理，为什么要分代管理呢？分代管理主要是为了方便垃圾回收，这样做基于2个事实，第一，大部分对象很快就不再使用；第二，还有一部分不会立即无用，但也不会持续很长时间。\n虚拟机划分为年轻代、老年代、和永久代，如下图所示。\n  \n\n\n年轻代主要用来存放新创建的对象，年轻代分为 Eden 区和两个 Survivor 区。大部分对象在 Eden 区中生成。当 Eden 区满时，还存活的对象会在两个 Survivor 区交替保存，达到一定次数的对象会晋升到老年代。\n老年代用来存放从年轻代晋升而来的，存活时间较长的对象。\n永久代，主要保存类信息等内容，这里的永久代是指对象划分方式，不是专指 1.7 的 PermGen，或者 1.8 之后的 Metaspace。\n\n根据年轻代与老年代的特点，JVM 提供了不同的垃圾回收算法。垃圾回收算法按类型可以分为引用计数法、复制法和标记清除法。\n\n引用计数法是通过对象被引用的次数来确定对象是否被使用，缺点是无法解决循环引用的问题。\n复制算法需要 from 和 to 两块相同大小的内存空间，对象分配时只在 from 块中进行，回收时把存活对象复制到 to 块中，并清空 from 块，然后交换两块的分工，即把 from 块作为 to 块，把 to 块作为 from 块。缺点是内存使用率较低。\n标记清除算法分为标记对象和清除不在使用的对象两个阶段，标记清除算法的缺点是会产生内存碎片。\n\nJVM 中提供的年轻代回收算法 Serial、ParNew、Parallel Scavenge 都是复制算法，而 CMS、G1、ZGC 都属于标记清除算法。\n详解 CMS 算法基于分代回收理论，详细介绍几个典型的垃圾回收算法，先来看 CMS 回收算法。CMS 在 JDK1.7 之前可以说是最主流的垃圾回收算法。CMS 使用标记清除算法，优点是并发收集，停顿小。\nCMS 算法如下图所示。\n  \n\n\n第一个阶段是初始标记，这个阶段会 stop the world，标记的对象只是从 root 集最直接可达的对象；\n第二个阶段是并发标记，这时 GC \n第三个阶段是重新标记阶段，这个阶段是第二个 stop the world 的阶段，停顿时间比并发标记要小很多，但比初始标记稍长，主要对对象进行重新扫描并标记；\n第四个阶段是并发清理阶段，进行并发的垃圾清理\n最后一个阶段是并发重置阶段，为下一次 GC 重置相关数据结构。\n\n详解 G1 算法G1 在 1.9 版本后成为 JVM 的默认垃圾回收算法，G1 的特点是保持高回收率的同时，减少停顿。\nG1 算法取消了堆中年轻代与老年代的物理划分，但它仍然属于分代收集器。G1 算法将堆划分为若干个区域，称作 Region，如下图中的小方格所示。一部分区域用作年轻代，一部分用作老年代，另外还有一种专门用来存储巨型对象的分区。\n\n\nG1 也和 CMS 一样会遍历全部的对象，然后标记对象引用情况，在清除对象后会对区域进行复制移动整合碎片空间。\nG1 回收过程如下。\n\nG1 的年轻代回收，采用复制算法，并行进行收集，收集过程会 STW。\nG1 的老年代回收时也同时会对年轻代进行回收。主要分为四个阶段：\n依然是初始标记阶段完成对根对象的标记，这个过程是STW的\n并发标记阶段，这个阶段是和用户线程并行执行的；\n最终标记阶段，完成三色标记周期;\n复制&#x2F;清除阶段，这个阶段会优先对可回收空间较大的 Region 进行回收，即 garbage first，这也是 G1 名称的由来。\n\n\n\nG1 采用每次只清理一部分而不是全部的 Region 的增量式清理，由此来保证每次 GC 停顿时间不会过长。\n总结如下，G1 是逻辑分代不是物理划分，需要知道回收的过程和停顿的阶段。此外还需要知道，G1 算法允许通过 JVM 参数设置 Region 的大小，范围是 1～32MB，可以设置期望的最大 GC 停顿时间等。有兴趣读者也可以对 CMS 和 G1 使用的三色标记算法做简单了解。\n详解 ZGC算法ZGC特点ZGC 是最新的 JDK1.11 版本中提供的高效垃圾回收算法，ZGC 针对大堆内存设计可以支持 TB 级别的堆，ZGC 非常高效，能够做到 10ms 以下的回收停顿时间。\n这么快的响应，ZGC 是如何做到的呢？这是由于 ZGC 具有以下特点。\n\nZGC 使用了着色指针技术，我们知道 64 位平台上，一个指针的可用位是 64 位，ZGC 限制最大支持 4TB 的堆，这样寻址只需要使用 42 位，那么剩下 22 位就可以用来保存额外的信息，着色指针技术就是利用指针的额外信息位，在指针上对对象做着色标记\n第二个特点是使用读屏障，ZGC 使用读屏障来解决 GC 线程和应用线程可能并发修改对象状态的问题，而不是简单粗暴的通过 STW 来进行全局的锁定。使用读屏障只会在单个对象的处理上有概率被减速。\n由于读屏障的作用，进行垃圾回收的大部分时候都是不需要 STW 的，因此 ZGC 的大部分时间都是并发处理，也就是 ZGC 的第三个特点。\n第四个特点是基于 Region，这与 G1 算法一样，不过虽然也分了 Region，但是并没有进行分代。ZGC 的 Region 不像 G1 那样是固定大小，而是动态地决定 Region 的大小，Region 可以动态创建和销毁。这样可以更好的对大对象进行分配管理。\n第五个特点是压缩整理。CMS 算法清理对象时原地回收，会存在内存碎片问题。ZGC 和 G1 一样，也会在回收后对 Region 中的对象进行移动合并，解决了碎片问题。\n\n虽然 ZGC 的大部分时间是并发进行的，但是还会有短暂的停顿。来看一下 ZGC 的回收过程。\nZGC 回收过程如下图所示，使用 ZGC 算法进行回收，从上往下看。初始状态时，整个堆空间被划分为大小不等的许多 Region，即图中绿色的方块。\n\n\n开始进行回收时，ZGC 首先会进行一个短暂的 STW，来进行 roots 标记。这个步骤非常短，因为 roots 的总数通常比较小。\n然后就开始进行并发标记，如上图所示，通过对对象指针进行着色来进行标记，结合读屏障解决单个对象的并发问题。其实，这个阶段在最后还是会有一个非常短的 STW 停顿，用来处理一些边缘情况，这个阶段绝大部分时间是并发进行的，所以没有明显标出这个停顿。\n下一个是清理阶段，这个阶段会把标记为不在使用的对象进行回收，如上图所示，把橘色的不在使用的对象进行了回收。\n最后一个阶段是重定位，重定位就是对 GC 后存活的对象进行移动，来释放大块的内存空间，解决碎片问题。\n重定位最开始会有一个短暂的 STW，用来重定位集合中的 root 对象。暂停时间取决于 root 的数量、重定位集与对象的总活动集的比率。\n最后是并发重定位，这个过程也是通过读屏障，与应用线程并发进行的。\n面试考察点\n深入了解 JVM 的内存模型和 Java 的内存模型；\n要了解类的加载过程，了解双亲委派机制；\n要了解常用的 GC 算法的特点、执行过程，和适用场景，例如 G1 适合对最大延迟有要求的场合，ZGC 适用于 64 位系统的大内存服务中\n要了解常用的 JVM 参数，明白对不同参数的调整会有怎样的影响，适用什么样的场景，例如垃圾回收的并发数、偏向锁设置等。\n\n加分项如果想要给面试官留下更好的印象，注意这些加分项。\n\n如果在编译器优化方面有深入的了解的话，会让面试官觉得你对技术的深度比较有追求。例如知道在编程时如何合理利用栈上分配降低 GC 压力、如何编写适合内联优化等代码等。\n如果你能有线上实际问题的排查经验或思路那就更好了，面试官都喜欢动手能力强的同学。例如解决过线上经常 FullGC 问题，排查过内存泄露问题等。\n如果能有针对特定场景的 JVM 优化实践或者优化思路，也会有意想不到的效果。例如针对高并发低延迟的场景，如何调整 GC 参数尽量降低 GC 停顿时间，针对队列处理机如何尽可能提高吞吐率等；\n如果对最新的 JVM 技术趋势有所了解，也会给面试官留下比较深刻的印象。例如了解 ZGC 高效的实现原理，了解 Graalvm 的特点等。\n\n真题汇总总结 JVM 相关的面试真题，第一部分真题如下，课后可以重点练习。\n\n解题思路如下所示。\n\n\n第 1 题 Java 内存模型前面讲过，面试时回答这个问题时记得和面试官确认是希望回答 JVM 的内存模型，还是 Java 对内存访问的模型，不要答跑偏。\n第 2 题要复习一下什么场景下会触发 FullGC，例如年轻代晋升时老年代空间不足，例如永久代空间不足等。\n第 3～6 题前面已经有过讲解，因此不再重复。\n\n第二部分真题如下所示。\n\n\n解题思路如下所示。\n\n第 7 题 volatile 要重点回答强制主内存读写同步以及防止指令重排序两点。\n第 8、9 题前面已经讲过\n第 10 题重点介绍出强、弱、软、虚四种引用，以及在 GC 中的处理方式。\n第 11 题可以了解一下 Java 自带的几种工具的功能，例如 JMC 中的飞行记录器，堆分析工具 MAT，线程分析工具 jstack 和获取堆信息的 jmap 等。\n\n","categories":["总结笔记"],"tags":["java","jmm","jvm"]},{"title":"Java并发与多线程总结","url":"/2021_10_18_multity_thread/","content":"本文的主要内容是 Java 的多线程和并发。重点知识有线程的状态转换、线程的同步与互斥、线程池的运作机制详解，以及JUC 中常用的工具类。\n多线程知识点\n\n多线程协作时，因为对资源的锁定与等待会产生死锁，这里需要了解产生死锁的四个基本条件，要明白竞争条件与临界区的概念，知道可以通过破坏造成死锁的 4 个条件来防止死锁。\n前面讲过进程间的通信方式，这里还要知道线程间的通信方式，通信主要指线程之间的协作机制，例如 wait、notify 等。\n还需要知道 Java 为多线程提供的一些机制，例如 ThreadLocal 用来保存线程独享的数据， Fork&#x2F;Join 机制用于大任务的分割与汇总，Volatile 对多线程数据可见性的保证，以及线程的中断机制\n其他还有：ThreadLocal 的实现机制。Fork&#x2F;Join 的工作窃取算法等内容\n详解线程状态转换线程是 JVM 执行任务的最小单元，理解线程的状态转换是理解后续多线程问题的基础。在 JVM 运行中，线程一共有 NEW、RUNNABLE、BLOCKED、WAITING、TIMED_WAITING、TERMINATED 六种状态，这些状态对应 Thread.State 枚举类中的状态\n如下图所示，当创建一个线程时，线程处在 NEW 状态，运行 Thread 的 start 方法后，线程进入 RUNNABLE 可运行状态。\n\n\n这时，所有可运行状态的线程并不能马上运行，而是需要先进入就绪状态等待线程调度，如图中间所示的 READY 状态。在获取 CPU 后才能进入运行状态，如图中所示的 RUNNING。运行状态可以随着不同条件转换成除 NEW 以外的其他状态。\n如图左侧所示，在运行态中的线程进入 synchronized 同步块或者同步方法时，如果获取锁失败，则会进入到 BLOCKED 状态。当获取到锁后，会从 BLOCKED 状态恢复到就绪状态\n如图右侧所示，运行中的线程还会进入等待状态，这两个等待一个是有超时时间的等待，例如调用 Object.wait、Thread.join 等；另外一个是无超时的等待，例如调用 Thread.join 或者 Locksupport.park 等。这两种等待都可以通过 notify 或 unpark 结束等待状态并恢复到就绪状态。\n最后是线程运行完成结束时，如图下侧所示，线程状态变成 TERMINATED。\n详解 CAS 与 ABA 问题这部分内容详解线程的同步与互斥，解决线程同步与互斥的主要方式是 CAS、synchronized 和 lock。\nCASCAS 是乐观锁的一种实现方式，是一种轻量级锁，JUC 中很多工具类的实现就是基于 CAS 的。CAS 操作的流程如下图所示，线程在读取数据时不进行加锁，在准备写回数据时，比较原值是否修改，若未被其他线程修改则写回，若已被修改，则重新执行读取流程。这是一种乐观策略，认为并发操作并不总会发生。\n\n\n比较并写回的操作是通过操作系统原语实现的，保证执行过程中不会被中断。\nABACAS 容易出现 ABA 问题，就是如下面时序图所示，如果线程 T1 读取值 A 之后，发生两次写入，先由线程 T2 写回了 B，又由 T3 写回了 A，此时 T1 在写回比较时，值还是 A，就无法判断是否发生过修改。\n\n\nABA 问题不一定会影响结果，但还是需要防范，解决的办法可以增加额外的标志位或者时间戳。JUC 工具包中提供了这样的类。\n详解 synchronizedsynchronized 是最常用的线程同步手段之一，它是如何保证同一时刻只有一个线程可以进入临界区呢\nsynchronized 对对象进行加锁，在 JVM 中，对象在内存中分为三块区域：对象头、实例数据和对齐填充。在对象头中保存了锁标志位和指向 monitor 对象的起始地址，如下图所示，右侧就是对象对应的 Monitor 对象。当 Monitor 被某个线程持有后，就会处于锁定状态，如图中的 Owner 部分，会指向持有 Monitor 对象的线程。另外 Monitor 中还有两个队列，用来存放进入及等待获取锁的线程。\n\n\nsynchronized 应用在方法上时，在字节码中是通过方法的 ACC_SYNCHRONIZED 标志来实现的，synchronized 应用在同步块上时，在字节码中是通过 monitorenter 和 monitorexit 实现的\n针对 synchronized 获取锁的方式，JVM 使用了锁升级的优化方式，就是先使用偏向锁优先同一线程然后再次获取锁，如果失败，就升级为 CAS 轻量级锁，如果失败就会短暂自旋，防止线程被系统挂起。最后如果以上都失败就升级为重量级锁。\n详解 AQS 与 Lock在介绍 Lock 前，先介绍 AQS，也就是队列同步器，这是实现 Lock 的基础。下图就是 AQS 的结构图，从图中可以看出，AQS 有一个 state 标记位，值为1 时表示有线程占用，其他线程需要进入到同步队列等待。同步队列是一个双向链表。\n\n\n当获得锁的线程需要等待某个条件时，会进入 condition 的等待队列，等待队列可以有多个。当 condition 条件满足时，线程会从等待队列重新进入同步队列进行获取锁的竞争。ReentrantLock 就是基于 AQS 实现的，如下图所示，ReentrantLock 内部有公平锁和非公平锁两种实现，差别就在于新来的线程是否比已经在同步队列中的等待线程更早获得锁。\n和 ReentrantLock 实现方式类似，Semaphore 也是基于 AQS 的，差别在于 ReentrantLock 是独占锁，Semaphore 是共享锁。\n\n\n详解线程池线程池通过复用线程，避免线程频繁地创建和销毁。Java 的 Executors 工具类中提供了 5 种类型的线程池创建方法，如下图所示，来看它们的特点和适用场景。\n\n\n\n固定大小线程池，特点是线程数固定，使用无界队列，适用于任务数量不均匀的场景、对内存压力不敏感但系统负载比较敏感的场景\nCached 线程池，特点是不限制线程数，适用于要求低延迟的短期任务场景；\n单线程线程池，就是一个线程的固定线程池，适用于需要异步执行但需要保证任务顺序的场景；\nScheduled 线程池，适用于定期执行任务场景，支持按固定频率定期执行和按固定延时定期执行两种方式\n工作窃取线程池，使用的是 ForkJoinPool，是固定并行度的多任务队列，适合任务执行时长不均匀的场景。\n\n详解线程池参数线程池除了工作窃取线程池外，都是通过 ThreadPoolExecutor 的不同初始化参数来创建的。\n创建参数列表如下图所示。\n\n\n\n第一个参数设置核心线程数。默认情况下核心线程会一直存活。\n第二个参数设置最大线程数。决定线程池最多可以创建的多少线程。\n第三个参数和第四个参数用来设置线程空闲时间，和空闲时间的单位，当线程闲置超过空闲时间就会被销毁。可以通过 allowCoreThreadTimeOut 方法来允许核心线程被回收。\n第五个参数设置缓冲队列，上图中左下方的三个队列是设置线程池时常使用的缓冲队列。其中 ArrayBlockingQueue 是一个有界队列，就是指队列有最大容量限制。LinkedBlockingQueue 是无界队列，就是队列不限制容量。最后一个是 SynchronousQueue，是一个同步队列，内部没有缓冲区。\n第六个参数设置线程池工厂方法，线程工厂用来创建新线程，可以用来对线程的一些属性进行定制，例如线程的 group、线程名、优先级等。一般使用默认工厂类即可。\n第七个参数设置线程池满时的拒绝策略。如上图右下方所示有四种策略，Abort 策略在线程池满后，提交新任务时会抛出 RejectedExecutionException，这个也是默认的拒绝策略。Discard 策略会在提交失败时对任务直接进行丢弃。CallerRuns 策略会在提交失败时，由提交任务的线程直接执行提交的任务。DiscardOldest 策略会丢弃最早提交的任务。\n\n再来看前面的几种线程池都是使用怎样的参数来创建的。\n\n固定大小线程池创建时核心和最大线程数都设置成指定的线程数，这样线程池中就只会使用固定大小的线程数。\n队列使用无界队列 LinkedBlockingQueue\nSingle 线程池就是线程数设置为 1 的固定线程池\nCached 线程池的核心线程数设置为 0，最大线程数是 Integer.MAX_VALUE，主要是通过把缓冲队列设置成 SynchronousQueue，这样只要没有空闲线程就会新建。\nScheduled 线程池与前几种不同的是使用了 DelayedWorkQueue，这是一种按延迟时间获取任务的优先级队列。\n\n详解线程池执行流程向线程提交任务时可以使用 execute 和 submit，区别就是 submit 可以返回一个 future 对象，通过 future 对象可以了解任务执行情况，可以取消任务的执行，还可获取执行结果或执行异常。submit 最终也是通过 execute 执行的。\n\n向线程池提交任务时的执行顺序如下图所示。\n\n\n\n向线程池提交任务时，会首先判断线程池中的线程数是否大于设置的核心线程数，如果不大于，就创建一个核心线程来执行任务\n如果大于核心线程数，就会判断缓冲队列是否满了，如果没有满，则放入队列，等待线程空闲时执行任务。\n如果队列已经满了，则判断是否达到了线程池设置的最大线程数，如果没有达到，就创建新线程来执行任务。\n如果已经达到了最大线程数，则执行指定的拒绝策略。\n\n这里需要注意队列的判断与最大线程数判断的顺序，不要搞反\n详解 JUC 工具类JUC 是 Java 提供的用于多线程处理的工具类库，来看其中的常用工具类的作用，如下图所示。\n\n\n如上图所示，第一行的类都是基本数据类型的原子类，包括 AtomicBoolean、AtomicLong、AtomicInteger 类。\n\nAtomicLong 通过 unsafe 类实现，基于CAS。unsafe 类是底层工具类，JUC 中很多类的底层都使用到了 unsafe 包中的功能。unsafe 类提供了类似 C 的指针操作，提供 CAS 等功能。unsafe 类中的所有方法都是 native 修饰的\nLongAdder等 4 个类是 JDK1.8 中提供的更高效的操作类。LongAdder 基于 Cell 实现，使用分段锁思想，是一种空间换时间的策略，更适合高并发场景；LongAccumulator 提供了比 LongAdder 更强大的功能，能够指定对数据的操作规则，例如可以把对数据的相加操作改成相乘操作。\n\n第二行中的类提供了对对象的原子读写功能，后两个类 AtomicStampedReference 和 AtomicMarkableReference 用于解决前面提到的 ABA 问题，分别基于时间戳和标记位来解决问题。\n再看下图。\n\n\n第一行的类主要是锁相关的类，例如前面介绍过的 Reentrant 重入锁。\n\n与 ReentrantLock 的独占锁不同，Semaphore 是共享锁，允许多个线程共享资源，适用于限制使用共享资源线程数量的场景，例如 100 个车辆要使用 20 个停车位，那么最多允许 20 个车占用停车位\nStampedLock 是JDK 1.8 改进的读写锁，是使用一种 CLH 的乐观锁，能够有效防止写饥饿。所谓写饥饿就是在多线程读写时，读线程访问非常频繁，导致总是有读线程占用资源，写线程很难加上写锁。\n\n第二行中主要是异步执行相关的类\n\n重点了解 JDK 1.8 中提供的 CompletableFuture，可以支持流式调用，可以方便的进行多 future 的组合使用，例如可以同时执行两个异步任务，然后对执行结果进行合并处理。还可以很方便地设置完成时间\n另外一个是 JDK 1.7 中提供的 ForkJoinPool，采用分治思想，将大任务分解成多个小任务处理，然后在合并处理结果。ForkJoinPool 的特点是使用工作窃取算法，可以有效平衡多任务时间长短不一的场景。\n\n其他 JUC 常用工具如下图所示。\n\n\n第一行是常用的阻塞队列，讲解线程池时已经简单介绍过了，这里再补充一些。\n\nLinkedBlockingDeque 是双端队列，也就是可以分别从队头和队尾操作入队、出队。\nArrayBlockingQueue 单端队列，只能从队尾入队，队头出队。\n\n第二行是控制多线程协作时使用的类。\n\nCountDownLatch 实现计数器功能，可以用来控制等待多个线程执行任务后进行汇总\nCyclicBarrier 可以让一组线程等待至某个状态之后，再全部同时执行，一般在测试时使用，可以让多线程更好的并发执行。\nSemaphore 用来控制对共享资源的访问并发度。\n\n最后一行是比较常用的两个集合类，ConcurrentHashMap 我们前面的课程已经详细介绍过了，这里可以了解 CopyOnWriteArrayList，COW 通过写入数据时进行 copy 修改，然后更新引用的方式，来消除并行读写中的锁使用，比较适合读多写少，数据量比较小，但是并发非常高的场景。\n考察点讲解完本课时的知识点，总结下面试考察点。\n\n要理解线程同步与互斥的原理，包括临界资源、临界区的概念，知道重量级锁、轻量级锁、自旋锁、偏向锁、重入锁、读写锁的概念。\n要掌握线程安全相关机制，例如 CAS、synchronized、Lock 三种同步方式的实现原理、要明白 ThreadLocal 是每个线程独享的局部变量，了解 ThreadLocal 使用弱引用的 ThreadLocalMap 保存不同的 ThreadLocal 变量。\n要了解 JUC 中的工具类的使用场景与主要的几种工具类的实现原理，例如 Reentrantlock，ConcurrentHashMap、LongAdder 等实现方式。\n要熟悉线程池的原理、使用场景、常用配置，例如大量短期任务的场景适合使用 Cached 线程池；系统资源比较紧张时，可以选择固定线程池。另外注意慎用无界队列，可能会有 OOM 的风险。\n要深刻理解线程的同步与异步、阻塞与非阻塞，同步和异步的区别在于任务是否是同一个线程执行，阻塞与非阻塞的区别在于异步执行任务时，线程是会阻塞等待结果，还是会继续执行后续逻辑。\n\n加分项掌握了上面这些内容，如果能做到这几点加分项，一定会给面试官留下更好的印象。\n\n可以结合实际项目经验或者实际案例介绍原理，例如介绍线程池设置时，可以提到自己的项目中有一个需要高吞吐量的场景，使用了 Cached 的线程池。\n如果有过解决多线程问题的经验或者排查思路的话会获得面试加分。\n能够熟悉常用的线程分析工具与方法，例如会用 jstack 分析线程的运行状态，查找锁对象持有状况等。\n了解 Java 8 对 JUC 工具类做了哪些增强，例如提供了 LongAdder 来替换 AtomicLong，更适合并发度比较高的场景。\n了解 Reactive 异步编程思想，了解 back pressure 背压的概念与应用场景。\n\n真题汇总总结相关的面试真题，如下图所示，对重点题目提供一些思路。\n\n\n\n第 1 题如何实现一个生产者与消费者模型？可以尝试通过锁、信号量、线程通信、阻塞队列等不同方式实现。\n\n第 4 题 wait 与 sleep 的有什么不同？回答的要点四个：\n\nwait 属于 Object 类，sleep 属于 Thread 类；\nwai\n使用的位置不同，wait 需要在同步块中使用，sleep 可以在任意地方\nleep 需要捕获异常，而 wait 不需要。\n\n\n第 6 题，读写锁适用于什么场景？可以回答读写锁适合读并发多，写并发少的场景，另外一个解决这种场景的方法是 copyonwrite。\n\n\n\n\n\n第 7 题，线程之间如何通信？主要可以介绍一下 wait&#x2F;notify 机制，共享变量的 synchronized 或者 Lock 同步机制等。\n第 8 题，保证线程安全的方法有哪些？可以提 CAS、synchronized、Lock，以及 ThreadLocal 等机制。\n第 9 题，如何尽可能提高多线程并发性能？可以从尽量减少临界区范围，使用 ThreadLocal，减少线程切换、使用读写锁或 copyonwrite 等机制这些方面来回答\n第 10 题，ThreadLocal 用来解决什么问题？ThreadLocal 是如何实现的？可以重点回答 ThreadLocal 不是用来解决多线程共享变量的问题，而是用来解决线程数据隔离的问题。\n\n","categories":["总结笔记"],"tags":["java","多线程","并发"]},{"title":"研发排障常用工具总结","url":"/2021_10_20_tools/","content":"本文主要介绍常用的工具，将会讲解三个知识点：\n\nJVM 相关工具的作用和适用场景；\nGit 常用命令和工作流；\nLinux 系统中常用分析工具。\n\n常用工具汇总常用工具汇总如下图所示。\n\n\n\n说明：这里列出的都是一些相对独立的工具或者命令，不包括像 ZK、Redis 这样的服务，以及像 Spring 这类的框架。这些工具都是各自类型中最常用的，该图不是以全面为目的。\n团队协作工具如上图所示，先看左边的团队协作类工具。\n\nAnt+ivy、Maven、Gradle 都是用来构建项目、管理依赖的工具。其中 Ant 通过直接引用 jar 文件来进行依赖管理，通过脚本来描述，执行不同的构建目标。目前 Ant 使用得已经比较少，Maven 是比较主流的项目管理工具。\nMaven 通过 POM 文件对项目进行描述，通过约定提供可执行的目标，通过 GroupId、artifactId、version等坐标对依赖关系进行管理，Maven 可以从远程或本地仓库中自动下载依赖\nGradle 是结合了 Ant 与 Maven 优点的自动化构建工具，基于 Groovy 的 DSL 也就是领域专用语言。既有 Ant 的强大和灵活，又有 Maven 的生命周期管理，可以自动下载依赖。目前使用 Gradle 管理项目的团队也越来越多\nGit 和 SVN 都是版本管理工具，最主要区别是 SVN 是集中式的，Git 是分布式的，分支管理更灵活。目前 SVN 使用的已经相对较少了，Git 是大部分互联网公司使用的版本管理工具，后面的详解部分会专门针对 Git 的使用及 Git 工作流进行讲解。\n\n质量保证工具质量保证类工具有 CheckStyle、FindBugs、SonarQube 等等。\n\n其中 CheckStyle、FindBugs 是静态代码检测工具，可以通过 IDE 集成，对本地代码进行检测。\nSonarQube 是代码质量管理平台，默认集成了前面提到的两种工具，比较适合对项目质量进行整体保障。\n\n压测工具压测工具类有 LoadRunner、JMeter、AB、JMH。\n\nLoadRunner 和 JMeter 都是比较专业的测试工具，可以提供专业的报表和数据分析，比较适合 QA 人员使用\nAB 是 Apache 提供的一个简洁方便的压测工具，比较适合研发人员对 HTTP 接口进行简单并发压测\nJMH 主要是针对 JVM 进行基准测试，更关注方法层面的性能基准，如果想知道方法在两种不同实现下的吞吐量，就可以使用 JMH。对于应聘 Java 研发岗位的同学来说，测试工具部分可以重点了解一下 JMH。\n\n容器与代理工具容器与代理部分，目前主流的 Java Web 容器是 Tomcat，主流的代理是 Nginx。不过这里要多了解一些趋势，就是随着微服务的盛行，Envoy、OpenResty、Kong、Zuul 等的 API Gateway 网关的使用也越来越普遍。随着 DevOps 理念的普及，CI&#x2F;CD 也就是持续集成、持续部署，也越来越被大家所重视，这部分我们需要知道比较常用的 Jenkins 和 GitLab CI。\n\nJenkins，老牌的持续集成框架，可以支持不同类型的项目的构建、测试、部署，支持丰富的插件功能，和易用的管理界面。\nGitLab CI 作为 GitLab 提供的一个持续集成的套件，完美和 GitLab 进行集成，更加简单易用，比较适合 CI 流程比较简单的项目。\nTravis 和 CircleCI 都是开源项目中比较常用的持续集成框架，如果是研发开源项目可以进一步了解这两个框架的使用方式。\n\n文档管理工具如上图右边，JVM 工具和 Linux 系统分析工具，在后面会重点讲解，这里略过。来看文档管理工具。\n\nJavaDoc 通过注解方式，来对 Java 类和方法进行描述，并生成描述文档。\nSwagger 是一个规范、完整的框架，用于生成、描述 RESTfulAPI，Swagger 支持多种语言，提供了可视化的 Swagger UI，Java 中 Swagger 使用注解方式描述接口、参数、返回值等，非常适合用来对 RESTful 接口进行管理，特别是跨语言的 Web 服务。\n\n网络工具服务之间一般都要通过网络进行交互，所以工程师在调试、排查问题时需要掌握常用的网络工具。\n这里介绍几种常用网络工具。\n\nPostman 是调试网页的 Chrome 插件，相当于一个客户端，可以模拟用户发起的 HTTP 请求，是高效的接口测试工具，非常适合用来对 HTTP 接口进行联调与测试。\nWireshark 是个功能强大的网络包分析工具，支持各种协议的网络包分析，可以直接抓包，也可以配合 tcpdump 来使用，分析 tcpdump 抓包的结果。例如分析 HTTP 服务发、收包时间，链接的建立、关闭过程，请求包的分包大小与时序，TCP 窗口大小等等。\nFiddler 只针对 HTTP 请求进行抓包，可以修改请求或者模拟慢网速，是 Web 前端、移动端调试的利器，Charles 与 Fiddler 功能类似，支持 mac 系统，比较适合移动端抓包使用。\n\n详解 JVM 工具JMC首先是 JVM 的相关工具，第一个要介绍的是 JMC，就是 Java Mission Control。JMC 是 JDK1.7 中提供的图形化 JVM 监控与分析工具，如下图所示，JMC 包括 JVM 浏览器和 JMX 控制台，以及 JFR 也就是飞行记录器三部分。\n\n\n\nJVM 浏览器可以列出正在运行的 Java 程序的 JVM，每个 JVM 实例叫作一个 JVM 连接。JVM 浏览器使用 JDP 也就是 Java 发现协议，可以连接到本地和远程运行的 JVM。\nJMX 是 Java 管理扩展规范，能够管理并监控 JVM。JMX 通过对 Mbean 的管理，可以实时收集 JVM 信息，比如类实例信息、堆使用情况、CPU 负载、线程信息等，以及其他可以通过 MBeans 管理的一些运行时属性。\nJFR 提供了深入到 JVM 内部去看运行时状态的能力，是一个非常强大的性能 Profile 工具，适合对程序进行调优和问题排查。JFR对JVM运行时产生的事件进行采集，可以通过指定采集的事件类型和频率来收集非常全面的数据信息。这里我主要介绍一下使用JFR可以分析到哪些信息\n如下图所示，JFR 可以采集、分析五大类信息。\n\n\n\n内存信息，可以获取到 GC 的不同阶段及耗时情况、GC 的停顿时间、GC 的分代大小等配置信息，能够查看到对象分配，包括 TLAB 栈上分配情况，以及对象统计信息等等。\n代码信息，可以分析出热点的类、热点的方法、热点的调用树、运行时的异常信息、编译情况包括 OSR 栈上替换等信息，以及类的加载与卸载情况。\n线程信息部分，可以分析到：热点的线程、线程的争用情况、线程的等待时间、以及锁相关的信息。\nIO 信息部分，可以获得收集期间的磁盘 IO，也就是文件读写信息，以及网络 IO 等信息\n系统信息，可以获取到操作系统信息、进程相关信息以及环境变量等信息。\n\n总结一下：JMX 和 JFR 都可以获得 JVM 运行的信息。JMX 主要用来对 JVM 进行监控与管理，通过扩展 Mbean 支持自定义的管理能力。JFR 主要用来对 JVM 运行信息进行周期性采集，用来对运行状况进行分析。\nBTrace如果分析线上问题时，发现日志打的不全、无法定位怎么办？添加日志重新上线肯定不是个好主意，特别是调试时，可能需要反复添加日志来定位问题。或者，线上出现的问题很难复现，你根本没有机会添加日志再继续分析，这时就需要使用到 BTrace了。BTrace 是一个 JVM 实时监控工具，被 Java 工程师奉为性能调优和线上题诊断的神器。\nBTrace 基于动态字节码修改技术来实现对运行时的 Java 程序进行跟踪和替换。也就是说可以在不重启 JVM 的情况下监控系统运行情况，获取 JVM 运行时的数据信息，比如方法参数、返回值、全局变量、堆栈信息等。\nBTrace 可以做什么：\n\n可以对方法进行定位拦截，获取方法的入参、返回值、执行时间等信息\n可以查看某类对象的创建情况\n可以对内存使用情况进行统计，可以查看对象大小\n可以查看同步块执行情况\n可以查看异常抛出情况及导致异常的参数信息\n能够支持定时执行检测任务\n能够查看类加载的信息\n能够进行死锁检测\n可以打印线程栈信息\n可以监控文件或网络的读写情况。\n\n如上所述，BTrace 的功能非常强大，几乎无所不能。因为 Btrace 会把逻辑直接植入到运行的 JVM 中，为了保证安全，在使用上进行了一些限制。\n那么，BTrace 不能做什么：\n\nBTrace 不能创建新的对象\n不能抛出或捕获异常\n不能使用循环，例如 for、while\nBTrace 脚本的属性和方法必须使用 static 修饰\n不能使用 synchronized 的同步块或同步方法\n不能调用实例方法或静态方法，只能使用 BTraceUtils 类中提供的方法。\n\n可见，使用 BTrace 的条件还是非常严格的。需要注意三点：\n\n不恰当地使用 BTrace 可能导致 JVM 崩溃；\nBTrace 所做的修改是会一直生效的，直到重新启动 JVM 后才会消除；\n可以通过设置 JVM 参数取消 BTrace 的安全限制。\n\n其他 JVM 工具\n\n\njps 用来查看 Java 进程的信息，包括进程 id、主类名称、主类全路径等\njmap 可以查看JVM中对象的统计信息，包括内存占用、实例个数、对象类型等等，jmap 可以把堆 dump 下来配合内存分析工具 MAT 进行分析。\njstat 对 JVM 的资源和性能进行实时监控，统计项主要包括：类加载情况、内存容量及使用量、 GC 次数和时间等等。\njstack 可以查看 JVM 线程栈的信息，包括：线程名称、序号、优先级 prio、线程状态、锁状态等。\njinfo 可以查看运行中 JVM 的全部参数，还可以设置部分参数。\njcmd 是 JDK1.7 后提供的工具，可以向 JVM 发送诊断命令。它的功能非常强大，基本上包括了 jmap、jstack、jstat 的功能。可以重点了解这个工具。\n其他还有 jconsole、JProfiler、jvisualVM 等，功能跟 JMC 基本重合，建议直接使用 JMC 即可。\n\n列举几个实际应用场景。\n\n当你排查线上问题，需要查看 GC 日志，发现没有打印 GC 的详细信息，可以通过 jinfo 开启 JVM 参数 PrintGCDetails 来动态生效\n当你分析内存泄露风险时，可以通过 jmap 或 jcmd 定期获取堆对象的统计信息，来发现持续增长的可疑对象。\n当你遇到某一时刻所有服务都出现耗时较高的问题，可以通过 jstat 来观察 GC 回收状况，看看 GC 停顿耗时是否过高。\n当你遇到 JVM 中某一个服务卡死或者停止处理时，可以通过 jstack 查看线程栈，看是否有多个线程处于 BLOCKED 状态产生了死锁。\n当你的服务上线后发现性能达不到预期，可以使用 JMC 来分析 JVM 运行信息，看看有哪些热点方法可以优化，哪些线程争用可以避免。\n\n详解 GitGit 常用命令Git 与 SVN 的区别在前面知识点汇总时已经简单介绍过了。再看 Git 的常用命令及对应的使用场景。\nGit 对版本是分布式管理，因此有四个保存数据的区域，如下图中浅绿色的部分，分别是本地工作区 Workspace、本地暂存区 Stage、本地仓库和远程仓库。\n\n\n开发时先从远程拉取代码到工作区，可以有 clone、pull、fetch+checkout 几种方式，如图中向左的几个箭头所示。在提交代码时，先通过 add 命令添加到暂存区，然后 commit 提交到本地仓库，最后使用 push 推送到远程仓库。如图中向右的几个箭头所示。\n稍微注意一下 fetch 与 pull 的区别。\n\nfetch 是从远程仓库同步到本地仓库，但并不会合并到工作区\npull 相当于执行 fetch 命令+merge 命令，先同步到本地仓库，然后在 merge 到工作区。\n\nGit 的命令行提示非常友好，对常用 Git 操作的说明非常完善，其他的命令不展开介绍。\nGit 常用工作流使用 Git 进行团队协作开发时，多人协作、多分支开发是很常见的。为了更好得管理代码，需要制定一个工作流程，这就是我们说的工作流，也可以叫分支管理策略。常见的基于 Git 的工作流有 Git-flow工作流、GitHub 工作流和 GitLab 工作流，如下图所示。\n\n\n\nGit-Flow 工作流\n\n如上图左侧所示，Git-Flow 按功能来说，分为 5 条分支，在图中以不同颜色表示，其中 master 和 develop 是长期分支。master 分支上的代码都是版本发布状态；develop 分支则代表最新的开发进度\n当需要开发某些功能时，就从 develop 拉出 feature 分支进行开发，开发完成并验证后就可以合并回 develop 分支。当 develop 上的代码达到一个稳定的状态，可以发布版本的时候，会从 develop 合并到 release 分支进行发布，如果验证有问题就在 release 分支进行修复，修复验证通过后进行正式发布，然后合并到 master 分支和 develop 分支。还有一个 hotfix 分支用来做线上的紧急 bug 修复，hotfix 直接从 master 拉出分支修改，修改验证完成后直接合并回 master，并同步到 develop 分支。\nGit-Flow 流程非常完善，但对于很多开发人员和团队来说，会稍微有些复杂，而且没有图形页面。\n\n\nGitHub 工作流\n\n现在来看另一种更简单的工作流，如上图所示，中间的 GitHub 工作流\nGitHub 工作流只有一个长期分支 master，而且 master 分支的代码永远是可发布状态。如果有新功能开发，可以从 master 分支上检出新分支，开发完成需要合并时，创建一个合并到 master 到 PR，也就是 pull request。当 review 通过或者验证通过后，代码合并到 master 分支。GitHub 工作流中 hotfix 热修复的流程和 feature 分支完全一\n\n\nGitLab 工作流\n\n如上图所示，看右面的 GitLab 工作流。前两种工作流各有优缺点，Git-Flow 稍微复杂，GitHub 的单一主分支有时会略显不足。GitLab 结合了两者的优势，既支持 Git-Flow 的多分支策略，也有 GitHub Flow 的一些机制，比如 Merge Request和 issue 跟踪。GitLab工作流使用 pre-production 分支来进行预发管理，使用 prodution 分支来发布版本。我的团队目前使用的就是 GitLab 工作流。\n\n\n\nLinux 工具来看 Linux 系统下常用的分析工具。首先是如下图表格中列出的 stat 系列。\n\n\n\nvmstat 可以获得有关进程、内存页面交换、虚拟内存、线程上下文切换、等待队列等信息。能够反映系统的负载情况。一般用来查看进程等待数量、内存换页情况、系统上下文切换是否频繁等。\niostat 工具可以对系统的磁盘操作活动进行监视，同时也可以显示 CPU 使用情况，一般用来排查与文件读写有关的问题，例如排查文件写入耗时较高时，可以查看 await 和 util 是否过高。iotop 是查看磁盘 I&#x2F;O 使用状况的 top 类工具，想知道到底哪个进程产生了大量的 IO 时可以使用 iotop。\nifstat 是简洁的实时网络流量监控工具，可以查看系统的网络出口、入口使用情况。iftop 可以用来监控网卡的实时流量、反向解析 IP、显示端口信息等，通过iftop很容易找到哪个ip在霸占网络流量。\nnetstat 是一个监控系统网络状态的工具，它可以查看网络连接状态，监听了哪些接口、链接相关的进程等信息，能够显示与 IP、TCP、UDP 和 ICMP 协议相关的统计数据，是非常常用的网络工具。\ndstat 是一个全能实时系统信息统计工具，能够统计 CPU 占用，内存占用，网络状况，系统负载，进程信息，磁盘信息等等，可以用来替换 vmstat、iostat、netstat 和i fstat 这些工具。\n\n再来看如下图的几个工具。\n\n\n\nstrace 是一个用于诊断、调试程序运行时系统调用的工具，可以动态跟踪程序的运行，能够清楚地看到一个程序运行时产生的系统调用过程及其使用的参数、返回值和执行耗时。\nJVM 执行 native 方法时，可以很方便的通过 strace 来进行调试，例如在执行系统读写时，线程卡住很长时间，就可以用 strace 来查看系统调用的参数和耗时。\nGDB 是一个强大的命令行调试工具，可以让程序在受控的环境中运行，让被调试的程序在指定的断点处停住，也可以动态的改变程序的执行环境。当 JVM 因为未知原因 crash 时，可以通过 GDB 来分析 crash 时产生的 coredump 文件，来分析定位问题。\nlsof 是一个列出当前系统打开文件的工具。Linux 中一切皆文件，包括设备、链接等都是以文件形式管理的，因此通过 lsof 工具查看文件列表对系统监测以及排错都很有帮助。\ntcpdump 是一个强大的网络抓包工具，在分析服务之间调用时非常有用。可以将网络中传送的数据包抓取下来进行分析。tcpdump 提供灵活的抓取策略，支持针对网络层、协议、主机、网络或端口的过滤，并提供 and、or、not 等逻辑语句来去掉不想要的信息。\ntraceroute 是一个网络路由分析工具，利用 ICMP 协议定位本地计算机和目标计算机之间的所有路由。traceroute 对服务，特别是经过公网的服务之间的网络问题排查非常有帮助。\n\n考察点以上是常用工具的知识重点。接下来从面试官角度总结一下面试考察点：\n\n掌握常用的 JVM 分析工具主要用来分析哪类的问题，例如线程死锁可以用线程分析工具 jstack；内存溢出可以使用 jmap 查看堆中占用最大的对象类型；需要对程序性能进行分析时，可以使用 JMC 中的飞行记录器等等\n掌握常用的代码版本管理工具 Git，包括 Git 的常用命令与常见问题，以及理解 Git 工作流。例如知道 Git 的 merge 与 Git rebase 的区别，merge 是提交 commit 合并修改，rebase 是修改提交历史记录。知道自己团队在协作开发时，使用的哪种工作流，有什么样的优缺点。\n掌握 Linux 系统下的常用工具，也是突出实战能力。了解不同问题应该使用哪类工具来进行分析。例如，磁盘写入经常耗时较高可以通过 iostat 来分析磁盘 IO 情况，如果不能确定问题，可以通过 strace 对文件写入的系统调用进行分析；或者 CPU 负载较高，想要定位哪个线程导致，可以通过 top 结合 jstack 来进行分析等等。\n\n本课时的考察点以知识广度为主，对于不同类型的工具，需要知道适用场合，重点考察实际应用经验。面试时这部分内容可能会被问到一些原理，但一般不会深入询问工具的具体实现。\n加分项对于常用工具这部分，面试官可能不会直接问你“会用某某工具吗”，所以对于这一课的知识，你需要主动出击，才能获得加分。比如，在面试官询问项目经验时，带出你了解的工具，来体现你的知识广度与实战经验。\n举个例子，当面试官询问你遇到过哪些线上问题时，你可以说遇到过单机请求耗时高的问题，通过 JMC 的飞行记录器采样分析，发现写 log 日志时线程竞争非常激烈，很多线程在等待写锁时耗时非常大，进一步通过 iostat 排查发现 util 利用率百分比很高，最后定位是磁盘出现问题。解决方法，一方面更换磁盘解决了问题，另一方面对写竞争较激烈的日志文件使用了异步 log 机制。这样回答，既可以突出你对常用工具的掌握能力，也可以突出你的实战和解决问题能力\n另外再给提供两个思路：\n\n可以在介绍自己开发的某个项目时，提到在上线前使用 JMC 做了性能 Profile，发现并优化了某些问题。\n在介绍项目方案时，讲到自己对某两个不同方案进行了 JMH 测试，来验证方案实现的性能，等等。这两个 Case 都能够做到主动出击，体现自己的对常用工具的理解与掌握能力。\n\n真题汇总最后列出一些真题用于参考练习，如下。\n\n\n\n学习 JMC、BTrace、tcpdump、strace 等工具的使用。\n\n","categories":["总结笔记"],"tags":["java","jvm","git","linux"]},{"title":"数据结构与算法总结","url":"/2021_10_19_algo/","content":"本文的主题为数据结构与算法。行业里流行一种说法：程序 &#x3D; 数据结构 + 算法。虽然有些夸张，但足以说明数据结构与算法的重要性。本文重点讲解四个知识点：\n\n从搜索树到 B+ 树，讲解与树有关的数据结构；\n字符串匹配相关的题目；\n算法面试经常考察的 TopK 问题\n算法题的几种常用解题方法。\n\n数据结构知识点首先看数据结构的知识点都有哪些，如下图所示。\n\n\n\n队列和栈是经常使用的数据结构，需要了解它们的特点。队列是先进先出，栈是后进先出。\n表，包括很多种，有占用连续空间的数组、用指针链接的单向和双向链表，首尾相接的循环链表、以及散列表，也叫哈希表\n图，在特定领域使用的比较多，例如路由算法中会经常使用到，图分为有向图、无向图及带权图，这部分需要掌握图的深度遍历和广度遍历算法，了解最短路径算法\n树的内容，树一般用作查找与排序的辅助结构，剩下两个部分都和树有关，一个是二叉树，一个是多叉树。\n多叉树包括 B 树族，有 B 树、B+ 树、B* 树，比较适合用来做文件检索；另外一个是字典树，适合进行字符串的多模匹配。\n二叉树包括平衡二叉树、红黑树、哈夫曼树，以及堆，适合用于进行数据查找和排序。这部分需要了解二叉树的构建、插入、删除操作的实现，需要掌握二叉树的前序、中序、后序遍历。\n\n算法知识点来看算法部分的知识点汇总，如下图所示。\n\n\n\n算法题的常用解题方法。\n复杂度是衡量算法好坏的标准之一，我们需要掌握计算算法时间复杂度和空间复杂度的方法。计算时间复杂度的方法一般是找到执行次数最多的语句，然后计算语句执行次数的数量级，最后用大写 O 来表示结果。\n常用的字符串匹配算法，了解不同算法的匹配思路\n排序也是经常考察的知识点，排序算法分为插入、交换、选择、归并、基数五类，其中快速排序和堆排序考察的频率最高，要重点掌握，需要能够手写算法实现\n常用的查找算法，包括二分查找、二叉排序树、B 树、Hash、BloomFilter 等，需要了解它们的适用场景，例如二分查找适合小数量集内存查找，B 树适合文件索引，Hash 常数级的时间复杂度更适合对查找效率要求较高的场合，BloomFilter 适合对大数据集进行数据存在性过滤。\n\n详解二叉搜索树二叉搜索树如下图所示，二叉搜索树满足这样的条件，每个节点包含一个值，每个节点至多有两个子树。每个节点左子树节点的值都小于自身的值，每个节点右子树节点的值都大于自身的值。\n\n\n二叉树的查询时间复杂度是 log(N)，但是随着不断的插入、删除节点，二叉树的树高可能会不断变大，当一个二叉搜索树所有节点都只有左子树或者都只有右子树时，其查找性能就退化成线性的了。\n平衡二叉树平衡二叉树可以解决上面这个问题，平衡二叉树保证每个节点左右子树的高度差的绝对值不超过 1，例如 AVL 树。AVL 树是严格的平衡二叉树，插入或删除数据时可能经常需要旋转来保持平衡，比较适合插入、删除比较少的场景\n红黑树红黑树是一种更加实用的非严格的平衡二叉树。红黑树更关注局部平衡而非整体平衡，确保没有一条路径会比其他路径长出 2 倍，所以是接近平衡的，但减少了许多不必要的旋转操作，更加实用。前面提到过，Java 8 的 HashMap 中就应用了红黑树来解决散列冲突时的查找问题。TreeMap 也是通过红黑树来保证有序性的。\n红黑树除了拥有二叉搜索树的特点外，还有以下规则，如下图所示。\n\n\n\n每个节点不是红色就是黑色。\n根节点是黑色。\n每个叶子节点都是黑色的空节点，如图中的黑色三角。\n红色节点的两个子节点都是黑色的。\n任意节点到其叶节点的每条路径上，包含相同数量的黑色节点。\n\n详解B 树B树B 树是一种多叉树，也叫多路搜索树。B 树中每个节点可以存储多个元素，非常适合用在文件索引上，可以有效减少磁盘 IO 次数。B 树中所有结点的最大子节点数称为 B 树的阶，如下图所示是一棵 3 阶 B 树，也叫 2-3 树。\n\n\n一个 m 阶 B 树有如下特点：\n\n非叶节点最多有 m 棵子树\n根节点最少有两个子树，非根、非叶节点最少有 m&#x2F;2 棵子树；\n非叶子结点中保存的关键字个数，等于该节点子树个数−1，就是说一个节点如果有 3棵子树，那么其中必定包含 2 个关键字；\n非叶子节点中的关键字大小有序，如上图中左边的节点中 37、51 两个元素就是有序的；\n节点中每个关键字的左子树中的关键字都小于该关键字，右子树中的关键字都大于该关键字。如上图中关键字 51 的左子树有 42、49，都小于 51，右子树的节点有 59，大于51；\n所有叶节点都在同一层。\n\nB 树在查找时，从根结点开始，对结点内的有序的关键字序列进行二分查找，如果找到就结束，没有找到就进入查询关键字所属范围的子树进行查找，直到叶节点。\n总结一下：\n\nB 树的关键字分布在整颗树中，一个关键字只出现在一个节点中；\n搜索可能在非叶节点停止；\nB 树一般应用在文件系统。\n\nB+树下图是 B 树的一个变种，叫作 B+ 树。\n\n\nB+ 树的定义与 B 树基本相同，除了下面这几个特点。\n\n节点中的关键字与子树数目相同，比如节点中有 3 个关键字，那么就有 3 棵子树；\n关键字对应的子树中的节点都大于或等于关键字，子树中包括关键字自身\n所有关键字都出现在叶子节点中；\n所有叶子节点都有指向下一个叶子节点的指针。\n\n与 B 树不同，B+ 树在搜索时不会在非叶子节点命中，一定会查询到叶子节点；另外一个，叶子节点相当于数据存储层，保存关键字对应的数据，而非叶子节点只保存关键字和指向叶节点的指针，不保存关键字对应的数据，所以同样数量关键字的非叶节点，B+ 树比 B 树要小很多。\nB+ 树更适合索引系统，MySQL 数据库的索引就提供了 B+ 树实现。原因有三个：\n\n由于叶节点之间有指针相连，B+ 树更适合范围检索\n由于非页节点只保存关键字和指针，同样大小非叶节点，B+ 树可以容纳更多的关键字，可以降低树高，查询时磁盘读写代价更低；\nB+ 树的查询效率比较稳定。任何关键字的查找必须走一条从根结点到叶子结点的路，所有关键字查询的路径长度相同，效率相当。\n\n最后可以简单了解，还有一种 B* 树的变种，在 B+ 树的非叶节点上，也增加了指向同一层下一个非叶节点的指针。\n详解字符串匹配字符串匹配问题在面试时，字符串相关的问题经常作为算法考察题，下面来看字符串匹配的问题。先来了解一道常考的面试题：“判断给定字符串中的括号是否匹配”。\n一般面试题目的描述都比较简单，在解答前，可以跟面试官进一步沟通一下题目要求和细节。以这道题为例，可以跟面试官确认括号的范围，是不是只考虑大中小括号就可以，包不包括尖括号；对函数的入参和返回值有没有什么样的要求；需不需要考虑针对大文件的操作等。\n我们假定细化后本题的要求为：只考虑大中小括号；不考虑针对大文件的操作，以字符串作为入参，返回值为布尔类型；未出现括号也算作匹配的情况。那么，解题思路如下。\n\n字符匹配问题可以考虑使用栈的特性来处理。\n遇到左括号时入栈，遇到右括号时出栈对比，看是不是成对的括号。\n当匹配完成时，如果栈内为空说明匹配，否则说明左括号多于右括号。\n\n字符串代码来看实际的实现代码，如下图所示。\n\n\n按照上面的思路，需要对字符串进行遍历，所以首先要能确定栈操作的触发条件，就是定义好括号对，方便入栈和出栈匹配。这里要注意，编码实现时一定要注意编码风格与规范，例如变量命名必须要有明确意义，不要简单使用 a、b 这种没有明确意义的变量名。\n我们首先定义了 brackets 的 map，key 是所有右括号，value 是对应的左括号，这样定义方便出栈时对比括号是否是成对\n再看一下匹配函数的逻辑。这里也要注意，作为工具类函数，要做好健壮性防御，首先要对输入参数进行验空。\n然后我们定义一个保存字符类型的栈，开始对输入的字符串进行遍历。\n如果当前字符是 brackets 中的值，也就是左括号，则入栈。这里要注意，map 的值查询方法是 O(N) 的，因为本题中括号种类很少，才使用这种方式让代码更简洁一些。如果当前字符不是左括号，在使用 containskey 来判断是不是右括号。如果是右括号，需要检验是否匹配，如果栈为空表示右括号多于左括号，如果栈不空，但出栈的左括号不匹配，这两种情况都说明字符串中的括号是不匹配的。\n当遍历完成时，如果栈中没有多余的左括号，则匹配。\n最后强调一下：编码题除了编程思路，一定要注意编程风格和细节点的处理。\n字符串解题思路接下来，总结一下字符串匹配类问题的解题技巧。\n\n首先要认真审题，避免答偏。可以先确定是单模式匹配问题还是多模式匹配问题，命中条件是否有多个。\n然后确定对算法时间复杂度或者内存占用是否有额外要求。\n最后要明确期望的返回值是什么，比如存在有多个命中结果时，是返回第一个命中的，还是全部返回。\n\n关于解题思路。\n\n如果是单模式匹配问题，可以考虑使用 BM 或者 KMP 算法。\n如果是多模匹配，可以考虑使用 Tire 树来解决。\n在实现匹配算法时，可以考虑用前缀或者后缀匹配的方式来进行。\n最后可以考虑是否能够通过栈、二叉树或者多叉树等数据结构来辅助解决。\n\n建议了解一下常见的字符串单模、多模匹配算法的处理思路。\n详解TopKTopK 问题是在实际业务中经常出现的典型问题，例如微博的热门排行就属于 TopK 问题。\nTopK 一般是要求在 N 个数的集合中找到最小或者最大的 K 个值，通常 N 都非常得大。TopK 可以通过排序的方式解决，但是时间复杂度较高，一般是 O(nk)，这里我们来看看更加高效的方法。\n如下图所示，首先取前 K 个元素建立一个大根堆，然后对剩下的 N-K 个元素进行遍历，如果小于堆顶的元素，则替换掉堆顶元素，然后调整堆。当全部遍历完成时，堆中的 K 个元素就是最小的 K 个值。\n\n\n这个算法的时间复杂度是 N*logK。算法的优点是不用在内存中读入全部的元素，能够适用于非常大的数据集。\nTopK 变种问题TopK 变种的问题，就是从 N 个有序队列中，找到最小或者最大的 K 个值。这个问题的不同点在于，是对多个数据集进行排序。由于初始的数据集是有序的，因此不需要遍历完 N 个队列中所有的元素。因此，解题思路是如何减少要遍历的元素。\n解题思路如下图所示。\n\n\n\n第一步先用 N 个队列的队头元素，也就是每个队列的最小元素，组成一个有 K 个元素的小根堆。方式同 TopK 中的方法\n第二步获取堆顶值，也就是所有队列中最小的一个元素\n第三步用这个堆顶元素所在队列的下一个值放入堆顶，然后调整堆。\n最后重复这个步骤直到获取够 K 个数。\n\n这里还可以有个小优化就是第三步往堆顶放入新值时，跟堆的最大值进行一下比较，如果已经大于堆中最大值，就可以提前终止循环了。这个算法的时间复杂度是 (N+K-1)*logK，注意这里与队列的长度无关。\n详解常用算法算法的知识点比较多，提高算法解题能力需要适当刷题，但不能单纯依靠刷题来解决问题。需要掌握几种常用解题思路与方法，才能以不变应万变。这里讲一下：分治、动态规划、贪心、回溯和分支界定这五种常用的算法题解题方法，来看看它们分别适用于什么场景，如何应用。\n分治法分治法的思想是将一个难以直接解决的复杂问题或者大问题，分割成一些规模较小的相同问题，分而治之。比如快速排序、归并排序等都是应用了分治法\n适合使用分治法的场景需要满足三点要求：\n\n可以分解为子问题；\n子问题的解可以合并为原问题的解；\n子问题之间没有关联。\n\n使用分治法解决问题的一般步骤如下图表格所示\n\n\n\n第一步，要找到最小子问题的求解方法；\n第二步，要找到合并子问题解的方法；\n第三步，要找到递归终止条件。\n\n动态规划法动态规划法，与分治法类似，也是将问题分解为多个子问题。与分治法不同的是，子问题的解之间是有关联的。前一子问题的解，为后一子问题的求解提供了有用的信息。动态规划法依次解决各子问题，在求解每一个子问题时，列出所有局部解，通过决策保留那些有可能达到全局最优的局部解。最后一个子问题的解就是初始问题的解。\n使用动态规划的场景需要也满足三点条件\n\n子问题的求解必须是按顺序进行的；\n相邻的子问题之间有关联关系\n最后一个子问题的解就是初始问题的解。\n\n使用动态规划解决问题时，如上图表格第二行。\n\n第一步，先要分析最优解的性质；\n第二步，递归的定义最优解\n第三步，记录不同阶段的最优值；\n第四步，根据阶段最优解选择全局最优解\n\n贪心算法第三个贪心算法，因为它考虑的是局部的最优解，所以贪心算法不是对所有问题都能得到整体最优解。贪心算法的关键是贪心策略的选择。贪心策略必须具备无后效性，就是说某个状态以后的过程不会影响以前的状态，只与当前状态有关。\n贪心算法使用的场景必须满足两点：\n\n局部最优解能产生全局最优解；\n就是刚才说的必须具备无后效性。\n\n如下图所示，使用贪心算法解题的一般步骤为：\n\n第一步，先分解为子问题；\n第二步、按贪心策略计算每个子问题的局部最优解\n第三步，合并局部最优解。\n\n\n\n回溯算法回溯算法实际上是一种深度优先的搜索算法，按选优的条件向前搜索，当探索到某一步时，发现原先选择并不优或达不到目标，就退回上一步重新选择，这种走不通就退回再走的方法就是回溯法。\n回溯法适用于能够深度优先搜索，并且需要获取解空间的所有解的场合，例如迷宫问题等。\n如上图所示，回溯法一般的解题步骤为：\n\n第一步先针对所给问题，确定问题的解空间；\n第二步、确定结点的扩展搜索规则；\n第三步，以深度优先方式搜索解空间，并在搜索过程中用剪枝函数避免无效搜索。\n\n分支界定法最后是分支界定法，与回溯法的求解目标不同。回溯法的求解目标是找出满足约束条件的所有解，而分支界定法的求解目标则是找出满足约束条件的一个解。\n分支界定法适用于广度优先搜索，并且获取解空间的任意解就可以的场合，例如求解整数规划问题\n如上图所示，分支界定法一般的解题步骤：\n\n第一步先确定解的特征\n第二步在确定子节点搜索策略，例如是先入先出，还是先入后出；\n第三步通过广度优先遍历寻找解。\n\n考察点以上是针对数据结构与算法内容划的重点。接下来，从面试官角度出发，总结相关的面试考察点：\n\n了解基本数据结构及特点，例如数据结构中有哪些二叉树，这些树有哪些特点；\n要熟练掌握表、栈、队列、树，深刻理解不同类型实现的使用场景，例如红黑树适合用来做搜索，B+ 树适合用来做索引；\n要了解常用的搜索、排序算法，及复杂度和稳定性。特别是快速排序和堆排序的实现，要熟练掌握；\n要了解常用的字符串处理算法，和处理的思路，例如BM算法使用后缀匹配进行字符串匹配；\n要能够分析算法实现的复杂度，特别是时间复杂度，例如TopK问题的时间复杂度计算；\n要了解五种常用的解题方法，解决问题的思路和解决哪类问题，以及解题的步骤。\n\n加分项要想在算法面试的相关题目获得面试官的加分，牢记下面几点：\n\n能够将数据结构与实际使用场景结合，例如介绍红黑树时结合 TreeMap 的实现；介绍 B+ 树时结合 MySQL 中的索引实现等等；\n能知道不同算法在业务场景中有哪些应用，例如 TopK 算法在热门排序中的应用；\n面对模糊的题目能主动沟通确认条件和边界，例如前面介绍的括号匹配问题时列举的那些细节点，都可以跟面试官再次确认；\n在书写算法代码前，先讲一下解题思路，不要一上来埋头就写。一般解题思路存在问题时，面试官都会适当进行引导\n能够发现解答中的一些问题，给出改进的思路。比如面试时由于时间关系，大家可能都会选择比较保守的解题思路，不一定就是最优解，这时可以在解答后，指出当前算法存在的一些问题，以及改进的思路。比如可以考虑使用多线程的方式来提高求解性能\n\n真题汇总\n\n\n第 1、2 题都是基础算法，必须要牢牢掌握，一些题目要记住递归与非递归的实现，例如树的遍历、快速排序等；\n类似第 5 题这样的对使用内存进行限制的题目，要考虑使用分治思想进行分解处理\n第 6 题数组去重，可以有排序和 Hash 两种思路。\n\n\n\n\n第 9 题成语接龙，可以考虑使用深度优先搜索解决\n第 10 题寻找两节点公共祖先，可以考虑通过递归与非递归两种方式实现。\n\n","categories":["总结笔记"],"tags":["java","多线程","并发"]},{"title":"缓存中间件总结","url":"/2021_10_22_cache/","content":"本文介绍缓存相关的知识点以及Memcache和Redis这两个最常使用的缓存。重点学习以下三个方面的内容：\n\n使用缓存时常遇到的典型问题；\nMemcache的内存结构；\nRedis相关的知识点以及Redis常用结构的实现。\n\n缓存知识点\n\n类型缓存是高并发场景下提高热点数据访问性能的一个有效手段，在开发项目时会经常使用到。缓存的类型分为：本地缓存、分布式缓存和多级缓存。\n本地缓存就是在进程的内存中进行缓存，比如我们的JVM堆中，可以用LRUMap来实现，也可以使用Ehcache这样的工具来实现。本地缓存是内存访问，没有远程交互开销，性能最好，但是受限于单机容量，一般缓存较小且无法扩展。\n分布式缓存可以很好得解决这个问题。分布式缓存一般都具有良好的水平扩展能力，对较大数据量的场景也能应付自如。缺点就是需要进行远程请求，性能不如本地缓存。\n为了平衡这种情况，实际业务中一般采用多级缓存，本地缓存只保存访问频率最高的部分热点数据，其他的热点数据放在分布式缓存中。\n淘汰策略不管是本地缓存还是分布式缓存，为了保证较高性能，都是使用内存来保存数据，由于成本和内存限制，当存储的数据超过缓存容量时，需要对缓存的数据进行剔除。一般的剔除策略有FIFO淘汰最早数据、LRU 剔除最近最少使用、和LFU剔除最近使用频率最低的数据几种策略。\nMemcache注意后面会把Memcache简称为MC。\n先来看看MC 的特点：\n\nMC处理请求时使用多线程异步IO的方式，可以合理利用CPU多核的优势，性能非常优秀；\nMC功能简单，使用内存存储数据，只支持K-V结构，不提供持久化和主从同步功能；\nMC的内存结构以及钙化问题后面会详细介绍;\nMC对缓存的数据可以设置失效期，过期后的数据会被清除；\n失效的策略采用延迟失效，就是当再次使用数据时检查是否失效；\n当容量存满时，会对缓存中的数据进行剔除，剔除时除了会对过期key进行清理，还会按LRU策略对数据进行剔除。\n\n另外，使用MC 有一些限制：\n\nkey不能超过250个字节\nvalue不能超过1M字节；\nkey的最大失效时间是30天。\n\nRedis先简单说一下Redis 的特点，方便和MC比较。\n与MC不同的是，Redis采用单线程模式处理请求。这样做的原因有2个：一个是因为采用了非阻塞的异步事件处理机制；另一个是缓存数据都是内存操作IO时间不会太长，单线程可以避免线程上下文切换产生的代价。\nRedis支持持久化，所以Redis不仅仅可以用作缓存，也可以用作NoSQL数据库。\n相比MC，Redis还有一个非常大的优势，就是除了K-V之外，还支持多种数据格式，例如list、set、sorted set、hash等。\nRedis提供主从同步机制，以及Cluster集群部署能力，能够提供高可用服务\n详解 Memcache（MC）内存结构首先来看MC的内存结构。MC默认是通过 SlabAllocator来管理内存，如下图所示。Slab机制主要是用来解决频繁malloc&#x2F;free会产生内存碎片的问题。\n\n\n如图左侧，MC会把内存分为许多不同类型的Slab，每种类型Slab用来保存不同大小的对象。每个Slab由若干的Page组成，如图中浅绿色的模块。不同Slab的Page，默认大小是一样的，都是1M，这也是默认MC存储对象不能超过1M的原因。每个Page内又划分为许多的Chunk，Chunk就是实际用来保存对象的空间，就是图中橘色的。不同类型的Slab中Chunk的大小是不同的，当保存一个对象时，MC会根据对象的大小来选择最合适的Chunk来存储，减少空间浪费。\nSlab Allocator创建Slab时的参数有三个，分别是Chunk大小的增长因子，Chunk大小的初始值以及Page的大小。在运行时会根据要保存的对象大小来逐渐创建Slab\n钙化问题来考虑这样一个场景，使用MC来保存用户信息，假设单个对象大约300字节。这时会产生大量的384字节大小的Slab。运行一段时间后，用户信息增加了一个属性，单个对象的大小变成了500字节，这时再保存对象需要使用768字节的Slab，而MC 中的容量大部分创建了384字节的Slab，所以768的Slab非常少。这时虽然384Slab的内存大量空闲，但768Slab还是会根据LRU算法频繁剔除缓存，导致MC的剔除率升高，命中率降低。这就是所谓的MC钙化问题。\n解决钙化问题可以开启MC的Automove机制，每10s调整Slab。也可以分批重启MC缓存，不过要注意重启时要进行一定时间的预热，防止雪崩问题。另外，在使用Memcached时，最好计算一下数据的预期平均长度，调整growth factor， 以获得最恰当的设置，避免内存的大量浪费。\n详解 RedisRedis的知识点结构如下图所示。\n\n\n功能来看Redis提供的功能。\nBitmap位图是支持按bit位来存储信息，可以用来实现BloomFilter；HyperLogLog提供不精确的去重计数功能，比较适合用来做大规模数据的去重统计，例如统计UV；Geospatial可以用来保存地理位置，并作位置距离计算或者根据半径计算位置等。这三个其实也可以算作一种数据结构。\npub&#x2F;sub功能是订阅发布功能，可以用作简单的消息队列。\nPipeline可以批量执行一组指令，一次性返回全部结果，可以减少频繁的请求应答。\nRedis支持提交Lua脚本来执行一系列的功能。\n最后一个功能是事务，但Redis提供的不是严格的事务，Redis只保证串行执行命令，并且能保证全部执行，但是执行命令失败时并不会回滚，而是会继续执行下去。\n持久化Redis提供了RDB和AOF两种持久化方式，RDB是把内存中的数据集以快照形式写入磁盘，实际操作是通过fork子进程执行，采用二进制压缩存储；AOF是以文本日志的形式记录Redis处理的每一个写入或删除操作。\nRDB把整个Redis的数据保存在单一文件中，比较适合用来做灾备，但缺点是快照保存完成之前如果宕机，这段时间的数据将会丢失，另外保存快照时可能导致服务短时间不可用。\nAOF对日志文件的写入操作使用的追加模式，有灵活的同步策略，支持每秒同步、每次修改同步和不同步，缺点就是相同规模的数据集，AOF要大于RDB，AOF在运行效率上往往会慢于RDB。\n高可用来看Redis的高可用。Redis支持主从同步，提供Cluster集群部署模式，通过Sentinel哨兵来监控Redis主服务器的状态。当主挂掉时，在从节点中根据一定策略选出新主，并调整其他从slaveof到新主。\n选主的策略简单来说有三个：\n\n slave的priority设置的越低，优先级越高；\n 同等情况下，slave复制的数据越多优先级越高；\n 相同的条件下runid越小越容易被选中。\n\n\n在Redis集群中，sentinel也会进行多实例部署，sentinel之间通过Raft协议来保证自身的高可用。\nRedisCluster使用分片机制，在内部分为16384个slot插槽，分布在所有master节点上，每个master节点负责一部分slot。数据操作时按key做CRC16来计算在哪个slot，由哪个master进行处理。数据的冗余是通过slave节点来保障。\nkey 失效机制Redis的key可以设置过期时间，过期后Redis采用主动和被动结合的失效机制，一个是和MC一样在访问时触发被动删除，另一种是定期的主动删除。\n淘汰策略Redis提供了6种淘汰策略，一类是只针对设置了失效期的key做LRU、最小生存时间和随机剔除；另一类是针对所有key做LRU、随机剔除。当然，也可以设置不剔除，容量满时再存储对象会返回异常，但是已存在的key还可以继续读取\n新特性可以了解一下Redis4.0和5.0的新特性，例如5.0的Stream，是一个可以支持多播，也就是一写多读的消息队列。还可以了解一下4.0的模块机制等。\n数据结构Redis内部使用字典来存储不同类型的数据，如下图中的dictht，字典由一组dictEntry组成，其中包括了指向key和value的指针以及指向下一个dictEntry的指针。\n\n\n在Redis中，所有的对象都被封装成了redisObject，如图中浅绿的模块。redisObject包括了对象的类型，就是Redis支持的string、hash、list、set和sortedset5种类型。另外redisObject还包括了具体对象的存储方式，如图最右边的虚线标出的模块内的几种类型。\n下面结合类型来介绍具体的数据存储方式。\n\nstring类型是Redis中最常使用的类型，内部的实现是通过SDS（Simple Dynamic String ）来存储的。SDS类似于Java中的ArrayList，可以通过预分配冗余空间的方式来减少内存的频繁分配。\nlist类型，有ziplist压缩列表和linkedlist双链表实现。ziplist是存储在一段连续的内存上，存储效率高，但是它不利于修改操作，适用于数据较少的情况；linkedlist在插入节点上复杂度很低，但它的内存开销很大，每个节点的地址不连续，容易产生内存碎片。此外在3.2版本后增加了quicklist，结合了两者的优点，quicklist本身是一个双向无环链表，它的每一个节点都是一个ziplist。\nhash类型在Redis中有ziplist和hashtable两种实现。当Hash表中所有的key和value字符串长度都小于64字节且键值对数量小于512个时，使用压缩表来节省空间；超过时，转为使用hashtable。\nset类型的内部实现可以是intset或者hashtable，当集合中元素小于512且所有的数据都是数值类型时，才会使用intset，否则会使用hashtable。\nsorted set是有序集合，有序集合的实现可以是ziplist或者是skiplist跳表。有序集合的编码转换条件与hash和list有些不同，当有序集合中元素数量小于128个并且所有元素长度都小于64字节时会使用ziplist，否则会转换成skiplist。\n\n提示：Redis的内存分配是使用jemalloc进行分配。jemalloc将内存空间划分为小、大、巨大三个范围，并在范围中划分了小的内存块，当存储数据时，选择大小最合适的内存块进行分配，有利于减小内存碎片。\n缓存常见问题对使用缓存时常遇到几个问题，整理出一个表格，如下图所示。\n\n\n缓存更新方式第一个问题是缓存更新方式，这是决定在使用缓存时就该考虑的问题。\n缓存的数据在数据源发生变更时需要对缓存进行更新，数据源可能是DB，也可能是远程服务。更新的方式可以是主动更新。数据源是DB时，可以在更新完DB后就直接更新缓存。\n当数据源不是DB而是其他远程服务，可能无法及时主动感知数据变更，这种情况下一般会选择对缓存数据设置失效期，也就是数据不一致的最大容忍时间\n这种场景下，可以选择失效更新，key不存在或失效时先请求数据源获取最新数据，然后再次缓存，并更新失效期。\n但这样做有个问题，如果依赖的远程服务在更新时出现异常，则会导致数据不可用。改进的办法是异步更新，就是当失效时先不清除数据，继续使用旧的数据，然后由异步线程去执行更新任务。这样就避免了失效瞬间的空窗期。另外还有一种纯异步更新方式，定时对数据进行分批更新。实际使用时可以根据业务场景选择更新方式。\n数据不一致第二个问题是数据不一致的问题，可以说只要使用缓存，就要考虑如何面对这个问题。缓存不一致产生的原因一般是主动更新失败，例如更新DB后，更新Redis因为网络原因请求超时；或者是异步更新失败导致。\n解决的办法是，如果服务对耗时不是特别敏感可以增加重试；如果服务对耗时敏感可以通过异步补偿任务来处理失败的更新，或者短期的数据不一致不会影响业务，那么只要下次更新时可以成功，能保证最终一致性就可以\n缓存穿透第三个问题是缓存穿透。产生这个问题的原因可能是外部的恶意攻击，例如，对用户信息进行了缓存，但恶意攻击者使用不存在的用户id频繁请求接口，导致查询缓存不命中，然后穿透DB查询依然不命中。这时会有大量请求穿透缓存访问到DB。\n解决的办法如下。\n\n对不存在的用户，在缓存中保存一个空对象进行标记，防止相同ID再次访问DB。不过有时这个方法并不能很好解决问题，可能导致缓存中存储大量无用数据。\n使用BloomFilter过滤器，BloomFilter的特点是存在性检测，如果BloomFilter中不存在，那么数据一定不存在；如果BloomFilter中存在，实际数据也有可能会不存在。非常适合解决这类的问题。\n\n缓存击穿第四个问题是缓存击穿，就是某个热点数据失效时，大量针对这个数据的请求会穿透到数据源\n解决这个问题有如下办法。\n\n可以使用互斥锁更新，保证同一个进程中针对同一个数据不会并发请求到DB，减小DB压力。\n使用随机退避方式，失效时随机sleep一个很短的时间，再次查询，如果失败再执行更新\n针对多个热点key同时失效的问题，可以在缓存时使用固定时间加上一个小的随机数，避免大量热点key同一时刻失效。\n\n缓存雪崩第五个问题是缓存雪崩。产生的原因是缓存挂掉，这时所有的请求都会穿透到DB。\n解决方法：\n\n使用快速失败的熔断策略，减少DB瞬间压力；\n使用主从模式和集群模式来尽量保证缓存服务的高可用。\n\n实际场景中，这两种方法会结合使用。\n考察点这一块内容的主要面试考察点是对缓存特性的理解，对MC、Redis的特点和使用方式的掌握。\n\n要知道缓存的使用场景，不同类型缓存的使用方式，例如：\n\n对DB热点数据进行缓存减少DB压力；对依赖的服务进行缓存，提高并发性能；\n单纯K-V缓存的场景可以使用MC，而需要缓存list、set等特殊数据格式，可以使用Redis；\n需要缓存一个用户最近播放视频的列表可以使用Redis的list来保存、需要计算排行榜数据时，可以使用Redis的zset结构来保存。\n\n\n要了解MC和Redis的常用命令，例如原子增减、对不同数据结构进行操作的命令等。\n\n了解MC 和Redis在内存中的存储结构，这对评估使用容量会很有帮助\n\n了解MC 和Redis的数据失效方式和剔除策略，比如主动触发的定期剔除和被动触发延期剔除\n\n要理解Redis的持久化、主从同步与Cluster部署的原理，比如RDB和AOF的实现方式与区别。\n\n\n加分项如果想要在面试中获得更好的表现，还应了解下面这些加分项。\n第一，是要结合实际应用场景来介绍缓存的使用。例如调用后端服务接口获取信息时，可以使用本地+远程的多级缓存；对于动态排行榜类的场景可以考虑通过Redis的sorted set来实现等等\n第二，最好你有过分布式缓存设计和使用经验，例如项目中在什么场景使用过Redis，使用了什么数据结构，解决哪类的问题；使用MC时根据预估值大小调整McSlab分配参数等等。\n第三，最好可以了解缓存使用中可能产生的问题。比如Redis是单线程处理请求，应尽量避免耗时较高的单个请求任务，防止相互影响；Redis服务应避免和其他CPU密集型的进程部署在同一机器；或者禁用Swap内存交换，防止Redis的缓存数据交换到硬盘上，影响性能。再比如前面提到的MC钙化问题等等。\n第四，要了解Redis的典型应用场景，例如，使用Redis来实现分布式锁；使用Bitmap来实现BloomFilter，使用HyperLogLog来进行UV统计等等。\n最后，知道Redis4.0、5.0中的新特性，例如支持多播的可持久化消息队列Stream；通过Module系统来进行定制功能扩展等等。\n真题汇总\n\n\n第1～4题前面都有提到，不再赘述\n第5题，可以从主从读写分离、多从库、多端口实例，以及Cluster集群部署来支持水平扩展等几方面回答，高可用可以回答用Sentinel来保证主挂掉时重新选主并完成从库变更。\n第6题，可以使用Redis的sorted set来实现延时队列，使用时间戳做Score，消费方使用zrangbyscore来获取指定延迟时间之前的数据。\n\n简单场景下分布式锁可以使用setnx实现，使用setnx设置key，如果返回1表示设置成功，即获取锁成功，如果返回0则获取锁失败。setnx需要同时使用px参数设置超时时间，防止获取锁的实例宕机后产生死锁。\n严格场景下，可以考虑使用RedLock方案。但是实现比较复杂。\n\n","categories":["总结笔记"],"tags":["redis","memcache","缓存击穿","缓存雪崩","缓存穿透"]},{"title":"缓存设计考量点总结","url":"/2021_11_05_cache_design/","content":"本文主要讲缓存的基本思想、缓存的优点、缓存的代价、然后介绍三种缓存读写模式、两种缓存分类方法、缓存设计架构考量点。\n缓存的定义\n缓存最初的含义，是指用于加速 CPU 数据交换的 RAM，即随机存取存储器，通常这种存储器使用更昂贵但快速的静态 RAM（SRAM）技术，用以对 DRAM进 行加速。这是一个狭义缓存的定义。\n而广义缓存的定义则更宽泛，任何可以用于数据高速交换的存储介质都是缓存，可以是硬件也可以是软件。\n\n\n\n缓存存在的意义就是通过开辟一个新的数据交换缓冲区，来解决原始数据获取代价太大的问题，让数据得到更快的访问。本课主要聚焦于广义缓存，特别是互联网产品大量使用的各种缓存组件和技术。\n缓存原理缓存的基本思想\n\n缓存构建的基本思想是利用时间局限性原理，通过空间换时间来达到加速数据获取的目的，同时由于缓存空间的成本较高，在实际设计架构中还要考虑访问延迟和成本的权衡问题。这里面有 3 个关键点。\n\n一是时间局限性原理，即被获取过一次的数据在未来会被多次引用，比如一条微博被一个人感兴趣并阅读后，它大概率还会被更多人阅读，当然如果变成热门微博后，会被数以百万&#x2F;千万计算的更多用户查看\n二是以空间换时间，因为原始数据获取太慢，所以我们开辟一块高速独立空间，提供高效访问，来达到数据获取加速的目的。\n三是性能成本 Tradeoff，构建系统时希望系统的访问性能越高越好，访问延迟越低小越好。但维持相同数据规模的存储及访问，性能越高延迟越小，成本也会越高，所以在系统架构设计时，你需要在系统性能和开发运行成本之间做取舍。比如左边这张图，相同成本的容量，SSD 硬盘容量会比内存大 10～30 倍以上，但读写延迟却高 50～100 倍。\n\n缓存的优势缓存的优势主要有以下几点：\n\n提升访问性能\n降低网络拥堵\n减轻服务负载\n增强可扩展性\n\n通过前面的介绍，我们已经知道缓存存储原始数据，可以大幅提升访问性能。不过在实际业务场景中，缓存中存储的往往是需要频繁访问的中间数据甚至最终结果，这些数据相比 DB 中的原始数据小很多，这样就可以减少网络流量，降低网络拥堵，同时由于减少了解析和计算，调用方和存储服务的负载也可以大幅降低。缓存的读写性能很高，预热快，在数据访问存在性能瓶颈或遇到突发流量，系统读写压力大增时，可以快速部署上线，同时在流量稳定后，也可以随时下线，从而使系统的可扩展性大大增强。\n缓存的代价然而不幸的是，任何事情都有两面性，缓存也不例外，我们在享受缓存带来一系列好处的同时，也注定需要付出一定的代价。\n\n首先，服务系统中引入缓存，会增加系统的复杂度。\n其次，由于缓存相比原始 DB 存储的成本更高，所以系统部署及运行的费用也会更高。\n最后，由于一份数据同时存在缓存和 DB 中，甚至缓存内部也会有多个数据副本，多份数据就会存在一致性问题，同时缓存体系本身也会存在可用性问题和分区的问题。这就需要我们加强对缓存原理、缓存组件以及优秀缓存体系实践的理解，从系统架构之初就对缓存进行良好设计，降低缓存引入的副作用，让缓存体系成为服务系统高效稳定运行的强力基石。\n\n一般来讲，服务系统的全量原始数据存储在 DB 中（如 MySQL、HBase 等），所有数据的读写都可以通过 DB 操作来获取。但 DB 读写性能低、延迟高，如 MySQL 单实例的读写 QPS 通常只有千级别（3000～6000），读写平均耗时 10～100ms 级别，如果一个用户请求需要查 20 个不同的数据来聚合，仅仅 DB 请求就需要数百毫秒甚至数秒。而 cache 的读写性能正好可以弥补 DB 的不足，比如 Memcached 的读写 QPS 可以达到 10～100万 级别，读写平均耗时在 1ms 以下，结合并发访问技术，单个请求即便查上百条数据，也可以轻松应对。\n但 cache 容量小，只能存储部分访问频繁的热数据，同时，同一份数据可能同时存在 cache 和 DB，如果处理不当，就会出现数据不一致的问题。所以服务系统在处理业务请求时，需要对 cache 的读写方式进行适当设计，既要保证数据高效返回，又要尽量避免数据不一致等各种问题。\n缓存读写模式如下图，业务系统读写缓存有 3 种模式：\n\nCache Aside（旁路缓存）\nRead&#x2F;Write Through（读写穿透）\nWrite Behind Caching（异步缓存写入）\n\n     \n\nCache Aside\n如上图所示，Cache Aside 模式中，业务应用方对于写，是更新 DB 后，直接将 key 从 cache 中删除，然后由 DB 驱动缓存数据的更新；而对于读，是先读 cache，如果 cache 没有，则读 DB，同时将从 DB 中读取的数据回写到 cache。\n这种模式的特点是，业务端处理所有数据访问细节，同时利用 Lazy 计算的思想，更新 DB 后，直接删除 cache 并通过 DB 更新，确保数据以 DB 结果为准，则可以大幅降低 cache 和 DB 中数据不一致的概率。\n如果没有专门的存储服务，同时是对数据一致性要求比较高的业务，或者是缓存数据更新比较复杂的业务，这些情况都比较适合使用 Cache Aside 模式。如微博发展初期，不少业务采用这种模式，这些缓存数据需要通过多个原始数据进行计算后设置。在部分数据变更后，直接删除缓存。同时，使用一个 Trigger 组件，实时读取 DB 的变更日志，然后重新计算并更新缓存。如果读缓存的时候，Trigger 还没写入 cache，则由调用方自行到 DB 加载计算并写入 cache。\nRead&#x2F;Write Through\n\n如上图，对于 Cache Aside 模式，业务应用需要同时维护 cache 和 DB 两个数据存储方，过于繁琐，于是就有了 Read&#x2F;Write Through 模式。在这种模式下，业务应用只关注一个存储服务即可，业务方的读写 cache 和 DB 的操作，都由存储服务代理。存储服务收到业务应用的写请求时，会首先查 cache，如果数据在 cache 中不存在，则只更新 DB，如果数据在 cache 中存在，则先更新 cache，然后更新 DB。而存储服务收到读请求时，如果命中 cache 直接返回，否则先从 DB 加载，回种到 cache 后返回响应。\n这种模式的特点是，存储服务封装了所有的数据处理细节，业务应用端代码只用关注业务逻辑本身，系统的隔离性更佳。另外，进行写操作时，如果 cache 中没有数据则不更新，有缓存数据才更新，内存效率更高。\n微博 Feed 的 Outbox Vector（即用户最新微博列表）就采用这种模式。一些粉丝较少且不活跃的用户发表微博后，Vector 服务会首先查询 Vector Cache，如果 cache 中没有该用户的 Outbox 记录，则不写该用户的 cache 数据，直接更新 DB 后就返回，只有 cache 中存在才会通过 CAS 指令进行更新。\nWrite Behind Caching   \nWrite Behind Caching 模式与 Read&#x2F;Write Through 模式类似，也由数据存储服务来管理 cache 和 DB 的读写。不同点是，数据更新时，Read&#x2F;write Through 是同步更新 cache 和 DB，而 Write Behind Caching 则是只更新缓存，不直接更新 DB，而是改为异步批量的方式来更新 DB。该模式的特点是，数据存储的写性能最高，非常适合一些变更特别频繁的业务，特别是可以合并写请求的业务，比如对一些计数业务，一条 Feed 被点赞 1万 次，如果更新 1万 次 DB 代价很大，而合并成一次请求直接加 1万，则是一个非常轻量的操作。但这种模型有个显著的缺点，即数据的一致性变差，甚至在一些极端场景下可能会丢失数据。比如系统 Crash、机器宕机时，如果有数据还没保存到 DB，则会存在丢失的风险。所以这种读写模式适合变更频率特别高，但对一致性要求不太高的业务，这样写操作可以异步批量写入 DB，减小 DB 压力。\n讲到这里，缓存的三种读写模式讲完了，你可以看到三种模式各有优劣，不存在最佳模式。实际上，我们也不可能设计出一个最佳的完美模式出来，如同前面讲到的空间换时间、访问延迟换低成本一样，高性能和强一致性从来都是有冲突的，系统设计从来就是取舍，随处需要 trade-off。这个思想会贯穿整个 cache 设计。\n缓存分类及常用缓存介绍前面介绍了缓存的基本思想、优势、代价以及读写模式，接下来一起看下互联网企业常用的缓存有哪些分类。\n按宿主层次分类按宿主层次分类的话，缓存一般可以分为本地 Cache、进程间 Cache 和远程 Cache。\n\n本地 Cache 是指业务进程内的缓存，这类缓存由于在业务系统进程内，所以读写性能超高且无任何网络开销，但不足是会随着业务系统重启而丢失。\n进程间 Cache 是本机独立运行的缓存，这类缓存读写性能较高，不会随着业务系统重启丢数据，并且可以大幅减少网络开销，但不足是业务系统和缓存都在相同宿主机，运维复杂，且存在资源竞争。\n远程 Cache 是指跨机器部署的缓存，这类缓存因为独立设备部署，容量大且易扩展，在互联网企业使用最广泛。不过远程缓存需要跨机访问，在高读写压力下，带宽容易成为瓶颈。\n\n本地 Cache 的缓存组件有 Ehcache、Guava Cache 等，开发者自己也可以用 Map、Set 等轻松构建一个自己专用的本地 Cache。进程间 Cache 和远程 Cache 的缓存组件相同，只是部署位置的差异罢了，这类缓存组件有 Memcached、Redis、Pika 等。\n按存储介质分类还有一种常见的分类方式是按存储介质来分，这样可以分为内存型缓存和持久化型缓存。\n\n内存型缓存将数据存储在内存，读写性能很高，但缓存系统重启或 Crash 后，内存数据会丢失。\n持久化型缓存将数据存储到 SSD&#x2F;Fusion-IO 硬盘中，相同成本下，这种缓存的容量会比内存型缓存大 1 个数量级以上，而且数据会持久化落地，重启不丢失，但读写性能相对低 1～2 个数量级。Memcached 是典型的内存型缓存，而 Pika 以及其他基于 RocksDB 开发的缓存组件等则属于持久化型缓存。\n\n接下来会聊聊到如何引入缓存并进行设计架构，以及在缓存设计架构中的一些关键考量点。\n缓存的引入及架构设计缓存组件选择在设计架构缓存时，你首先要选定缓存组件，比如要用 Local-Cache，还是 Redis、Memcached、Pika 等开源缓存组件，如果业务缓存需求比较特殊，你还要考虑是直接定制开发一个新的缓存组件，还是对开源缓存进行二次开发，来满足业务需要\n缓存数据结构设计确定好缓存组件后，你还要根据业务访问的特点，进行缓存数据结构的设计。对于直接简单 KV 读写的业务，你可以将这些业务数据封装为 String、Json、Protocol Buffer 等格式，序列化成字节序列，然后直接写入缓存中。读取时，先从缓存组件获取到数据的字节序列，再进行反序列化操作即可。对于只需要存取部分字段或需要在缓存端进行计算的业务，你可以把数据设计为 Hash、Set、List、Geo 等结构，存储到支持复杂集合数据类型的缓存中，如 Redis、Pika 等。\n缓存分布设计确定了缓存组件，设计好了缓存数据结构，接下来就要设计缓存的分布。可以从 3 个维度来进行缓存分布设计。\n\n首先，要选择分布式算法，是采用取模还是一致性 Hash 进行分布。取模分布的方案简单，每个 key 只会存在确定的缓存节点，一致性 Hash 分布的方案相对复杂，一个 key 对应的缓存节点不确定。但一致性 Hash 分布，可以在部分缓存节点异常时，将失效节点的数据访问均衡分散到其他正常存活的节点，从而更好地保证了缓存系统的稳定性。\n其次，分布读写访问如何进行实施，是由缓存 Client 直接进行 Hash 分布定位读写，还是通过 Proxy 代理来进行读写路由？Client 直接读写，读写性能最佳，但需要 Client 感知分布策略。在缓存部署发生在线变化时，也需要及时通知所有缓存 Client，避免读写异常，另外，Client 实现也较复杂。而通过 Proxy 路由，Client 只需直接访问 Proxy，分布逻辑及部署变更都由 Proxy 来处理，对业务应用开发最友好，但业务访问多一跳，访问性能会有一定的损失。\n最后，缓存系统运行过程中，如果待缓存的数据量增长过快，会导致大量缓存数据被剔除，缓存命中率会下降，数据访问性能会随之降低，这样就需要将数据从缓存节点进行动态拆分，把部分数据水平迁移到其他缓存节点。这个迁移过程需要考虑，是由 Proxy 进行迁移还是缓存 Server 自身进行迁移，甚至根本就不支持迁移。对于 Memcached，一般不支持迁移，对 Redis，社区版本是依靠缓存 Server 进行迁移，而对 Codis 则是通过 Admin、Proxy 配合后端缓存组件进行迁移。\n\n存架构部署及运维管理设计完毕缓存的分布策略后，接下来就要考虑缓存的架构部署及运维管理了。架构部署主要考虑如何对缓存进行分池、分层、分 IDC，以及是否需要进行异构处理。\n\n核心的、高并发访问的不同数据，需要分别分拆到独立的缓存池中，进行分别访问，避免相互影响；访问量较小、非核心的业务数据，则可以混存。\n对海量数据、访问超过 10～100万 级的业务数据，要考虑分层访问，并且要分摊访问量，避免缓存过载。\n如果业务系统需要多 IDC 部署甚至异地多活，则需要对缓存体系也进行多 IDC 部署，要考虑如何跨 IDC 对缓存数据进行更新，可以采用直接跨 IDC 读写，也可以采用 DataBus 配合队列机进行不同 IDC 的消息同步，然后由消息处理机进行缓存更新，还可以由各个 IDC 的 DB Trigger 进行缓存更新。\n某些极端场景下，还需要把多种缓存组件进行组合使用，通过缓存异构达到最佳读写性能。\n站在系统层面，要想更好得管理缓存，还要考虑缓存的服务化，考虑缓存体系如何更好得进行集群管理、监控运维等。\n\n缓存设计架构的常见考量点在缓存设计架构的过程中，有一些非常重要的考量点，如下图所示，只有分析清楚了这些考量点，才能设计架构出更佳的缓存体系。\n\n\n读写方式首先是 value 的读写方式。是全部整体读写，还是只部分读写及变更？是否需要内部计算？比如，用户粉丝数，很多普通用户的粉丝有几千到几万，而大 V 的粉丝更是高达几千万甚至过亿，因此，获取粉丝列表肯定不能采用整体读写的方式，只能部分获取。另外在判断某用户是否关注了另外一个用户时，也不需要拉取该用户的全部关注列表，直接在关注列表上进行检查判断，然后返回 True&#x2F;False 或 0&#x2F;1 的方式更为高效。\nKV size然后是不同业务数据缓存 KV 的 size。如果单个业务的 KV size 过大，需要分拆成多个 KV 来缓存。但是，不同缓存数据的 KV size 如果差异过大，也不能缓存在一起，避免缓存效率的低下和相互影响。\nkey 的数量key 的数量也是一个重要考虑因素。如果 key 数量不大，可以在缓存中存下全量数据，把缓存当 DB 存储来用，如果缓存读取 miss，则表明数据不存在，根本不需要再去 DB 查询。如果数据量巨大，则在缓存中尽可能只保留频繁访问的热数据，对于冷数据直接访问 DB。\n读写峰值另外，对缓存数据的读写峰值，如果小于 10万 级别，简单分拆到独立 Cache 池即可。而一旦数据的读写峰值超过 10万 甚至到达 100万 级的QPS，则需要对 Cache 进行分层处理，可以同时使用 Local-Cache 配合远程 cache，甚至远程缓存内部继续分层叠加分池进行处理。微博业务中，大多数核心业务的 Memcached 访问都采用的这种处理方式。\n命中率缓存的命中率对整个服务体系的性能影响甚大。对于核心高并发访问的业务，需要预留足够的容量，确保核心业务缓存维持较高的命中率。比如微博中的 Feed Vector Cache，常年的命中率高达 99.5% 以上。为了持续保持缓存的命中率，缓存体系需要持续监控，及时进行故障处理或故障转移。同时在部分缓存节点异常、命中率下降时，故障转移方案，需要考虑是采用一致性 Hash 分布的访问漂移策略，还是采用数据多层备份策略。\n过期策略\n可以设置较短的过期时间，让冷 key 自动过期；\n也可以让 key 带上时间戳，同时设置较长的过期时间，比如很多业务系统内部有这样一些 key：key_20190801。\n\n平均缓存穿透加载时间平均缓存穿透加载时间在某些业务场景下也很重要，对于一些缓存穿透后，加载时间特别长或者需要复杂计算的数据，而且访问量还比较大的业务数据，要配置更多容量，维持更高的命中率，从而减少穿透到 DB 的概率，来确保整个系统的访问性能。\n缓存可运维性对于缓存的可运维性考虑，则需要考虑缓存体系的集群管理，如何进行一键扩缩容，如何进行缓存组件的升级和变更，如何快速发现并定位问题，如何持续监控报警，最好有一个完善的运维平台，将各种运维工具进行集成。\n缓存安全性对于缓存的安全性考虑，一方面可以限制来源 IP，只允许内网访问，同时对于一些关键性指令，需要增加访问权限，避免被攻击或误操作时，导致重大后果。\n","categories":["总结笔记"],"tags":["缓存设计","旁路缓存","读写穿透","异步缓存写入"]},{"title":"分布式系统架构演进总结","url":"/2021_10_24_framework/","content":"本文会讲解分布式系统架构以及面试中做项目介绍的技巧，重点有如下三部分：\n\n介绍系统架构的演进：包括微服务架构、云原生以及业界最新趋势 ServiceMesh。\n讲解微服务的基础知识点：Docker 和 K8s。\n教你如何更有效地做项目介绍。\n\n系统架构演进首先以演进的方式来了解不同的系统架构。\n单体架构最简单的系统架构是单体服务，如下图所示。\n \n\n一个项目中的多个服务，混合部署在一个进程内，服务之间的交互都是通过进程内调用完成的，正如图中 Service 之间的红色箭头所示。这样做的好处是可以快速开发、部署服务，服务之间调用的性能也最好\n当然，这种架构缺点也非常多，比如：\n\n随着业务的增长，项目越来越臃肿；\n服务之间因为 JAR 包引用导致频繁的依赖冲突；\n服务资源变更困难，因为一个服务可能被多个不同的业务引用，升级资源需要多个业务方同时升级；\n因为不同业务方都可以直连服务的数据资源，这个架构也存在明显的数据安全风险；\n修改代码后回归困难、架构难以调整等等。\n\n以上所有问题都是因为服务耦合在一起导致的。在服务规模不大的情况下，比较适合采用单体架构，方便快速迭代。但是当服务规模变大时，单体架构就不是一个好的选择。\n微服务架构当服务的规模变大时，为了解决服务耦合的问题，出现了 SOA 就是面向服务架构，它的起源是为了解决企业应用问题，随着不断演进，发展到目前业界普遍采用的微服务架构，微服务架构如下所示。\n \n\n微服务架构的思想就是让服务尽可能做到高内聚、低耦合，不同的服务单独开发、单独测试、单独部署。服务之间通过 RPC 或者 HTTP 进行远程交互，如图中的蓝色加粗箭头所示\n微服务架构解决了单体架构的耦合问题，但同时也带来了新的问题。因为服务部署在不同的进程或服务器中，要使用服务前需要先找到服务，即所谓的服务发现\n一般微服务使用两种发现方式，一种是前面课程介绍过的 RPC 方式，通过注册中心进行服务的注册和订阅，来完成服务发现，比如图中间灰色的 Registry 模块。这种方式由服务的调用端获得到全部可用服务节点，由 Client 侧进行负载均衡，调用服务。另外一种是通过 HTTP 协议调用服务端提供的 RESTful 接口，这种方式不需要 Client 侧做服务发现，而是在 Server 端通过 Nignx 这样的反向代理来提供 Server 侧的负载均衡\n不论哪种方式，服务的交互都从进程内通信变成了远程通信，所以性能必然会受到一些影响。此外由于很多不确定性的因素，例如网络拥塞、Server 端服务器宕机、挖掘机铲断机房光纤等等，需要许多额外的功能和措施才能保证微服务流畅稳定的工作。前面在 Spring Cloud 内容中提到的 Hystrix 熔断器、Ribbon客户端负载均衡器、Eureka注册中心等等都是用来解决这些问题的微服务组件。\nCAP 原则与 BASE 理论在微服务架构中，有必要了解一下分布式系统中的 CAP 原则与 BASE 理论。如下图所示，CAP 原则指的是在一个分布式系统中，Consistency 一致性、 Availability 可用性、Partition tolerance 分区容错性，这三个特性最多只能同时满足两个，三者不可兼得。\n \n\n其中一致性指所有节点在同一时间的数据完全一致；可用性指任何时候对分布式系统总是可以成功读和写；分区容错性是指当某些节点或网络分区故障的时候，仍然能够提供满足一致性和可用性的服务\n既然无法同时满足三个特征，那就会有三种取舍。\n第一个选择是 CA，就是放弃分区容错，这也就等同于放弃了分布式系统，所以 CA 只存在于单机系统。\n第二个选择是 CP，也就是选择强一致和分区容错，允许极端情况下出现短时的服务不可用。采用 CP 原则实现的分布式系统比如 ZooKeeper。ZooKeeper是一个分布式协调系统，强一致性是 ZK 的主要目标，允许出现短时的系统不可用。也正是因为这个原因，ZK 其实并不适合用来做微服务的注册中心。其他选择 CP 实现的系统还有 Consul、etcd 等。\n第三个选择是 AP，也就是选择分区容错和高可用，允许数据出现短时间不一致。采用 AP 原则的分布式系统有 Eureka、Nacos。在服务注册的场景，短期的不一致一般不会对服务交互产生影响，因此采用 AP 原则的注册中心才是微服务比较适合的选择\n然后，介绍一下 BASE 理论，如上图底部的词汇所示，BASE 是指 Basically Available 基本可用，Soft-state 软状态，Eventual Consistency 最终一致性，它是对 CAP 中一致性和可用性权衡的结果。BASE 的核心思想是即使无法做到强一致性，也可以根据系统特点，采用适当的方式达到最终一致性。\n云原生服务继续讲解系统架构的演进。微服务架构的思路是服务解耦合，这会导致一个大的业务拆分成众多小的服务，每个服务的部署需要考虑单点问题，需要多机房多节点部署，会造成系统资源的浪费\n另外在服务扩容时需要重新整理服务运行依赖的环境，对微服务的普及有一定阻碍。容器化技术把服务的运行环境进行打包管理，解决了服务扩缩容时对运行环境的管理问题以及服务器的利用率问题。因此随着容器技术逐渐成熟，微服务架构也快速普及\n云原生架构由微服务组成，它不是一种业务系统架构，而是一种能够快速、持续、可靠、规模化地交付业务服务的模式。\n如下图所示，图上部列出了云原生的三个特征：\n\n容器化的微服务，这是云原生的主体；\nDevops，是对微服务的动态管理；\n持续交付能力，这是云原生的目的。\n\n \n\n\n云原生服务需要底层的云服务提供 IaaS 基础设施或者 PaaS 平台设施来运行，IaaS 可以理解为提供了服务器资源，PaaS 平台可以理解为提供了运行环境。\n常见的实现方式有两种：自建的私有云和云厂商提供的公有云。公有云比如阿里云、AWS、腾讯云等等，像新浪微博内部使用的是私有云与公有云结合的混合云模式。\n接下来看云原生应用开发的最佳实践原则：12 要素，如下图所示。\n \n\n12 要素定义了设计 SaaS 应用时需要遵循的一些基本原则，SaaS 是软件即服务的缩写，通过云原生应用来提供服务。 \n第 1 个要素是基准代码，是指代码由版本管理工具来管理，一个应用只有一份基准代码，运行时有多个的部署实例。\n第 2 个要素依赖，是指要在应用中显示的声明依赖，方便服务进行构建\n第 3 个要素配置，指要在环境中存储配置，而不是写在代码配置文件中。也就是说，配置与代码要分开管理，从代码外部进行加载，例如测试环境的配置、仿真环境的配置以及生产环境的配置都应该从对应的环境中进行加载。\n第 4 个要素后端服务，是指要把依赖的后端服务统一看作资源来对待。不论是 DB、缓存还是 HTTP 服务\n第 5 个要素是构建、发布、运行，是指要严格区分应用的构建、发布、运行这三个步骤，并且必须按顺序进行。\n第 6 个要素进程，是指应用以一个或多个进程运行，要保证应用的无状态性\n第 7 个要素端口绑定，是指不同的应用使用不同的端口提供服务。应用与端口是绑定的，不是指具体的某个端口号，而是指一旦服务启动确定了端口，那么这个端口就能够提供对应的服务，直到应用进程停止。\n第 8 个要素并发，是指应用进程之间可以并发处理，因此可以通过多进程方式进行水平扩展。\n第 9 个要素易处理，是指应用应该容易被管理，可以通过优雅停机和快速启动，构建最健壮的服务。\n第 10 个要素开发&#x2F;生产等价，是指要保证在开发、预览、生产等不同环境下的应用，尽可能一致。\n第 11 个要素日志，是指要合理记录应用的运行日志，并把日志当作一种事件流来对待，方便对日志的收集和处理。\n第 12 个要素管理进程，是指要把后台管理任务当作一次性进程来运行，而不是常驻后台进程的方式。\n以上 12 要素是对设计云原生服务的指导原则，在实际项目中可以结合实际业务场景进行架构设计，不一定完全照搬。\nService Mesh云原生应用是目前大部分互联网公司的服务架构推进方向，那么下一代的服务架构是什么样呢？这里介绍一个最新的服务化趋势，它离实际应用可能还有些遥远，我们可以静待它的发展\nService Mesh 是 2017 年逐渐在国内进入大家视野的一种架构方式，被誉为下一代的微服务。Service Mesh 在微服务的基础上引入了一个 Sidecar 边车的概念，如图中左下方的放大图所示，每个服务会伴生着部署一个 Sidecar，服务之间的交互不再由服务自身来完成，服务所有的出、入请求都交由这个 Sidecar 来进行处理，通过管理平面对所有的 Sidecar 进行统一管理，由 Sidecar 来实现服务发现、负载均衡、流量调度等能力。\n \n\n目前最有代表性的 Service Mesh 开源实现，是由 Google、IBM、Lyft 三家一起维护的 Istio，有兴趣的话可以持续关注，这里就不详细展开了。\n那么 Service Mesh 与微服务的区别是什么呢？Service Mesh 又可以解决哪些问题呢？如下图所示。\n \n\n微服务的出现是为了解决多个服务之间耦合的问题，如图中绿色的竖线，就是微服务架构做的事情，把 Service A、B、C 进行了解耦，服务单独部署、单独管理。这时每个服务都需要实现例如服务发现、服务的远程交互、交互过程中的负载均衡、高可用策略、服务熔断等等一系列的功能，这些功能与服务自身的业务逻辑没有任何关系\nService Mesh 的思路是把业务逻辑与业务无关的功能进行解耦，如图中红色的线，对服务进行横切，把与服务交互相关的功能从服务中剥离出来，统一交给 Sidecar 去实现，让服务更聚焦于业务逻辑，提高研发效率。同时由于功能相对独立，Sidecar 可以更专注于服务的交互与管理，更方便实现极致的功能与性能。\n所以，Service Mesh 不是一个全新的技术，它对业务与服务交互、管理进行了拆分，提供统一、强大的服务管理能力，是在微服务基础上的演进\n另外，Service Mesh 由于使用独立的 Sidecar 进程，天然适合为不同语言的服务提供统一的服务治理能力，因此跨语言服务治理也是 Service Mesh 的一个重要特点，像微博基于 Motan 研发的 Weibo Mesh，初衷就是为了解决内部不同语言之间服务化的问题。\n由于引入了额外的 Sidecar，Service Mesh 的架构复杂度更高，也会带来额外的可用性和性能问题，这也是 Service Mesh 架构需要努力解决的问题\n架构设计的意义通过了解系统架构的演进，我们发现，从单体架构到微服务架构，实现了服务之间解耦，但带来了额外的服务发现与交互问题；从微服务到 Service Mesh，实现了业务与服务治理功能的解耦，但是引入了额外的可用性和性能问题，架构复杂度也随之提高。那么这样做的意义在哪里？\n系统架构的设计从来就是一个权衡的艺术，很多情况下，我们只是让问题进行了转移，方便对问题进行集中整治和处理，让服务更聚焦业务研发，不同的功能就交给专门的组件来处理，正所谓术业有专攻。通过架构的演进，虽然当下没有消灭复杂度，但可以成功的让问题变的透明化，变的业务无感知，提升服务整体的开发效率与扩展能力，拓宽服务能力的上限。\n容器化基础微服务之所以能够快速发展，很重要的一个原因就是：容器化技术的发展和容器管理系统的成熟。所以接下来学习微服务架构的基础，容器化技术 Docker 和容器集群管理系统 Kubernetes。\nDocker 作用Docker 的作用主要是快速的构建、部署、运行服务，通过服务镜像能够为服务提供版本管理\n通过容器化技术可以屏蔽不同运行环境的差异，让服务在任何 Docker 环境中运行，就像 Java 的一次编译到处运行。\nDocker 是轻量虚拟化技术，可以在一台宿主机上运行多个服务，对运行的服务之间进行了有效的隔离，提高宿主机的资源利用率。\nDocker 特点\n开源，意味着可以免费使用 Docker 容器技术。\n基于 LXC 实现的轻量虚拟化，Docker 容器直接运行进程，不需要模拟，运行效率非常高。\n能够支持大规模构建。\nDocker 的架构十分灵活，可扩展不同的实现，例如支持不同存储驱动实现。\nDocker 提供可视化 UI，管理非常简单。\n\nDocker 主要概念\n镜像，就是服务代码和运行环境的封装，服务的版本管理就是通过镜像来实现的，镜像是部署的基础。\n容器，就是 Container，容器是基于镜像的服务运行状态，可以基于一个镜像运行多个容器。\n守护进程是运行在宿主机上的管理进程，用户通过 Client 与守护进程进行交互\n客户端是用来和守护进程交互的命令行工具，也可以通过 Socket 或者 RESTful API 访问远程的 Docker 守护进程。\n镜像仓库，类似我们的 Git 代码仓库，镜像仓库用来保存、管理不同服务不同版本的镜像。服务部署时会从镜像仓库拉取对应版本的镜像进行部署。\n\nDocker 实现原理Docker 是通过对不同运行进程进行隔离来实现虚拟化，主要利用三种方式来实现服务的隔离，如下图所示。\n \n\n首先是利用 Linux 的 Namespace 命名空间，来隔离进程之间的可见性，不同的服务进程彼此属于不同的 Namespace，互相无法感知对方的存在\nDocker 实现了 Host、Container、None 和 Bridge 四种网络模式，默认使用 Bridge 桥接模式。每一个容器在创建时都会创建一对虚拟网卡，两个虚拟网卡组成了数据的通道，其中一个会放在容器中，另一个会加入到 Docker0 的网桥中。Docker0 网桥通过 iptables 中的配置与宿主机上的网卡相连，所有符合条件的请求都会通过 iptables 转发到 Docker0 并由网桥分发给对应的容器网卡。为了防止容器进程修改宿主机的文件目录，Docker 通过改变进程访问文件目录的根节点，结合 Namespace 来隔离不同容器进程可以访问的文件目录。\n然后，通过 Namespace，Docker 隔离了进程、网络和文件目录，但是在进程运行中的 CPU 和内存等还是共享状态。Docker 通过 Control Groups 也就是 Cgroups 来对进程使用的资源进行限制，包括 CPU、内存和网络带宽等。\n那么，Docker 是如何把镜像运行起来的呢？Docker 的镜像是分层结构，例如一个服务镜像可以由操作系统层、基础环境层、Web 容器层、服务代码层，层层依赖构成。通过 UnionFS 就是联合文件系统把 Image 中的不同分层作为不同的只读目录，而 Container 是在只读的镜像目录上创建的可读可写的目录，通过这种方式来把镜像运行起来的。Docker 提供了 AUFS、Overlay、Devicemapper、ZFS 等多种不同存储驱动实现。\nKubernetes 作用Kubernetes 也叫 K8s，因为 K 与 s 之间一共有 8 个字母。K8s 是一个容器集群管理系统，不是一个 PaaS 平台，PaaS 平台是可以运行在 K8s 之上的。\nK8s 的作用是进行容器集群管理，它只针对容器管理，不部署源码不编译应用。它能够实现服务容器的自动部署与按指定条件进行自动扩缩容服务，来实现对应用的管理，支持应用的负载均衡、滚动更新、资源监控等等。\nKubernetes 特点\n可移植，支持在公有云，私有云，混合云中运行；\n可扩展，K8s 采用模块化实现方式，插件化的架构，可挂载，可组合\n自动化，支持服务的自动部署，自动重启，自动复制，自动伸缩。\n\nKubernetes 重要概念K8s 中的概念非常的多，这里列出了比较重要的几个。\nK8s 是容器集群管理系统，容器首先需要运行在宿主机上，因此，K8s 首先要管理宿主机集群，K8s 分为 Master 节点和 Node 节点，也叫 Worker Node\nMaster 负责管理节点，管理 K8s 集群。Master 协调集群中的所有行为&#x2F;活动，例如应用的运行、修改、更新等。\nNode 节点用来运行容器。Node 上可以运行多个 Pod，Pod 是 K8s 创建或部署的基本单位，Pod 中可以运行多个 Container，一个 Container 就是一个运行中的服务镜像。\nPod 中的 Container 共享网络与存储。 \nService 是 K8s 中的一个逻辑概念，通过对不同的 Pod 添加标签，来划分为不同的 Service。\nDeployment 表示用户对 K8s 集群的一次更新操作，可以是创建一个新的服务，更新一个新的服务，也可以是滚动升级一个服务。\nKubernetes 架构下图是 K8s 架构图，图左侧绿色的模块代表 Master 节点，右侧蓝色的模块代表运行容器的Worker Node 节点。\n \n\n先来看 Master 节点中的架构，灰色的部分是 Master 中的模块，其中 API Server 是用户对 K8s 中资源操作的唯一入口，创建应用部署、管理部署状态等都需要通过 api server 进行。API Server 提供了认证、授权、访问控制、API 注册和发现等机制\nController Manager 负责维护集群的状态，比如故障检测、自动扩展、滚动更新等，Controller Manager 包含多个可以扩展 Controller，例如 Node Controller 负责初始化 Node 节点，获取运行中的 Node 信息、Route Controller 负责配置集群间通信的路由信息、Service Controller 负责监听服务创建、更新、删除等事件来调整负载均衡信息等等。\nScheduler 负责资源的调度，按照预定的调度策略选择哪个 Pod 运行在哪个节点上\n另外图下方的绿色模块是用来保存整个集群的状态的 etcd。\n图最左侧的 kubectl 是用于运行 K8s 命令的管理工具，kubectl 与 API Server 进行交互，通过 API Server 下发对 K8s 集群的指令。\n再来看图右侧的 Worker Node。刚才介绍概念时提过，Node 用来运行应用容器，所以 Node 中必须要有一个容器运行时，可以是 Docker，也可以是其他的容器技术，例如 Rkt。\nNode 中部署应用时，每个应用都由一个 Pod 组成，可以把 Pod 看作一个虚拟服务器，上面可以运行一个或多个 Container 容器。当应用服务需要多个进程共同协作时，可以把这些协作的镜像打包放在一个 Pod 中，共享 Pod 的存储和网络。比如 istio 中的 Sidecar 代理模式，就是通过在服务的 Pod 中注入一个 Sidecar 镜像来实现与服务 IP 绑定，进行流量控制的。\n看到右面图中灰色的两个 Node 模块，kubelet 负责与 Master 通信，它周期性地访问 APIcontroller 进行检查和报告、执行容器的操作，维护容器的生命周期，也负责 Volume（CVI）和网络（CNI）的管理。\nkube-proxy 处理网络代理和每个容器的负载均衡，它通过改变 iptables 规则来控制在容器上的 TCP 和 UDP 包。\nK8s 把所有被管理的资源看作对象，对资源的管理就变成了对对象属性的设置。K8s 对对象的配置采用 YAML 格式进行描述。K8s 中的对象概念非常多，大致可以分为四类：\n\n资源对象，例如 Pod、Job；\n配置对象，例如 Node、Namespace、Service；\n存储对象，例如 Volume、PresidentVolume；\n策略对象，例如 SecurityContext、ResourceQuota、LimitRange 等等。\n\n如果感兴趣，可以在课后练习。比如最简单的，可以在单机环境中，使用 Minikube 来部署 K8s 进行练习。\n考察点系统架构主要看一个人的综合能力和发展潜力怎么样，考察点有这几个方面：\n第一，要对分布式架构有自己的理解。比如系统可用性、扩展性，比如故障的应对方法，包括熔断、容灾、流量迁移、多机房多活；再比如架构设计中的解耦合等等。\n第二，要了解系统架构优化的常用方法。比如：并行、异步、水平扩展和垂直扩展、预处理、缓存、分区（Sharding）等等。\n第三，会考察对负责的工作了解程度、是否有责任⼼。如果连自己负责的服务的部署规模，调用量级都不清楚，怎么能有很强的责任心呢？\n加分项加分点主要在于表现出面试者的学习能力和思考能力。\n第一个，了解业界最新趋势，比如 Service Mesh 的思路和要解决的问题。\n第二个，在介绍项目时，如果有不同方案的选型或对比会更好。比如在介绍项目架构时，有两个方案，一个是同步方案，一个是异步方案，这两个方案各有什么优缺点，最后结合业务场景、实际需求、请求量级 选择了某一种。\n面试技巧介绍项目\n面试时，一定会遇到介绍项目这个问题。我见过的大多数人在里表现的并不好：要么讲不清楚项目的结构与交互流程；要么不能理解项目架构为什么要这样设计；要么没有思考过项目存在哪些问题，有哪些可以改进的地方。不仅是针对面试，在工作中我们更应该搞清楚这些问题，尤其是工作 1～3 年的工程师们。\n那么，在面试中如何更好地介绍自己负责的项目？如下所示，图中这些方法是根据面试考察点总结的，并且会提示每个方法要重点体现哪些能力。\n \n\n第一步，要简单交代项目背景，让面试官可以快速进入到项目上下文，更容易理解项目架构。一般采用 STAR 法则来进行介绍：\n\nSituation 介绍项目背景，比如这个项目是研发一个短视频 APP，配合公司主客户端来交叉提高用户量与活跃度\nTask 介绍自己的任务，比如我在这个项目中负责后端服务的架构设计与研发；\nAction 介绍自己做了哪些工作，比如当时用了 2 周时间做架构设计，4 周时间做研发，2 周时间测试上线；\nResult 介绍结果，这个也是大部分人容易忽视的部分，比如项目上线后 2 个月用户数 100w，后端服务接口总量峰值 50000qps，主要接口服务 SLA p99 小于 50ms。\n\n注意背景介绍是为后面详细介绍做铺垫，简洁明了即可。这一步主要体现你的表达能力。\n第二步，重点介绍项目的架构，这也是面试官最想了解的部分。\n务必要结合架构图、交互流程图来介绍，避免对一些关键问题理解歧义。架构图要注意边界清晰，就是你的服务与其他依赖的外部服务之间的边界，以及你服务内部模块之间的边界都要描述清晰。这有利于你下一步介绍自己做了哪些内容。这一步要体现出你对项目架构的理解\n第三步，介绍你在这个项目中具体做了哪些内容，例如我设计了整个架构，或者我实现了架构图中的某几个模块。注意这一步是你面试的绝对加分点，必须要把握住。\n这里要突出你在项目中做的最有挑战的点、优雅的架构设计、或者独特有效的解决方案。比如在数据量非常大的场景下，通过优化 Redis 存储结构，减少了 70% 的 Redis 使用容量；比如对查询接口应用双发功能使 p999 降低了 60%；再比如使用了 Trace 功能来快速定位问题，等等。这一步要体现出你的实现能力与亮点。\n第四步，要为第三步介绍的优秀架构或解决方案提供证明，比如前面介绍了系统架构中使用模块化来提高扩展性，那这里就可以说系统上线后，通过模块化方式支持了 7 个新业务的接入来体现你设计的架构的优点。\n这里要注意，所有的结果必须是可以量化的，不要用性能大幅提升，极大提高灵活性这类很虚的描述。好一点的表述可以是这样：通过增加二级缓存，对后端服务的调用请求从 7000qps 降低到 600qps。这一步要体现出你对项目的掌握能力和了解程度\n第五步，思考项目存在哪些问题，或者还有哪些可以进行优化的点。\n例如，现在项目 QPS 不高，某些任务是同步处理的，会有一定效率问题，这些处理步骤是可以异步执行的，如果请求量级增加，可以考虑使用 kafka 进行异步处理。处理时还应该考虑消息重复的问题，可以把处理逻辑设计成幂等性的。这一步要体现你对项目的思考以及总结反思能力。\n如果我作为面试官，遇到一位按照上面 5 个方向来交流的候选人，一定会非常看好他。\n面试技巧再来介绍几个备战面试的小技巧。\n第一点，肯定要提前思考、提前准备\n像项目架构图怎么画更容易理解，项目中到底哪个设计最有亮点，项目还存在哪些可以改进的地方等等问题，可能要花很多时间才能找到比较理想的答案，在面试现场临时回答难度非常大。一定要根据我前面的方法，提前准备。\n第二点，要记住项目在精不在多。\n有的人在介绍项目时，会抛出好几个项目，但每个项目介绍的都很潦草。在面试中，面试官是想通过项目介绍来考察你的各方面能力，一个重点的项目就足够了。一定要选你最了解、最能代表你能力的来介绍。\n第三点，我了解的，就是我的。\n有的同学可能因为机遇的原因，没有负责过重点的项目，不过项目介绍这么重要的考察点，也不能白白在这里丢分。你可以多了解一下其他同事或团队负责的项目，只要你能把细节搞明白，把架构理解透，那么知识就是你的，依然可以拿来进行介绍。\n第四点，要重点体现对架构的理解，对设计的思考\n这会让面试官觉得你会很有潜力。你可以在介绍项目设计思路时做适当的延伸。例如你可以说：在我的业务场景下，可以容忍低概率的消息丢失，所以基于性能优先考虑，去掉了 Kafka 的 ACK 应答。如果是严格要求不丢消息的场景，我会使用同步应答，并且使用最高消息可靠性等级。\n","categories":["总结笔记"],"tags":["架构设计"]},{"title":"缓存设计中七大经典问题总结","url":"/2021_11_06_cache_7_problems/","content":"在缓存系统的设计架构中，还有很多坑，很多的明枪暗箭，如果设计不当会导致很多严重的后果。设计不当，轻则请求变慢、性能降低，重则会数据不一致、系统可用性降低，甚至会导致缓存雪崩，整个系统无法对外提供服务\n接下来将对缓存设计中的7大经典问题，如下图，进行问题描述、原因分析，并给出日常研发中，可能会出现该问题的业务场景，最后给出这些经典问题的解决方案。本文先学习缓存失效、缓存穿透与缓存雪崩。\n\n\n缓存失效问题描述缓存第一个经典问题是缓存失效。上一课时讲到，服务系统查数据，首先会查缓存，如果缓存数据不存在，就进一步查DB，最后查到数据后回种到缓存并返回。缓存的性能比DB高50～100倍以上，所以我们希望数据查询尽可能命中缓存，这样系统负荷最小，性能最佳。缓存里的数据存储基本上都是以key为索引进行存储和获取的。业务访问时，如果大量的key同时过期，很多缓存数据访问都会miss，进而穿透到DB，DB的压力就会明显上升，由于DB的性能较差，只在缓存的1%～2%以下，这样请求的慢查率会明显上升。这就是缓存失效的问题。\n原因分析导致缓存失效，特别是很多key一起失效的原因，跟我们日常写缓存的过期时间息息相关。\n在写缓存时，我们一般会根据业务的访问特点，给每种业务数据预置一个过期时间，在写缓存时把这个过期时间带上，让缓存数据在这个固定的过期时间后被淘汰。一般情况下，因为缓存数据是逐步写入的，所以也是逐步过期被淘汰的。但在某些场景，一大批数据会被系统主动或被动从DB批量加载，然后写入缓存。这些数据写入缓存时，由于使用相同的过期时间，在经历这个过期时间之后，这批数据就会一起到期，从而被缓存淘汰。此时，对这批数据的所有请求，都会出现缓存失效，从而都穿透到DB，DB由于查询量太大，就很容易压力大增，请求变慢\n业务场景很多业务场景，稍不注意，就出现大量的缓存失效，进而导致系统DB压力大、请求变慢的情况。比如同一批火车票、飞机票，当可以售卖时，系统会一次性加载到缓存，如果缓存写入时，过期时间按照预先设置的过期值，那过期时间到期后，系统就会因缓存失效出现变慢的问题。类似的业务场景还有很多，比如微博业务，会有后台离线系统，持续计算热门微博，每当计算结束，会将这批热门微博批量写入对应的缓存。还比如，很多业务，在部署新IDC或新业务上线时，会进行缓存预热，也会一次性加载大批热数据。\n解决方案对于批量key缓存失效的问题，原因既然是预置的固定过期时间，那解决方案也从这里入手。设计缓存的过期时间时，使用公式：过期时间&#x3D;baes时间+随机时间。即相同业务数据写缓存时，在基础过期时间之上，再加一个随机的过期时间，让数据在未来一段时间内慢慢过期，避免瞬时全部过期，对DB造成过大压力，如下图所示。\n  \n\n缓存穿透问题描述第二个经典问题是缓存穿透。缓存穿透是一个很有意思的问题。因为缓存穿透发生的概率很低，所以一般很难被发现。但是，一旦你发现了，而且量还不小，你可能立即就会经历一个忙碌的夜晚。因为对于正常访问，访问的数据即便不在缓存，也可以通过DB加载回种到缓存。而缓存穿透，则意味着有特殊访客在查询一个不存在的key，导致每次查询都会穿透到DB，如果这个特殊访客再控制一批肉鸡机器，持续访问你系统里不存在的key，就会对DB产生很大的压力，从而影响正常服务。\n原因分析缓存穿透存在的原因，就是因为我们在系统设计时，更多考虑的是正常访问路径，对特殊访问路径、异常访问路径考虑相对欠缺\n缓存访问设计的正常路径，是先访问cache，cache miss后查DB，DB查询到结果后，回种缓存返回。这对于正常的key访问是没有问题的，但是如果用户访问的是一个不存在的key，查DB返回空（即一个NULL），那就不会把这个空写回cache。那以后不管查询多少次这个不存在的key，都会cache miss，都会查询DB。整个系统就会退化成一个“前端+DB“的系统，由于DB的吞吐只在cache的1%~2%以下，如果有特殊访客，大量访问这些不存在的key，就会导致系统的性能严重退化，影响正常用户的访问。\n业务场景缓存穿透的业务场景很多，比如通过不存在的UID访问用户，通过不存在的车次ID查看购票信息。用户输入错误，偶尔几个这种请求问题不大，但如果是大量这种请求，就会对系统影响非常大。\n解决方案那么如何解决这种问题呢？如下图所示。\n\n第一种方案就是，查询这些不存在的数据时，第一次查DB，虽然没查到结果返回NULL，仍然记录这个key到缓存，只是这个key对应的value是一个特殊设置的值。\n第二种方案是，构建一个BloomFilter缓存过滤器，记录全量数据，这样访问数据时，可以直接通过BloomFilter判断这个key是否存在，如果不存在直接返回即可，根本无需查缓存和DB。\n\n \n\n不过这两种方案在设计时仍然有一些要注意的坑。\n\n对于方案一，如果特殊访客持续访问大量的不存在的key，这些key即便只存一个简单的默认值，也会占用大量的缓存空间，导致正常key的命中率下降。所以进一步的改进措施是，对这些不存在的key只存较短的时间，让它们尽快过期；或者将这些不存在的key存在一个独立的公共缓存，从缓存查找时，先查正常的缓存组件，如果miss，则查一下公共的非法key的缓存，如果后者命中，直接返回，否则穿透DB，如果查出来是空，则回种到非法key缓存，否则回种到正常缓存。\n对于方案二，BloomFilter要缓存全量的key，这就要求全量的key数量不大，10亿条数据以内最佳，因为10亿条数据大概要占用1.2GB的内存。也可以用BloomFilter缓存非法key，每次发现一个key是不存在的非法key，就记录到BloomFilter中，这种记录方案，会导致BloomFilter存储的key持续高速增长，为了避免记录key太多而导致误判率增大，需要定期清零处理。\n\nBloomFilterBloomFilter是一个非常有意思的数据结构，不仅仅可以挡住非法key攻击，还可以低成本、高性能地对海量数据进行判断，比如一个系统有数亿用户和百亿级新闻feed，就可以用BloomFilter来判断某个用户是否阅读某条新闻feed。下面来对BloomFilter数据结构做一个分析，如下图所示。\n  \n\nBloomFilter的目的是检测一个元素是否存在于一个集合内。它的原理，是用bit数据组来表示一个集合，对一个key进行多次不同的Hash检测，如果所有Hash对应的bit位都是1，则表明key非常大概率存在，平均单记录占用1.2字节即可达到99%，只要有一次Hash对应的bit位是0，就说明这个key肯定不存在于这个集合内。\nBloomFilter的算法是，首先分配一块内存空间做bit数组，数组的bit位初始值全部设为0，加入元素时，采用k个相互独立的Hash函数计算，然后将元素Hash映射的K个位置全部设置为1。检测key时，仍然用这k个Hash函数计算出k个位置，如果位置全部为1，则表明key存在，否则不存在。\nBloomFilter的优势是，全内存操作，性能很高。另外空间效率非常高，要达到1%的误判率，平均单条记录占用1.2字节即可。而且，平均单条记录每增加0.6字节，还可让误判率继续变为之前的1&#x2F;10，即平均单条记录占用1.8字节，误判率可以达到1&#x2F;1000；平均单条记录占用2.4字节，误判率可以到1&#x2F;10000，以此类推。这里的误判率是指，BloomFilter判断某个key存在，但它实际不存在的概率，因为它存的是key的Hash值，而非key的值，所以有概率存在这样的key，它们内容不同，但多次Hash后的Hash值都相同。对于BloomFilter判断不存在的key ，则是100%不存在的，反证法，如果这个key存在，那它每次Hash后对应的Hash值位置肯定是1，而不会是0。\n缓存雪崩问题描述第三个经典问题是缓存雪崩。系统运行过程中，缓存雪崩是一个非常严重的问题。缓存雪崩是指部分缓存节点不可用，导致整个缓存体系甚至甚至服务系统不可用的情况。缓存雪崩按照缓存是否rehash（即是否漂移）分两种情况：\n\n缓存不支持rehash导致的系统雪崩不可\n缓存支持rehash导致的缓存雪崩不可用\n\n原因分析在上述两种情况中，缓存不进行rehash时产生的雪崩，一般是由于较多缓存节点不可用，请求穿透导致DB也过载不可用，最终整个系统雪崩不可用的。而缓存支持rehash时产生的雪崩，则大多跟流量洪峰有关，流量洪峰到达，引发部分缓存节点过载Crash，然后因rehash扩散到其他缓存节点，最终整个缓存体系异常。\n第一种情况比较容易理解，缓存节点不支持rehash，较多缓存节点不可用时，大量Cache访问会失败，根据缓存读写模型，这些请求会进一步访问DB，而且DB可承载的访问量要远比缓存小的多，请求量过大，就很容易造成DB过载，大量慢查询，最终阻塞甚至Crash，从而导致服务异常。\n第二种情况是怎么回事呢？这是因为缓存分布设计时，很多同学会选择一致性Hash分布方式，同时在部分节点异常时，采用rehash策略，即把异常节点请求平均分散到其他缓存节点。在一般情况下，一致性Hash分布+rehash策略可以很好得运行，但在较大的流量洪峰到临之时，如果大流量key比较集中，正好在某1～2个缓存节点，很容易将这些缓存节点的内存、网卡过载，缓存节点异常Crash，然后这些异常节点下线，这些大流量key请求又被rehash到其他缓存节点，进而导致其他缓存节点也被过载Crash，缓存异常持续扩散，最终导致整个缓存体系异常，无法对外提供服务。\n业务场景缓存雪崩的业务场景并不少见，微博、Twitter等系统在运行的最初若干年都遇到过很多次。比如，微博最初很多业务缓存采用一致性Hash+rehash策略，在突发洪水流量来临时，部分缓存节点过载Crash甚至宕机，然后这些异常节点的请求转到其他缓存节点，又导致其他缓存节点过载异常，最终整个缓存池过载。另外，机架断电，导致业务缓存多个节点宕机，大量请求直接打到DB，也导致DB过载而阻塞，整个系统异常。最后缓存机器复电后，DB重启，数据逐步加热后，系统才逐步恢复正常。\n解决方案预防缓存雪崩，这里给出3个解决方案。\n\n方案一，对业务DB的访问增加读写开关，当发现DB请求变慢、阻塞，慢请求超过阀值时，就会关闭读开关，部分或所有读DB的请求进行failfast立即返回，待DB恢复后再打开读开关，如下图。\n    \n\n方案二，对缓存增加多个副本，缓存异常或请求miss后，再读取其他缓存副本，而且多个缓存副本尽量部署在不同机架，从而确保在任何情况下，缓存系统都会正常对外提供服务。\n\n方案三，对缓存体系进行实时监控，当请求访问的慢速比超过阀值时，及时报警，通过机器替换、服务替换进行及时恢复；也可以通过各种自动故障转移策略，自动关闭异常接口、停止边缘服务、停止部分非核心功能措施，确保在极端场景下，核心功能的正常运行。\n\n\n实际上，微博平台系统，这三种方案都采用了，通过三管齐下，规避缓存雪崩的发生。\n数据不一致问题描述七大缓存经典问题的第四个问题是数据不一致。同一份数据，可能会同时存在DB和缓存之中。那就有可能发生，DB和缓存的数据不一致。如果缓存有多个副本，多个缓存副本里的数据也可能会发生不一致现象\n原因分析不一致的问题大多跟缓存更新异常有关。比如更新DB后，写缓存失败，从而导致缓存中存的是老数据。另外，如果系统采用一致性Hash分布，同时采用rehash自动漂移策略，在节点多次上下线之后，也会产生脏数据。缓存有多个副本时，更新某个副本失败，也会导致这个副本的数据是老数据。\n业务场景导致数据不一致的场景也不少。如下图所示，在缓存机器的带宽被打满，或者机房网络出现波动时，缓存更新失败，新数据没有写入缓存，就会导致缓存和DB的数据不一致。缓存rehash时，某个缓存机器反复异常，多次上下线，更新请求多次rehash。这样，一份数据存在多个节点，且每次rehash只更新某个节点，导致一些缓存节点产生脏数据。\n\n\n解决方案要尽量保证数据的一致性。这里也给出了3个方案，可以根据实际情况进行选择。\n\n第一个方案，cache更新失败后，可以进行重试，如果重试失败，则将失败的key写入队列机服务，待缓存访问恢复后，将这些key从缓存删除。这些key在再次被查询时，重新从DB加载，从而保证数据的一致性。\n第二个方案，缓存时间适当调短，让缓存数据及早过期后，然后从DB重新加载，确保数据的最终一致性。\n第三个方案，不采用rehash漂移策略，而采用缓存分层策略，尽量避免脏数据产生。\n\n\n\n数据并发竞争问题描述第五个经典问题是数据并发竞争。互联网系统，线上流量较大，缓存访问中很容易出现数据并发竞争的现象。数据并发竞争，是指在高并发访问场景，一旦缓存访问没有找到数据，大量请求就会并发查询DB，导致DB压力大增的现象。\n数据并发竞争，主要是由于多个进程&#x2F;线程中，有大量并发请求获取相同的数据，而这个数据key因为正好过期、被剔除等各种原因在缓存中不存在，这些进程&#x2F;线程之间没有任何协调，然后一起并发查询DB，请求那个相同的key，最终导致DB压力大增，如下图。\n\n\n业务场景数据并发竞争在大流量系统也比较常见，比如车票系统，如果某个火车车次缓存信息过期，但仍然有大量用户在查询该车次信息。又比如微博系统中，如果某条微博正好被缓存淘汰，但这条微博仍然有大量的转发、评论、赞。上述情况都会造成该车次信息、该条微博存在并发竞争读取的问题\n解决方案要解决并发竞争，有2种方案。\n\n方案一是使用全局锁。如下图所示，即当缓存请求miss后，先尝试加全局锁，只有加全局锁成功的线程，才可以到DB去加载数据。其他进程&#x2F;线程在读取缓存数据miss时，如果发现这个key有全局锁，就进行等待，待之前的线程将数据从DB回种到缓存后，再从缓存获取。\n\n\n\n\n方案二是，对缓存数据保持多个备份，即便其中一个备份中的数据过期或被剔除了，还可以访问其他备份，从而减少数据并发竞争的情况，如下图。\n\n\n\n\nHot key问题描述第六个经典问题是Hot key。对于大多数互联网系统，数据是分冷热的。比如最近的新闻、新发表的微博被访问的频率最高，而比较久远的之前的新闻、微博被访问的频率就会小很多。而在突发事件发生时，大量用户同时去访问这个突发热点信息，访问这个Hot key，这个突发热点信息所在的缓存节点就很容易出现过载和卡顿现象，甚至会被Crash。\n原因分析Hot key引发缓存系统异常，主要是因为突发热门事件发生时，超大量的请求访问热点事件对应的key，比如微博中数十万、数百万的用户同时去吃一个新瓜。数十万的访问请求同一个key，流量集中打在一个缓存节点机器，这个缓存机器很容易被打到物理网卡、带宽、CPU的极限，从而导致缓存访问变慢、卡顿。\n业务场景引发Hot key的业务场景很多，比如明星结婚、离婚、出轨这种特殊突发事件，比如奥运、春节这些重大活动或节日，还比如秒杀、双12、618等线上促销活动，都很容易出现Hot key的情况。\n解决方案要解决这种极热key的问题，首先要找出这些Hot key来。对于重要节假日、线上促销活动、集中推送这些提前已知的事情，可以提前评估出可能的热key来。而对于突发事件，无法提前评估，可以通过Spark，对应流任务进行实时分析，及时发现新发布的热点key。而对于之前已发出的事情，逐步发酵成为热key的，则可以通过Hadoop对批处理任务离线计算，找出最近历史数据中的高频热key。\n找到热key后，就有很多解决办法了。首先可以将这些热key进行分散处理，比如一个热key名字叫hotkey，可以被分散为hotkey#1、hotkey#2、hotkey#3，……hotkey#n，这n个key分散存在多个缓存节点，然后client端请求时，随机访问其中某个后缀的hotkey，这样就可以把热key的请求打散，避免一个缓存节点过载，如下图所示。\n \n\n其次，也可以key的名字不变，对缓存提前进行多副本+多级结合的缓存架构设计。\n再次，如果热key较多，还可以通过监控体系对缓存的SLA实时监控，通过快速扩容来减少热key的冲击。\n最后，业务端还可以使用本地缓存，将这些热key记录在本地缓存，来减少对远程缓存的冲击。\nBig key问题描述最后一个经典问题是Big key，也就是大Key的问题。大key，是指在缓存访问时，部分Key的Value过大，读写、加载易超时的现象。\n原因分析造成这些大key慢查询的原因很多。如果这些大key占总体数据的比例很小，存Mc，对应的slab较少，导致很容易被频繁剔除，DB反复加载，从而导致查询较慢。如果业务中这种大key很多，而这种key被大量访问，缓存组件的网卡、带宽很容易被打满，也会导致较多的大key慢查询。另外，如果大key缓存的字段较多，每个字段的变更都会引发对这个缓存数据的变更，同时这些key也会被频繁地读取，读写相互影响，也会导致慢查现象。最后，大key一旦被缓存淘汰，DB加载可能需要花费很多时间，这也会导致大key查询慢的问题。\n业务场景大key的业务场景也比较常见。比如互联网系统中需要保存用户最新1万个粉丝的业务，比如一个用户个人信息缓存，包括基本资料、关系图谱计数、发feed统计等。微博的feed内容缓存也很容易出现，一般用户微博在140字以内，但很多用户也会发表1千字甚至更长的微博内容，这些长微博也就成了大key，如下图。\n  \n\n解决方案对于大key，给出3种解决方案。\n\n第一种方案，如果数据存在Mc中，可以设计一个缓存阀值，当value的长度超过阀值，则对内容启用压缩，让KV尽量保持小的size，其次评估大key所占的比例，在Mc启动之初，就立即预写足够数据的大key，让Mc预先分配足够多的trunk size较大的slab。确保后面系统运行时，大key有足够的空间来进行缓存。\n\n\n\n\n第二种方案，如果数据存在Redis中，比如业务数据存set格式，大key对应的set结构有几千几万个元素，这种写入Redis时会消耗很长的时间，导致Redis卡顿。此时，可以扩展新的数据结构，同时让client在这些大key写缓存之前，进行序列化构建，然后通过restore一次性写入，如下图所示。\n\n\n\n\n第三种方案时，如下图所示，将大key分拆为多个key，尽量减少大key的存在。同时由于大key一旦穿透到DB，加载耗时很大，所以可以对这些大key进行特殊照顾，比如设置较长的过期时间，比如缓存内部在淘汰key时，同等条件下，尽量不淘汰这些大key。\n\n\n\n至此，本文关于缓存的7大经典问题全部介绍完了。\n我们要认识到，对于互联网系统，由于实际业务场景复杂，数据量、访问量巨大，需要提前规避缓存使用中的各种坑。你可以通过提前熟悉Cache的经典问题，提前构建防御措施， 避免大量key同时失效，避免不存在key访问的穿透，减少大key、热key的缓存失效，对热key进行分流。你可以采取一系列措施，让访问尽量命中缓存，同时保持数据的一致性。另外，你还可以结合业务模型，提前规划cache系统的SLA，如QPS、响应分布、平均耗时等，实施监控，以方便运维及时应对。在遇到部分节点异常，或者遇到突发流量、极端事件时，也能通过分池分层策略、key分拆等策略，避免故障发生。\n最终，你能在各种复杂场景下，面对高并发、海量访问，面对突发事件和洪峰流量，面对各种网络或机器硬件故障，都能保持服务的高性能和高可用。\n","categories":["总结笔记"],"tags":["缓存雪崩","缓存穿透","缓存失效","数据不一致","数据并发竞争","Hot key","Big key"]},{"title":"ThreadLocal分析","url":"/2022_02_10_threadlocal/","content":"ThreadLocal介绍ThreadLocal是什么ThreadLocal本地线程变量,线程自带的变量副本(实现了每一个线程副本都有一个专属的本地变量,主要解决的就是让每一个线程绑定自己的值,自己用自己的,不跟别人争抢。通过使用get()和set()方法,获取默认值或将其值更改为当前线程所存的副本的值从而避免了线程安全的问题)\nsynchronized或者lock,有个管理员,好比,现在大家签到,多个同学(线程),但是只有一只笔,只能同一个时间,只有一个线程(同学)签到,加锁(同步机制是以时间换空间,执行时间不一样,类似于排队)\nThreadLocal,人人有份,每个同学手上都有一支笔,自己用自己的,不用再加锁来维持秩序(同步机制是以空间换时间,为每一个线程都提供了一份变量的副本,从而实现同时访问,互不干扰同时访问,肯定效率高啊)\napi介绍\n\nprotected T initialValue?():initialValue():返回此线程局部变量的当前线程的”初始值”(对于initialValue()较为老旧,jdk1.8又加入了withInitial()方法)\n\nstatic ThreadLocal withInitial?(Supplier supplier):创建线程局部变量\n\nT get?():返回当前线程的此线程局部变量的副本中的值\n\nvoid set?(T value):将当前线程的此线程局部变量的副本设置为指定的值\n\nvoid remove?():删除此线程局部变量的当前线程的值\n\n\nThreadLocal源码分析Thread|ThreadLocal|ThreadLocalMap关系\nThread和ThreadLocal\nThreadLocal和ThreadLocalMap\n三者总概括\n\n\nThread类中有一个ThreadLocal.ThreadLocalMap threadLocals &#x3D; null的变量,这个ThreadLocal相当于是Thread类和ThreadLocalMap的桥梁,在ThreadLocal中有静态内部类ThreadLocalMap,ThreadLocalMap中有Entry数组\n当我们为threadLocal变量赋值,实际上就是以当前threadLocal实例为key,值为value的Entry往这个threadLocalMap中存放\nt.threadLocals &#x3D; new ThreadLocalMap(this, firstValue) 如下这行代码,可以知道每个线程都会创建一个ThreadLocalMap对象,每个线程都有自己的变量副本\n\n核心代码//核心代码说明public void set(T value) &#123;    Thread t = Thread.currentThread();    ThreadLocalMap map = getMap(t);    if (map != null)        map.set(this, value);    else        createMap(t, value);&#125;void createMap(Thread t, T firstValue) &#123;   t.threadLocals = new ThreadLocalMap(this, firstValue);&#125;ThreadLocalMap(ThreadLocal&lt;?&gt; firstKey, Object firstValue) &#123;    table = new Entry[INITIAL_CAPACITY];    int i = firstKey.threadLocalHashCode &amp; (INITIAL_CAPACITY - 1);    table[i] = new Entry(firstKey, firstValue);    size = 1;    setThreshold(INITIAL_CAPACITY);&#125;\n\nset方法详解\n首先获取当前线程,并根据当前线程获取一个Map\n如果获取的Map不为空,则将参数设置到Map中(当前ThreadLocal的引用作为key)\n如果Map为空,则给该线程创建 Map,并设置初始值/**     * 设置当前线程对应的ThreadLocal的值     *     * @param value 将要保存在当前线程对应的ThreadLocal的值     */    public void set(T value) &#123;        // 获取当前线程对象        Thread t = Thread.currentThread();        // 获取此线程对象中维护的ThreadLocalMap对象        ThreadLocalMap map = getMap(t);        // 判断map是否存在        if (map != null)            // 存在则调用map.set设置此实体entry            map.set(this, value);        else            // 1)当前线程Thread 不存在ThreadLocalMap对象            // 2)则调用createMap进行ThreadLocalMap对象的初始化            // 3)并将 t(当前线程)和value(t对应的值)作为第一个entry存放至ThreadLocalMap中            createMap(t, value);    &#125; /**     * 获取当前线程Thread对应维护的ThreadLocalMap      *      * @param  t the current thread 当前线程     * @return the map 对应维护的ThreadLocalMap      */    ThreadLocalMap getMap(Thread t) &#123;        return t.threadLocals;    &#125;\t/**     *创建当前线程Thread对应维护的ThreadLocalMap      *     * @param t 当前线程     * @param firstValue 存放到map中第一个entry的值     */\tvoid createMap(Thread t, T firstValue) &#123;        //这里的this是调用此方法的threadLocal        t.threadLocals = new ThreadLocalMap(this, firstValue);    &#125;    \t /*\t  * firstKey : 本ThreadLocal实例(this)\t  * firstValue ： 要保存的线程本地变量\t  */\tThreadLocalMap(ThreadLocal&lt;?&gt; firstKey, Object firstValue) &#123;\t        //初始化table\t        table = new ThreadLocal.ThreadLocalMap.Entry[INITIAL_CAPACITY];\t        //计算索引(重点代码)\t        int i = firstKey.threadLocalHashCode &amp; (INITIAL_CAPACITY - 1);\t        //设置值\t        table[i] = new ThreadLocal.ThreadLocalMap.Entry(firstKey, firstValue);\t        size = 1;\t        //设置阈值\t        setThreshold(INITIAL_CAPACITY);\t    &#125;\n\nget方法详解先获取当前线程的ThreadLocalMap变量,如果存在则返回值,不存在则创建并返回初始值\n/**   * 返回当前线程中保存ThreadLocal的值   * 如果当前线程没有此ThreadLocal变量,   * 则它会通过调用&#123;@link #initialValue&#125; 方法进行初始化值   *   * @return 返回当前线程对应此ThreadLocal的值   */  public T get() &#123;      // 获取当前线程对象      Thread t = Thread.currentThread();      // 获取此线程对象中维护的ThreadLocalMap对象      ThreadLocalMap map = getMap(t);      // 如果此map存在      if (map != null) &#123;          // 以当前的ThreadLocal 为 key,调用getEntry获取对应的存储实体e          ThreadLocalMap.Entry e = map.getEntry(this);          // 对e进行判空           if (e != null) &#123;              @SuppressWarnings(&quot;unchecked&quot;)              // 获取存储实体 e 对应的 value值              // 即为我们想要的当前线程对应此ThreadLocal的值              T result = (T)e.value;              return result;          &#125;      &#125;      /*      \t初始化 : 有两种情况有执行当前代码      \t第一种情况: map不存在,表示此线程没有维护的ThreadLocalMap对象      \t第二种情况: map存在, 但是没有与当前ThreadLocal关联的entry       */      return setInitialValue();  &#125;  /**   * 初始化   *   * @return the initial value 初始化后的值   */  private T setInitialValue() &#123;      // 调用initialValue获取初始化的值      // 此方法可以被子类重写, 如果不重写默认返回null      T value = initialValue();      // 获取当前线程对象      Thread t = Thread.currentThread();      // 获取此线程对象中维护的ThreadLocalMap对象      ThreadLocalMap map = getMap(t);      // 判断map是否存在      if (map != null)          // 存在则调用map.set设置此实体entry          map.set(this, value);      else          // 1)当前线程Thread 不存在ThreadLocalMap对象          // 2)则调用createMap进行ThreadLocalMap对象的初始化          // 3)并将 t(当前线程)和value(t对应的值)作为第一个entry存放至ThreadLocalMap中          createMap(t, value);      // 返回设置的值value      return value;\nremove方法详解\n首先获取当前线程,并根据当前线程获取一个Map\n如果获取的Map不为空,则移除当前ThreadLocal对象对应的entry/**     * 删除当前线程中保存的ThreadLocal对应的实体entry     */     public void remove() &#123;        // 获取当前线程对象中维护的ThreadLocalMap对象         ThreadLocalMap m = getMap(Thread.currentThread());        // 如果此map存在         if (m != null)            // 存在则调用map.remove            // 以当前ThreadLocal为key删除对应的实体entry             m.remove(this);     &#125;\n\nThreadLocal内存泄漏问题为什么源代码用弱引用？\n当function01方法执行完毕后,栈帧销毁强引用 tl 也就没有了。但此时线程的ThreadLocalMap里某个entry的key引用还指向这个对象\n若这个key引用是强引用,就会导致key指向的ThreadLocal对象及v指向的对象不能被gc回收,造成内存泄漏\n若这个key引用是弱引用就大概率会减少内存泄漏的问题(还有一个key为null的雷)。使用弱引用,就可以使ThreadLocal对象在方法执行完毕后顺利被回收且Entry的key引用指向为null\n\nkey为null的entry,原理解析\nThreadLocalMap使用ThreadLocal的弱引用作为key,如果一个ThreadLocal没有外部强引用引用他,那么系统gc的时候,这个ThreadLocal势必会被回收,这样一来,ThreadLocalMap中就会出现key为null的Entry,就没有办法访问这些key为null的Entry的value,如果当前线程再迟迟不结束的话(比如正好用在线程池),这些key为null的Entry的value就会一直存在一条强引用链\n虽然弱引用,保证了key指向的ThreadLocal对象能被及时回收,但是v指向的value对象是需要ThreadLocalMap调用get、set时发现key为null时才会去回收整个entry、value\n因此弱引用不能100%保证内存不泄露。我们要在不使用某个ThreadLocal对象后,手动调用remoev方法来删除它,尤其是在线程池中,不仅仅是内存泄露的问题,因为线程池中的线程是重复使用的,意味着这个线程的ThreadLocalMap对象也是重复使用的,如果我们不手动调用remove方法,那么后面的线程就有可能获取到上个线程遗留下来的value值,造成bug\n如果当前thread运行结束,threadLocal,threadLocalMap, Entry没有引用链可达,在垃圾回收的时候都会被系统进行回收\n但在实际使用中我们有时候会用线程池去维护我们的线程,比如在Executors.newFixedThreadPool()时创建线程的时候,为了复用线程是不会结束的,所以threadLocal内存泄漏就值得我们小心\n出现内存泄漏的真实原因 (1). 没有手动删除这个Entry (2). CurrentThread依然运行\n\nset、get方法会去检查所有键为null的Entry对象\n结论(在finally后面调用remove方法)\nThreadLocal小总结\nThreadLocal本地线程变量,以空间换时间,线程自带的变量副本,人手一份,避免了线程安全问题\n每个线程持有一个只属于自己的专属Map并维护了Thread Local对象与具体实例的映射,该Map由于只被持有它的线程访问,故不存在线程安全以及锁的问题3.ThreadLocalMap的Entry对ThreadLocal的引用为弱引用,避免了ThreadLocal对象无法被回收的问题\n都会通过expungeStaleEntry,cleanSomeSlots, replace StaleEntry这三个方法回收键为 null 的 Entry 对象的值(即为具体实例)以及 Entry 对象本身从而防止内存泄漏,属于安全加固的方法\n用完之后一定要remove操作\n\n使用案例解决SimpleDateFormat线程不安全//线程安全做法package h.xd.util;import java.text.ParseException;import java.text.SimpleDateFormat;import java.time.LocalDateTime;import java.time.format.DateTimeFormatter;import java.util.Date;public class ThreadLocalDateUtils &#123;    public static final ThreadLocal&lt;SimpleDateFormat&gt;sdfThreadLocal=            ThreadLocal.withInitial(()-&gt;new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;));    public static Date parseByThreadLocal(String stringDate) throws ParseException &#123;        return sdfThreadLocal.get().parse(stringDate);    &#125;    //DateTimeFormatter 代替 SimpleDateFormat    public static final DateTimeFormatter DATE_TIME_FORMAT = DateTimeFormatter.ofPattern(&quot;yyyy-MM-dd HH:mm:ss&quot;);    public static String formatForDateTime(LocalDateTime localDateTime) &#123;        return DATE_TIME_FORMAT.format(localDateTime);    &#125;    public static LocalDateTime parseForDateTime(String dateString) &#123;        return LocalDateTime.parse(dateString,DATE_TIME_FORMAT);    &#125;    public static void main(String[] args) throws Exception&#123;        for (int i = 1; i &lt;=3; i++) &#123;            new Thread(()-&gt;&#123;                try &#123;                    System.out.println(ThreadLocalDateUtils.parseByThreadLocal(&quot;2021-03-30 11:20:30&quot;));                    System.out.println(ThreadLocalDateUtils.parseForDateTime(&quot;2021-03-30 11:20:30&quot;));                     System.out.println(ThreadLocalDateUtils.formatForDateTime(LocalDateTime.now()));                &#125; catch (Exception e) &#123;                    e.printStackTrace();                &#125;finally &#123;                    ThreadLocalDateUtils.sdfThreadLocal.remove();                &#125;            &#125;,String.valueOf(i)).start();        &#125;    &#125;&#125;\n\n//不安全做法package h.xd.util;import java.text.ParseException;import java.text.SimpleDateFormat;import java.util.Date;public class DateUtils &#123;    public static SimpleDateFormat sdf=new SimpleDateFormat(&quot;yyyy-MM-dd HH:mm:ss&quot;);    /**     解决方案一:加入synchronized,用时间换空间,效率低     */    /**     如果不加会导致线程安全问题,SimpleDateFormat类内部有一个Calendar对象引用,     SimpleDateFormat相关的日期信息,例如sdf.parse(dateStr),sdf.format(date)     诸如此类的方法参数传入的日期相关String,Date等等, 都是交由Calendar引用来储存的.     这样就会导致一个问题如果你的SimpleDateFormat是个static的,那么多个thread之间     就会共享这个SimpleDateFormat,同时也是共享这个Calendar引用(相当于买票案列)     */    //public static synchronized Date parse(String stringDate) throws ParseException &#123;    public static  Date parse(String stringDate) throws ParseException &#123;        System.out.println(sdf.parse(stringDate));        return sdf.parse(stringDate);    &#125;    public static void main(String[] args) throws Exception&#123;        for (int i = 1; i &lt;=3; i++) &#123;            new Thread(()-&gt;&#123;                try &#123;                    DateUtils.parse(&quot;2021-03-30 11:20:30&quot;);                &#125; catch (Exception e) &#123;                    e.printStackTrace();                &#125;            &#125;,String.valueOf(i)).start();        &#125;    &#125;&#125;\n\n解决每个请求一个线程安全的连接问题// TestDao.javapackage com.h.xd.threadlocal;public class TestDao &#123;    private static ThreadLocal&lt;MConnection&gt; connectionThreadLocal = ThreadLocal.withInitial(TestDao::createConnection);    private static MConnection createConnection()&#123;        MConnection mConnection = new MConnection();        System.out.println(&quot;实例化了一个对象： &quot; + mConnection);        return mConnection;    &#125;    public static MConnection getConnection()&#123;        return connectionThreadLocal.get();    &#125;&#125;\n//MConnection.javapackage com.h.xd.threadlocal;public class MConnection &#123;&#125;\n\n//客户端调用，这里用springmvc的一个请求package com.h.xd.controller;import com.h.xd.threadlocal.MConnection;import com.h.xd.threadlocal.TestDao;import lombok.extern.slf4j.Slf4j;import org.springframework.web.bind.annotation.RequestMapping;import org.springframework.web.bind.annotation.RestController;@RestController@RequestMapping(value = &quot;/thread&quot;)@Slf4jpublic class ThreadController &#123;    @RequestMapping(value = &quot;&quot;)    public void home()&#123;        MConnection connection = TestDao.getConnection();        log.info(&quot;&#123;&#125;&quot;,connection);    &#125;&#125;\n","categories":["应用笔记"],"tags":["java","thread","ThreadLocal"]},{"title":"Java泛型梳理","url":"/2022_02_13_java_wild_type/","content":"在java中，泛型算是必须要掌握的一块硬核知识，在很多地方都会用到，这块如果理解了，在阅读其他框架源码的时候会让你更容易一些。\n关于泛型的解析上面，我们需要先了解一些类和接口，这些比较关键，这些都位于java.lang.reflect包中，类图如下：\nreflect包中的一些类Type接口这是一个顶层接口，java中的任何类型都可以用这个来表示，这个接口是Java编程语言中所有类型的公共超接口。这些类型包括原始类型、泛型类型、泛型变量类型、通配符类型、泛型数组类型、数组类型等各种类型。\n这个接口代码比较简单，源码：\npackage java.lang.reflect;/** * Type is the common superinterface for all types in the Java * programming language. These include raw types, parameterized types, * array types, type variables and primitive types. * * @since 1.5 */public interface Type &#123;    /**     * Returns a string describing this type, including information     * about any type parameters.     *     * @implSpec The default implementation calls &#123;@code toString&#125;.     *     * @return a string describing this type     * @since 1.8     */    default String getTypeName() &#123;        return toString();    &#125;&#125;\n\ngetTypeName()，用于返回具体类型的名称，是一个默认方法，默认会调用当前类的toString方法，实现类也可以对这个方法重写。\n\nGenericDeclaration接口所有声明泛型变量的公共接口，这个接口中定义了一个方法：\npackage java.lang.reflect;/** * A common interface for all entities that declare type variables. * * @since 1.5 */public interface GenericDeclaration extends AnnotatedElement &#123;    /**     * Returns an array of &#123;@code TypeVariable&#125; objects that     * represent the type variables declared by the generic     * declaration represented by this &#123;@code GenericDeclaration&#125;     * object, in declaration order.  Returns an array of length 0 if     * the underlying generic declaration declares no type variables.     *     * @return an array of &#123;@code TypeVariable&#125; objects that represent     *     the type variables declared by this generic declaration     * @throws GenericSignatureFormatError if the generic     *     signature of this generic declaration does not conform to     *     the format specified in     *     &lt;cite&gt;The Java&amp;trade; Virtual Machine Specification&lt;/cite&gt;     */    public TypeVariable&lt;?&gt;[] getTypeParameters();&#125;\n\n\ngetTypeParameters(),这个方法用于获取声明的泛型变量类型清单。\n泛型变量可以在类和方法中进行声明，从上面类图中也可以看出来，java中任何类可以使用Class对象表示，方法可以用Method类表示，类图中可以知，Class类和Method类实现了GenericDeclaration接口，所以可以调用他们的getTypeParameters方法获取其声明的泛型参数列表。\n\npublic class Demo1&lt;T1, T2 extends Integer, T3 extends Demo1I1 &amp; Demo1I2&gt;\n\n上面代码表示Demo1这个类中声明了3个泛型变量类型：T1、T2、T3，所以如果去调用这个类的Clas对象中的getTypeParameters方法可以获取到这三个泛型变量的信息。\n\npublic &lt;T1, T2 extends Integer, T3 extends Demo2I1 &amp; Demo2I2&gt; T3 m1(T1 t1, T2 t2, T3 t3, String s) &#123;    return t3;&#125;\n\n上面m1方法中声明了三个泛型类型变量：T1、T2、T3；java中可以方法的任何信息都可以通过Method对象来获取，Mehod类实现了GenericDeclaration接口，所以Method类中实现了GenericDeclaration接口中的getTypeParameters方法，调用这个方法就可以获取m1方法中3个泛型变量类型的信息。\n\nClass类Class类的对象表示JVM中一个类或者接口，每个java对象被加载到jvm中都会表现为一个Class类型的对象，java中的数组也被映射为Class对象，所有元素类型相同且维数相同的数组都共享一个class对象，通过Class对象可以获取类或者接口中的任何信息，比如：类名、类中声明的泛型信息、类的修饰符、类的父类信息、类的接口信息、类中的任何方法信息、类中任何字段信息等等。\n\nClass对象获取方式在程序中我们可以通过3中方式获取Class对象：1.类名.class2.对象.getClass()3.Class.forName(&quot;类或者接口的完整名称&quot;)\n\n常用的方法:\nField[] getFields()这个方法会返回当前类的以及其所有父类、父类的父类中所有public类型的字段。\n\nField[] getDeclaredFields()这个方法会返回当前类中所有字段（和修饰符无关），也就说不管这个字段是public还是private或者是protected，都会返回，有一点需要注意，只返回自己内部定义的字段，不包含其父类中的，这点需要注意，和getFields是有区别的。\n\nMethod[] getMethods()这个方法会返回当前类的以及其所有父类的、父类的父类的、自己实现的接口、父接口继承的接口中的所有public类型的方法，需要注意一下，接口中的方法默认都是public类型的，接口中的方法public修饰符是可以省略的。\n\nMethod[] getDeclaredMethods()返回当前类中定义的所有方法，不管这个方法修饰符是什么类型的，注意只包含自己内部定义的方法，不包含当前类的父类或者其实现的接口中定义的。\n\nType getGenericSuperclass()返回父类的类型信息，如果父类是泛型类型，会返回超类中泛型的详细信息，这个方法比较关键，后面会有详细案例。\n\nTypeVariable&lt;Class&gt;[] getTypeParameters()Class类继承了java.lang.reflect.GenericDeclaration接口，上面这个方法是在GenericDeclaration接口中定义的，Class类中实现了这个接口，用于返回当前类中声明的泛型变量参数列表。\n\n\nMethod类这个类用来表示java中的任何一个方法，通过这个类可以获取java中方法的任何信息，比如：方法的修饰符、方法名称、方法的参数、方法返回值、方法中声明的泛型参数列表等方法的一切信息。\n常用的方法\nString getName()用来获取方法的名称。\n\nType[] getGenericParameterTypes()返回方法的参数信息，如果参数是泛型类型的，会返回泛型的详细信息，这个方法后面会演示。\n\nType getGenericReturnType()返回方法的返回值类型，如果返回值是泛型的，会包含泛型的详细信息。\n\nTypeVariable[] getTypeParameters()Method类继承了java.lang.reflect.GenericDeclaration接口，上面这个方法是在GenericDeclaration接口中定义的，Method类中实现了这个接口，用于返回当前方法中声明的泛型变量参数列表。\n\n\nField类这个类用来表示java中的字段，通过这个类可以获取java中字段的任何信息，比如：字段的修饰符、字段名称、字段类型、泛型字段的类型等字段的一切信息。\n常用的方法\nString getName()获取字段的名称。\n\nClass&lt;?&gt; getType()获取字段类型所属的Class对象。\n\nType getGenericType()获取字段的类型，如果字段是泛型类型的，会返回泛型类型的详细信息；如果字段不是泛型类型的，和getType返回的结果是一样的。\n\nClass&lt;?&gt; getDeclaringClass()获取这个字段是在哪个类中声明的，也就是当前字段所属的类。\n\n\nParameterizedType接口这个接口表示参数化类型，例如List、Map&lt;Integer,String&gt;、UserMapper这种带有泛型的类型。\n常用方法\nType[] getActualTypeArguments()获取泛型类型中的类型列表，就是&lt;&gt;中包含的参数列表，如：List泛型类型列表只有一个是String，而Map&lt;Integer,String&gt;泛型类型中包含2个类型：Integer和String，UserMapper泛型类型为UserModel，实际上就是&lt;和&gt;中间包含的类型列表。\n\nType getRawType()返回参数化类型中的原始类型，比如：List的原始类型为List，UserMapper原始类型为UserMapper，也就是&lt;符号前面的部分。\n\nType[]  getOwnerType()\n返回当前类型所属的类型。例如存在A&lt;T&gt;类，其中定义了内部类InnerA&lt;I&gt;, 则InnerA&lt;I&gt;所属的类型为A&lt;I&gt;，如果是顶层类型则返回null。这种关系比较常见的示例是Map&lt;K,V&gt;接口与Map.Entry&lt;K,V&gt;接口，Map&lt;K,V&gt;接口是Map.Entry&lt;K,V&gt;接口的所有者。\n\nTypeVariable接口这个接口表示的是泛型变量，例如：List中的T就是类型变量；而class C1&lt;T1,T2,T3&gt;{}表示一个类，这个类中定义了3个泛型变量类型，分别是T1、T2和T2，泛型变量在java中使用TypeVariable接口来表示，可以通过这个接口提供的方法获取泛型变量类型的详细信息。\n常用的方法\nType[] getBounds()获取泛型变量类型的上边界，如果未明确什么上边界默认为Object。例如：class Test中K的上边界只有一个，是Person；而class Test&lt;T extend List &amp; Iterable&gt;中T的上边界有2个，是List和Iterable\n\nD getGenericDeclaration()获取声明该泛型变量的原始类型，例如：class Test中的K为泛型变量，这个泛型变量时Test类定义的时候声明的，说明如果调用getGenericDeclaration方法返回的就是Test对应的Class对象。\n\n\n还有方法中也可以定义泛型类型的变量，如果在方法中定义，那么上面这个方法返回的就是定义泛型变量的方法了，返回的就是Method对象。\n\nString getName()获取在源码中定义时的名字，如：class Test就是K；class Test1中就是T。\n\nWildcardType接口表示的是通配符泛型，通配符使用问号表示，例如：? extends Number和? super Integer。\n接口中定义了2个方法。\n\nType[] getUpperBounds()返回泛型变量的上边界列表。\n\nType[] getLowerBounds()返回泛型变量的下边界列表。\n\n\nGenericArrayType接口表示的是数组类型，且数组中的元素是ParameterizedType或者TypeVariable。\n例如：List[]或者T[]。\n这个接口只有一个方法：\n\nType getGenericComponentType()这个方法返回数组的组成元素。\n\n类中定义泛型变量语法:\nclass 类名&lt;泛型变量1,泛型变量2,泛型变量3 extends 上边界1,泛型变量4 extends 上边界类型1 &amp; 上边界类型2 &amp; 上边界类型3&gt;\n\n泛型变量需要在类名后面的括号中定义\n\n每个类中可以定义多个泛型变量，多个泛型变量之间用逗号隔开\n\n泛型变量可以通过extends关键字指定上边界，上边界可以对泛型变量起到了限定的作用，上边界可以指定0到多个，多个之间需要用&amp;符号隔开，如果不指定上边界，默认上边界为Object类型\n\n\n示例代码:\npackage h.xd.type;import java.lang.reflect.Type;import java.lang.reflect.TypeVariable;public class DemoClass&lt;T1, T2 extends Integer, T3 extends DemoClass1 &amp; DemoClass2&gt; &#123;    public static void main(String[] args) &#123;        //获取类的类型参数列表        TypeVariable&lt;Class&lt;DemoClass&gt;&gt;[] typeParameters = DemoClass.class.getTypeParameters();        for (TypeVariable&lt;Class&lt;DemoClass&gt;&gt; typeParameter : typeParameters) &#123;            System.out.println(&quot;变量名称:&quot; + typeParameter.getName());            System.out.println(&quot;这个变量在哪声明的:&quot; + typeParameter.getGenericDeclaration());            Type[] bounds = typeParameter.getBounds();            System.out.println(&quot;这个变量上边界数量:&quot; + bounds.length);            System.out.println(&quot;这个变量上边界清单:&quot;);            for (Type bound : bounds) &#123;                System.out.println(bound.getTypeName());            &#125;            System.out.println(&quot;--------------------&quot;);        &#125;    &#125;&#125;interface DemoClass1&#123;&#125;interface DemoClass2&#123;&#125;\n\n运行结果:\n变量名称:T1这个变量在哪声明的:class h.xd.type.DemoClass这个变量上边界数量:1这个变量上边界清单:java.lang.Object--------------------变量名称:T2这个变量在哪声明的:class h.xd.type.DemoClass这个变量上边界数量:1这个变量上边界清单:java.lang.Integer--------------------变量名称:T3这个变量在哪声明的:class h.xd.type.DemoClass这个变量上边界数量:2这个变量上边界清单:h.xd.type.DemoClass1h.xd.type.DemoClass2--------------------\n\n\n方法中定义泛型变量语法:\n方法修饰符 &lt;泛型变量1,泛型变量2,泛型变量3 extends 上边界1,泛型变量4 extends 上边界类型1 &amp; 上边界类型2 &amp; 上边界类型3&gt; 方法名称(参数1类型 参数1名称,参数2类型 参数2名称)\n\n示例代码:\npackage h.xd.type;import java.lang.reflect.Method;import java.lang.reflect.Type;import java.lang.reflect.TypeVariable;interface DemoFunction1 &#123;&#125;interface DemoFunction2 &#123;&#125;/** * 泛型方法中的泛型变量 */public class DemoFunction &#123;    public &lt;T1, T2 extends Integer, T3 extends DemoFunction1 &amp; DemoFunction2&gt; T3 m1(T1 t1, T2 t2, T3 t3, String s) &#123;        return t3;    &#125;    public static void main(String[] args) &#123;        //获取DemoFunction中声明的所有方法        Method[] methods = DemoFunction.class.getDeclaredMethods();        Method m1 = null;        //找到m1方法        for (Method method : methods) &#123;            if (method.getName().equals(&quot;m1&quot;)) &#123;                m1 = method;                break;            &#125;        &#125;        //获取方法的泛型参数列表        System.out.println(&quot;m1方法参数类型列表信息:----------&quot;);        Type[] genericParameterTypes = m1.getGenericParameterTypes();        for (Type genericParameterType : genericParameterTypes) &#123;            //3个参数都是泛型变量类型的，对应java中的TypeVariable            if (genericParameterType instanceof TypeVariable) &#123;                TypeVariable pt = (TypeVariable) genericParameterType;                System.out.println(&quot;变量类型名称:&quot; + pt.getTypeName());                System.out.println(&quot;变量名称:&quot; + pt.getName());                System.out.println(&quot;这个变量在哪声明的:&quot; + pt.getGenericDeclaration());                Type[] bounds = pt.getBounds();                System.out.println(&quot;这个变量上边界数量:&quot; + bounds.length);                System.out.println(&quot;这个变量上边界清单:&quot;);                for (Type bound : bounds) &#123;                    System.out.println(bound.getTypeName());                &#125;            &#125; else if (genericParameterType instanceof Class) &#123;                Class pt = (Class) genericParameterType;                System.out.println(&quot;参数类型名称:&quot; + pt.getTypeName());                System.out.println(&quot;参数类名:&quot; + pt.getName());            &#125;            System.out.println(&quot;--------------------&quot;);        &#125;        //获取方法的返回值，也是一个泛型变量        System.out.println(&quot;m1方法返回值类型信息:----------&quot;);        Type genericReturnType = m1.getGenericReturnType();        if (genericReturnType instanceof TypeVariable) &#123;            TypeVariable pt = (TypeVariable) genericReturnType;            System.out.println(&quot;变量名称:&quot; + pt.getName());            System.out.println(&quot;这个变量在哪声明的:&quot; + pt.getGenericDeclaration());            Type[] bounds = pt.getBounds();            System.out.println(&quot;这个变量上边界数量:&quot; + bounds.length);            System.out.println(&quot;这个变量上边界清单:&quot;);            for (Type bound : bounds) &#123;                System.out.println(bound.getTypeName());            &#125;            System.out.println(&quot;--------------------&quot;);        &#125;        //获取方法中声明的泛型参数列表        System.out.println(&quot;m1方法中声明的泛型变量类型列表:----------&quot;);        TypeVariable&lt;Method&gt;[] typeParameters = m1.getTypeParameters();        for (TypeVariable&lt;Method&gt; pt : typeParameters) &#123;            System.out.println(&quot;变量类型名称:&quot; + pt.getTypeName());            System.out.println(&quot;变量名称:&quot; + pt.getName());            System.out.println(&quot;这个变量在哪声明的:&quot; + pt.getGenericDeclaration());            Type[] bounds = pt.getBounds();            System.out.println(&quot;这个变量上边界数量:&quot; + bounds.length);            System.out.println(&quot;这个变量上边界清单:&quot;);            for (Type bound : bounds) &#123;                System.out.println(bound.getTypeName());            &#125;            System.out.println(&quot;--------------------&quot;);        &#125;    &#125;&#125;\n\n运行结果:\nm1方法参数类型列表信息:----------变量类型名称:T1变量名称:T1这个变量在哪声明的:public h.xd.type.DemoFunction1 h.xd.type.DemoFunction.m1(java.lang.Object,java.lang.Integer,h.xd.type.DemoFunction1,java.lang.String)这个变量上边界数量:1这个变量上边界清单:java.lang.Object--------------------变量类型名称:T2变量名称:T2这个变量在哪声明的:public h.xd.type.DemoFunction1 h.xd.type.DemoFunction.m1(java.lang.Object,java.lang.Integer,h.xd.type.DemoFunction1,java.lang.String)这个变量上边界数量:1这个变量上边界清单:java.lang.Integer--------------------变量类型名称:T3变量名称:T3这个变量在哪声明的:public h.xd.type.DemoFunction1 h.xd.type.DemoFunction.m1(java.lang.Object,java.lang.Integer,h.xd.type.DemoFunction1,java.lang.String)这个变量上边界数量:2这个变量上边界清单:h.xd.type.DemoFunction1h.xd.type.DemoFunction2--------------------参数类型名称:java.lang.String参数类名:java.lang.String--------------------m1方法返回值类型信息:----------变量名称:T3这个变量在哪声明的:public h.xd.type.DemoFunction1 h.xd.type.DemoFunction.m1(java.lang.Object,java.lang.Integer,h.xd.type.DemoFunction1,java.lang.String)这个变量上边界数量:2这个变量上边界清单:h.xd.type.DemoFunction1h.xd.type.DemoFunction2--------------------m1方法中声明的泛型变量类型列表:----------变量类型名称:T1变量名称:T1这个变量在哪声明的:public h.xd.type.DemoFunction1 h.xd.type.DemoFunction.m1(java.lang.Object,java.lang.Integer,h.xd.type.DemoFunction1,java.lang.String)这个变量上边界数量:1这个变量上边界清单:java.lang.Object--------------------变量类型名称:T2变量名称:T2这个变量在哪声明的:public h.xd.type.DemoFunction1 h.xd.type.DemoFunction.m1(java.lang.Object,java.lang.Integer,h.xd.type.DemoFunction1,java.lang.String)这个变量上边界数量:1这个变量上边界清单:java.lang.Integer--------------------变量类型名称:T3变量名称:T3这个变量在哪声明的:public h.xd.type.DemoFunction1 h.xd.type.DemoFunction.m1(java.lang.Object,java.lang.Integer,h.xd.type.DemoFunction1,java.lang.String)这个变量上边界数量:2这个变量上边界清单:h.xd.type.DemoFunction1h.xd.type.DemoFunction2--------------------\n\n通配符类型通配符在java中 使用?表示，例如：? extends Number和? super Integer。\njava中通配符对应的类型是WildcardType接口，可以通过这个接口来获取通配符具体的各种信息。\n\n通配符上边界通配符具体的类型，可以任意指定，但是我们可以限定通配符的上边界，上边界指定了这个通配符能够表示的最大的范围的类型。\n\n\n比如：？extends Integer，那么?对应的具体类型只能是Integer本身或者其子类型。\n\n\n通配符下边界也可以给通配符指定下边界，下边界定义了通配符能够表示的最小的类型。\n\n\n比如：? super C1，那么?对应的具体类型只能是C1类型或者C1的父类型。\n\npackage h.xd.type;import java.lang.reflect.Method;import java.lang.reflect.ParameterizedType;import java.lang.reflect.Type;import java.lang.reflect.WildcardType;import java.util.List;import java.util.Map;public class DemoWild &#123;    public static class C1 &#123;    &#125;    public static class C2 extends C1 &#123;    &#125;    public static List&lt;?&gt; m1(Map&lt;? super C2, ? extends C1&gt; map) &#123;        return null;    &#125;    public static void main(String[] args) throws NoSuchMethodException &#123;        Method m1 = DemoWild.class.getMethod(&quot;m1&quot;, Map.class);        //获取m1方法参数泛型详细参数信息        System.out.println(&quot;获取m1方法参数泛型详细参数信息&quot;);        Type[] genericParameterTypes = m1.getGenericParameterTypes();        for (Type genericParameterType : genericParameterTypes) &#123;            // m1的参数为Map&lt;? super C2, ? extends C1&gt;，这个是泛型类型的，所以是ParameterizedType接口类型            if (genericParameterType instanceof ParameterizedType) &#123;                ParameterizedType parameterizedType = (ParameterizedType) genericParameterType;                //下面获取Map后面两个尖括号中的泛型参数列表，对应? super C2, ? extends C1这部分的内容，这部分在java中对应WildcardType接口类型                Type[] actualTypeArguments = parameterizedType.getActualTypeArguments();                for (Type actualTypeArgument : actualTypeArguments) &#123;                    if (actualTypeArgument instanceof WildcardType) &#123;                        WildcardType wildcardType = (WildcardType) actualTypeArgument;                        //获取通配符的名称，输出是?                        System.out.println(&quot;通配符类型名称:&quot; + wildcardType.getTypeName());                        //获取通配符的上边界                        Type[] upperBounds = wildcardType.getUpperBounds();                        for (Type upperBound : upperBounds) &#123;                            System.out.println(&quot;通配符上边界类型：&quot; + upperBound.getTypeName());                        &#125;                        //获取通配符的下边界                        Type[] lowerBounds = wildcardType.getLowerBounds();                        for (Type lowerBound : lowerBounds) &#123;                            System.out.println(&quot;通配符下边界类型:&quot; + lowerBound.getTypeName());                        &#125;                        System.out.println(&quot;------------&quot;);                    &#125;                &#125;            &#125;        &#125;        //获取返回值通配符详细信息        System.out.println(&quot;获取m1方法返回值泛型类型详细信息&quot;);        Type genericReturnType = m1.getGenericReturnType();        // m1的返回值是List&lt;?&gt;，这个是个泛型类型，对应ParameterizedType接口，泛型中的具体类型是个通配符类型，通配符对应WildcardType接口类型        if (genericReturnType instanceof ParameterizedType) &#123;             ParameterizedType parameterizedType = (ParameterizedType) genericReturnType;            //下面获取List面两个尖括号中的泛型参数列表，对应?这部分的内容，这个是个通配符类型，这部分在java中对应WildcardType接口            Type[] actualTypeArguments = parameterizedType.getActualTypeArguments();            for (Type actualTypeArgument : actualTypeArguments) &#123;                if (actualTypeArgument instanceof WildcardType) &#123;                    WildcardType wildcardType = (WildcardType) actualTypeArgument;                    //获取通配符的名称，输出是?                    System.out.println(&quot;通配符类型名称:&quot; + wildcardType.getTypeName());                    //获取通配符的上边界                    Type[] upperBounds = wildcardType.getUpperBounds();                    for (Type upperBound : upperBounds) &#123;                        System.out.println(&quot;通配符上边界类型：&quot; + upperBound.getTypeName());                    &#125;                    //获取通配符的下边界                    Type[] lowerBounds = wildcardType.getLowerBounds();                    for (Type lowerBound : lowerBounds) &#123;                        System.out.println(&quot;通配符下边界类型:&quot; + lowerBound.getTypeName());                    &#125;                    System.out.println(&quot;------------&quot;);                &#125;            &#125;        &#125;    &#125;&#125;\n\n运行结果:\n获取m1方法参数泛型详细参数信息通配符类型名称:? super h.xd.type.DemoWild$C2通配符上边界类型：java.lang.Object通配符下边界类型:h.xd.type.DemoWild$C2------------通配符类型名称:? extends h.xd.type.DemoWild$C1通配符上边界类型：h.xd.type.DemoWild$C1------------获取m1方法返回值泛型类型详细信息通配符类型名称:?通配符上边界类型：java.lang.Object------------\n\n泛型数组数组中的元素为泛型，那么这个数组就是泛型类型的数组，泛型数组在java中使用GenericArrayType接口来表示，可以通过这个接口提供的方法获取泛型数组更详细的信息。\n如：List list []; List list [][];\n泛型数组类型的可以作为方法的参数、方法的返回值、泛型类的具体类型、字段的类型等等。\n示例代码:\npackage h.xd.type;import java.lang.reflect.Field;import java.lang.reflect.GenericArrayType;import java.lang.reflect.ParameterizedType;import java.lang.reflect.Type;import java.util.List;public class DemoList &#123;    List&lt;String&gt; list[];    public static void main(String[] args) throws NoSuchFieldException &#123;        Field list = DemoList.class.getDeclaredField(&quot;list&quot;);        //获取字段的泛型类型        Type genericType = list.getGenericType();        //看看字段的具体泛型类型        System.out.println(genericType.getClass());        if (genericType instanceof GenericArrayType) &#123;            GenericArrayType genericArrayType = (GenericArrayType) genericType;            //获取数组的具体类型，具体的类型就是List&lt;String&gt;，这个是个泛型类型，对应java中的ParameterizedType接口            Type genericComponentType = genericArrayType.getGenericComponentType();            System.out.println(genericComponentType.getClass());            if (genericComponentType instanceof ParameterizedType) &#123;                ParameterizedType parameterizedType = (ParameterizedType) genericComponentType;                System.out.println(parameterizedType.getRawType());                //调用getActualTypeArguments()获取List&lt;String&gt;中尖括号中的参数列表                Type[] actualTypeArguments = parameterizedType.getActualTypeArguments();                for (Type actualTypeArgument : actualTypeArguments) &#123;                    System.out.println(actualTypeArgument.getTypeName());                &#125;                System.out.println(parameterizedType.getOwnerType());            &#125;        &#125;    &#125;&#125;\n\n运行结果:\nclass sun.reflect.generics.reflectiveObjects.GenericArrayTypeImplclass sun.reflect.generics.reflectiveObjects.ParameterizedTypeImplinterface java.util.Listjava.lang.Stringnull\n\n综合案例返回结果封装:\n/** * 通用返回结果封装类 * Created by macro on 2019/4/19. */public class CommonResult&lt;T&gt; &#123;    /**     * 状态码     */    private long code;    /**     * 提示信息     */    private String message;    /**     * 数据封装     */    private T data;    protected CommonResult() &#123;    &#125;    protected CommonResult(long code, String message, T data) &#123;        this.code = code;        this.message = message;        this.data = data;    &#125;    /**     * 成功返回结果     *     * @param data 获取的数据     */    public static &lt;T&gt; CommonResult&lt;T&gt; success(T data) &#123;        return new CommonResult&lt;T&gt;(ResultCode.SUCCESS.getCode(), ResultCode.SUCCESS.getMessage(), data);    &#125;    /**     * 成功返回结果     *     * @param data 获取的数据     * @param  message 提示信息     */    public static &lt;T&gt; CommonResult&lt;T&gt; success(T data, String message) &#123;        return new CommonResult&lt;T&gt;(ResultCode.SUCCESS.getCode(), message, data);    &#125;    /**     * 失败返回结果     * @param errorCode 错误码     */    public static &lt;T&gt; CommonResult&lt;T&gt; failed(IErrorCode errorCode) &#123;        return new CommonResult&lt;T&gt;(errorCode.getCode(), errorCode.getMessage(), null);    &#125;    /**     * 失败返回结果     * @param errorCode 错误码     * @param message 错误信息     */    public static &lt;T&gt; CommonResult&lt;T&gt; failed(IErrorCode errorCode,String message) &#123;        return new CommonResult&lt;T&gt;(errorCode.getCode(), message, null);    &#125;    /**     * 失败返回结果     * @param message 提示信息     */    public static &lt;T&gt; CommonResult&lt;T&gt; failed(String message) &#123;        return new CommonResult&lt;T&gt;(ResultCode.FAILED.getCode(), message, null);    &#125;    /**     * 失败返回结果     */    public static &lt;T&gt; CommonResult&lt;T&gt; failed() &#123;        return failed(ResultCode.FAILED);    &#125;    /**     * 参数验证失败返回结果     */    public static &lt;T&gt; CommonResult&lt;T&gt; validateFailed() &#123;        return failed(ResultCode.VALIDATE_FAILED);    &#125;    /**     * 参数验证失败返回结果     * @param message 提示信息     */    public static &lt;T&gt; CommonResult&lt;T&gt; validateFailed(String message) &#123;        return new CommonResult&lt;T&gt;(ResultCode.VALIDATE_FAILED.getCode(), message, null);    &#125;    /**     * 未登录返回结果     */    public static &lt;T&gt; CommonResult&lt;T&gt; unauthorized(T data) &#123;        return new CommonResult&lt;T&gt;(ResultCode.UNAUTHORIZED.getCode(), ResultCode.UNAUTHORIZED.getMessage(), data);    &#125;    /**     * 未授权返回结果     */    public static &lt;T&gt; CommonResult&lt;T&gt; forbidden(T data) &#123;        return new CommonResult&lt;T&gt;(ResultCode.FORBIDDEN.getCode(), ResultCode.FORBIDDEN.getMessage(), data);    &#125;    public long getCode() &#123;        return code;    &#125;    public void setCode(long code) &#123;        this.code = code;    &#125;    public String getMessage() &#123;        return message;    &#125;    public void setMessage(String message) &#123;        this.message = message;    &#125;    public T getData() &#123;        return data;    &#125;    public void setData(T data) &#123;        this.data = data;    &#125;&#125;","categories":["总结笔记"],"tags":["java","泛型"]},{"title":"Java单例总结","url":"/2022_04_12_java_singleton/","content":"单例模式（Singleton Pattern）是指确保一个类在任何情况下都绝对只有一个实例，并提供一个全局的访问点。\n隐藏其所有的构造方法。\n属于创建型模式。\n单例模式的适用场景确保任何情况下都绝对只有一个实例。\nServletContext、ServletConfig、ApplicationContext、DBPool\n饿汉式package h.xd.java;import java.lang.reflect.Constructor;import java.util.concurrent.TimeUnit;/** * 优点：执行效率高，性能高，没有任何的锁 * 缺点：可能会造成内存浪费、反射单例失效、序列化失效 */public class HungrySingleton &#123;    //static 类加载的时候就创建，final：不可被覆盖。    private static final HungrySingleton hungrysingleton = new HungrySingleton();    private HungrySingleton()&#123;&#125;;    public static HungrySingleton getInstance()&#123;        return hungrysingleton;    &#125;    public static void main(String[] args) throws InterruptedException &#123;        for (int i = 0; i &lt; 20; i++) &#123;            //多线程实例化            new Thread(()-&gt;&#123;                System.out.println(HungrySingleton.getInstance());            &#125;).start();            //多线程反射实例化            new Thread(()-&gt;&#123;                try &#123;                    Constructor&lt;HungrySingleton&gt; declaredConstructor = HungrySingleton.class.getDeclaredConstructor();                    HungrySingleton hungrySingleton = declaredConstructor.newInstance();                    System.out.println(hungrySingleton);                &#125; catch (Exception e) &#123;                    e.printStackTrace();                &#125;            &#125;).start();        &#125;        TimeUnit.SECONDS.sleep(2);    &#125;&#125;\n\n懒汉式package h.xd.java;import java.lang.reflect.Constructor;import java.util.concurrent.TimeUnit;/** * 优点：节约了内存 * 缺点：线程不安全,synchronized性能低， 存在锁等待，反射锁失效、序列化失效 */public class LazySimpleSingleton &#123;    private static LazySimpleSingleton instance;    private LazySimpleSingleton()&#123;&#125;    public static synchronized LazySimpleSingleton getInstance()&#123;        if(instance == null)&#123;            instance = new LazySimpleSingleton();        &#125;        return instance;    &#125;    public static void main(String[] args) throws InterruptedException &#123;        for (int i = 0; i &lt; 20; i++) &#123;            //多线程实例化            new Thread(()-&gt;&#123;                System.out.println(LazySimpleSingleton.getInstance());            &#125;).start();            //多线程反射实例化            new Thread(()-&gt;&#123;                try &#123;                    Constructor&lt;LazySimpleSingleton&gt; declaredConstructor = LazySimpleSingleton.class.getDeclaredConstructor();                    LazySimpleSingleton lazySimpleSingleton = declaredConstructor.newInstance();                    System.out.println(lazySimpleSingleton);                &#125; catch (Exception e) &#123;                    e.printStackTrace();                &#125;            &#125;).start();        &#125;        TimeUnit.SECONDS.sleep(2);    &#125;&#125;\n\n双重校验懒汉式package h.xd.java;import java.lang.reflect.Constructor;import java.util.concurrent.TimeUnit;/** * 双重检查：1、检查是否要阻塞；2、检查是否创建实例。 * 优点：安全且在多线程情况下能保持高性能, * 第一个if判断避免了其他无用线程竞争锁来造成性能浪费， * 第二个if判断能拦截除第一个获得对象锁线程以外的线程。 * 缺点：但是可读性不高，代码不够优雅，反射失效 */public class LazyDoubleCheckSingleton &#123;    //volatile:解决了指令重排序问题（指令执行顺序不一定按代码来执行）。    private volatile static LazyDoubleCheckSingleton instance;    //私有化防实例化    private LazyDoubleCheckSingleton()&#123;&#125;    public static LazyDoubleCheckSingleton getInstance()&#123;        //检查是否需要阻塞        if(instance == null)&#123;            synchronized (LazyDoubleCheckSingleton.class)&#123;                //检查是否需要重新创建实例                if(instance == null)&#123;                    instance = new LazyDoubleCheckSingleton();                    //指令重排序问题                &#125;            &#125;        &#125;        return instance;    &#125;    public static void main(String[] args) throws InterruptedException &#123;        for (int i = 0; i &lt; 20; i++) &#123;            //多线程实例化            new Thread(()-&gt;&#123;                System.out.println(LazyDoubleCheckSingleton.getInstance());            &#125;).start();            //多线程反射实例化            new Thread(()-&gt;&#123;                try &#123;                    Constructor&lt;LazyDoubleCheckSingleton&gt; declaredConstructor = LazyDoubleCheckSingleton.class.getDeclaredConstructor();                    LazyDoubleCheckSingleton lazyDoubleCheckSingleton = declaredConstructor.newInstance();                    System.out.println(lazyDoubleCheckSingleton);                &#125; catch (Exception e) &#123;                    e.printStackTrace();                &#125;            &#125;).start();        &#125;        TimeUnit.SECONDS.sleep(2);    &#125;&#125;\n\n双重校验懒汉防反射package h.xd.java;import java.lang.reflect.Constructor;import java.util.concurrent.TimeUnit;/** * 双重检查：1、检查是否要阻塞；2、检查是否创建实例。 * 优点：安全且在多线程情况下能保持高性能, * 第一个if判断避免了其他无用线程竞争锁来造成性能浪费， * 第二个if判断能拦截除第一个获得对象锁线程以外的线程。 * 一定程度上解决了反射问题，反射会报错 * 缺点：但是可读性不高，代码不够优雅 */public class LazyDoubleCheckSingleton2 &#123;    //volatile:解决了指令重排序问题（指令执行顺序不一定按代码来执行）。    private volatile static LazyDoubleCheckSingleton2 instance;    //私有化防实例化    private LazyDoubleCheckSingleton2()&#123;        if(LazyDoubleCheckSingleton2.instance != null )&#123;            throw new RuntimeException(&quot;不允许非法的访问&quot;);        &#125;    &#125;    public static LazyDoubleCheckSingleton2 getInstance()&#123;        //检查是否需要阻塞        if(instance == null)&#123;            synchronized (LazyDoubleCheckSingleton2.class)&#123;                //检查是否需要重新创建实例                if(instance == null)&#123;                    instance = new LazyDoubleCheckSingleton2();                    //指令重排序问题                &#125;            &#125;        &#125;        return instance;    &#125;    public static void main(String[] args) throws InterruptedException &#123;        for (int i = 0; i &lt; 20; i++) &#123;            //多线程实例化            new Thread(()-&gt;&#123;                System.out.println(LazyDoubleCheckSingleton2.getInstance());            &#125;).start();            //多线程反射实例化            new Thread(()-&gt;&#123;                try &#123;                    Constructor&lt;LazyDoubleCheckSingleton2&gt; declaredConstructor = LazyDoubleCheckSingleton2.class.getDeclaredConstructor();                    LazyDoubleCheckSingleton2 lazyDoubleCheckSingleton2 = declaredConstructor.newInstance();                    System.out.println(lazyDoubleCheckSingleton2);                &#125; catch (Exception e) &#123;                    e.printStackTrace();                &#125;            &#125;).start();        &#125;        TimeUnit.SECONDS.sleep(2);    &#125;&#125;\n\n\n枚举单例package h.xd.java;import java.lang.reflect.Constructor;import java.util.concurrent.TimeUnit;/** * INSTANCE，是单例模式的唯一实例。 * increment,getCount方法永远是一个实例的方法，（方法内部线程不安全，方法是一个） * * 优点： * 使用枚举实现单例模式是线程安全的。 * 在多线程环境中，多个线程可以同时访问单例对象，但是由于枚举的特殊性质，只有一个实例对象被创建，所以不会出现线程安全问题。 * * 使用枚举实现单例模式可以避免序列化和反序列化的问题。 * 在 Java 中，当一个类被序列化并在另一个 JVM 中反序列化时，它会创建一个新的对象。 * 如果使用枚举实现单例模式，则不需要担心这个问题，因为枚举实例是在加载枚举类型时由 JVM 创建的，并且它们是全局可访问的，因此不会出现创建多个实例的情况。 * * 使用枚举实现单例模式可以防止反射攻击。 * 在 Java 中，反射机制可以通过 Class 类来获取对象的构造函数并创建新的对象。 * 如果使用枚举实现单例模式，则可以避免这种攻击，因为枚举类型的构造函数是私有的，不能通过反射来调用。 * * 使用枚举实现单例模式可以使代码更加简洁明了。 * 枚举类型本身就是单例的，因此不需要编写任何特殊的代码来实现单例模式。 * 并且具有有意义的名称和明确定义的值，这可以减少代码量和提高代码的可读性。 * * 缺点：非延时加载 */public enum EnumSingleton &#123;    INSTANCE;    private int count = 0;    public void increment()&#123;        count++;    &#125;    public int getCount()&#123;        return count;    &#125;    public static EnumSingleton getInstance()&#123;        return INSTANCE;    &#125;    public static void main(String[] args) throws InterruptedException &#123;        EnumSingleton.getInstance().increment();        EnumSingleton.getInstance().increment();        for (int i = 0; i &lt; 20; i++) &#123;            //多线程实例化            new Thread(()-&gt;&#123;                System.out.println(EnumSingleton.getInstance().getCount());            &#125;).start();            //多线程反射实例化            new Thread(()-&gt;&#123;                try &#123;                    Constructor&lt;EnumSingleton&gt; declaredConstructor = EnumSingleton.class.getDeclaredConstructor();                    EnumSingleton enumSingleton = declaredConstructor.newInstance();                    System.out.println(enumSingleton.getCount());                &#125; catch (Exception e) &#123;                    e.printStackTrace();                &#125;            &#125;).start();        &#125;        TimeUnit.SECONDS.sleep(2);    &#125;&#125;\n\n内部类单例package h.xd.java;import java.lang.reflect.Constructor;import java.util.concurrent.TimeUnit;/** * 利用了classloader机制来保证初始化 instance 时只有一个线程，线程安全； * 只有通过显式调用 getInstance 方法时，才会显式装载静态内部类，从而实例化instance，延迟加载。 * * 缺点： 反射失效 * */public class InnerSingleton &#123;    private InnerSingleton()&#123;&#125;    private static class SingletonHolder&#123;        private static final InnerSingleton INSTANCE = new InnerSingleton();    &#125;    public static InnerSingleton getInstance()&#123;        return SingletonHolder.INSTANCE;    &#125;    public static void main(String[] args) throws InterruptedException &#123;        for (int i = 0; i &lt; 20; i++) &#123;            //多线程实例化            new Thread(()-&gt;&#123;                System.out.println(InnerSingleton.getInstance());            &#125;).start();            //多线程反射实例化            new Thread(()-&gt;&#123;                try &#123;                    Constructor&lt;InnerSingleton&gt; declaredConstructor = InnerSingleton.class.getDeclaredConstructor();                    InnerSingleton innerSingleton = declaredConstructor.newInstance();                    System.out.println(innerSingleton);                &#125; catch (Exception e) &#123;                    e.printStackTrace();                &#125;            &#125;).start();        &#125;        TimeUnit.SECONDS.sleep(2);    &#125;&#125;\n\n容器思想单例package h.xd.java;import java.lang.reflect.Constructor;import java.util.Map;import java.util.concurrent.ConcurrentHashMap;import java.util.concurrent.TimeUnit;/** * 容器模型下的单例，利用多线程安全的hashMap  + sync 机制，实现 * 缺点： 反射失效，需要锁 */public class ContainerSingleton &#123;    private ContainerSingleton()&#123;&#125;    private static Map&lt;String,Object&gt; ioc = new ConcurrentHashMap&lt;String, Object&gt;();    public static synchronized Object getInstance(String className)&#123;        Object instance = null;        if(!ioc.containsKey(className))&#123;            try&#123;                instance = Class.forName(className).newInstance();                ioc.put(className,instance);            &#125;catch (Exception e)&#123;                e.printStackTrace();            &#125;            return  instance;        &#125;else &#123;            return ioc.get(className);        &#125;    &#125;    public static void main(String[] args) throws InterruptedException &#123;        for (int i = 0; i &lt; 20; i++) &#123;            //多线程实例化            new Thread(()-&gt;&#123;                System.out.println(ContainerSingleton.getInstance(&quot;h.xd.java.ContainerSingleton&quot;));            &#125;).start();//            多线程反射实例化            new Thread(()-&gt;&#123;                try &#123;                    Constructor&lt;ContainerSingleton&gt; declaredConstructor = ContainerSingleton.class.getDeclaredConstructor();                    ContainerSingleton enumSingleton = declaredConstructor.newInstance();                    System.out.println(enumSingleton);                &#125; catch (Exception e) &#123;                    e.printStackTrace();                &#125;            &#125;).start();        &#125;        TimeUnit.SECONDS.sleep(2);    &#125;&#125;\n\n防序列化单例package h.xd.java;import java.io.*;import java.util.concurrent.TimeUnit;public class SerializableSingleton implements Serializable &#123;    //序列化    //把内存中的对象的状态转化为字节码的形式    //把字节码通过IO输出流，写到磁盘！    //永久的保存下来    public final static  SerializableSingleton INSTANCE = new SerializableSingleton();    private SerializableSingleton()&#123;&#125;    public static SerializableSingleton getInstance()&#123;        return INSTANCE;    &#125;    /**     * 防止序列化，对象重复创建     * @return     */    public Object readResolve()&#123;        return INSTANCE;    &#125;    public static void main(String[] args) throws InterruptedException &#123;        SerializableSingleton instance = SerializableSingleton.getInstance();        FileOutputStream fos = null;        try &#123;            fos = new FileOutputStream(&quot;SerializableSingleton.obj&quot;);            ObjectOutputStream oos = new ObjectOutputStream(fos);            oos.writeObject(instance);            oos.flush();            oos.close();            System.out.println(instance);            for (int i = 0; i &lt; 20; i++) &#123;                //多线程实例化                new Thread(() -&gt; &#123;                    FileInputStream fis = null;                    SerializableSingleton instance1 = null;                    try &#123;                        fis = new FileInputStream(&quot;SerializableSingleton.obj&quot;);                        ObjectInputStream ois = new ObjectInputStream(fis);                        instance1 =(SerializableSingleton) ois.readObject();                        ois.close();                        System.out.println(instance1);                    &#125; catch (Exception e) &#123;                        e.printStackTrace();                    &#125;                &#125;).start();            &#125;            TimeUnit.SECONDS.sleep(2);        &#125;catch (Exception e)&#123;            e.printStackTrace();        &#125;    &#125;&#125;\n\n线程内部只有一个实例的单例package h.xd.java;import java.util.concurrent.TimeUnit;/** * 线程内部，单例， * 缺点：多线程失效、反射失效、序列化失效 */public class ThreadLocalSingleton &#123;    private static final ThreadLocal&lt;ThreadLocalSingleton&gt; INSTANCE =            ThreadLocal.withInitial(ThreadLocalSingleton::new);    private ThreadLocalSingleton()&#123;&#125;    public static ThreadLocalSingleton getInstance()&#123;        return INSTANCE.get();    &#125;    public static void main(String[] args) throws InterruptedException &#123;        for (int i = 0; i &lt; 1; i++) &#123;            //多线程实例化            new Thread(() -&gt; &#123;                System.out.println(ThreadLocalSingleton.getInstance());                System.out.println(ThreadLocalSingleton.getInstance());                System.out.println(ThreadLocalSingleton.getInstance());                System.out.println(ThreadLocalSingleton.getInstance());            &#125;).start();        &#125;        TimeUnit.SECONDS.sleep(2);    &#125;&#125;\n\n单例模式要点\n私有化构造器,防直接实例化\n保证线程安全\n按需选择延迟加载和非延时加载\n防止序列化和反序列化破坏单例\n防御反射攻击单例\n\n","categories":["总结笔记"],"tags":["java","单例模式"]},{"title":"Java 8 新特性一","url":"/2023_12_31_java/","content":"学习必须往深处挖，挖的越深，基础越扎实！\nJava 现在发布的版本很快，每年两个，但是真正会被大规模使用的是 3 年一个的 LTS 版本。\n每 3 年发布一个 LTS（Long-Term Support），长期维护版本。意味着只有Java 8 ，Java 11， Java 17，Java 21 才可能被大规模使用。\n每年发布两个正式版本，分别是 3 月份和 9 月份。\n在 Java 版本中，一个特性的发布都会经历孵化阶段、预览阶段和正式版本。其中孵化和预览可能会跨越多个 Java 版本。所以在介绍 Java 新特性时采用如下这种策略：\n\n每个版本的新特性，都会做一个简单的概述。\n单独出文介绍跟编码相关的新特性，一些如 JVM、性能优化的新特性不单独出文介绍。\n孵化阶段的新特性不出文介绍。\n首次引入为预览特性、新特性增强、首次引入的正式特性，单独出文做详细介绍。\n影响比较大的新特性如果在现阶段没有转正的新特性不单独出文介绍，单独出文的重大特性一般都在 Java 21 版本之前已转为正式特性，例如：\n虚拟线程，Java 19 引入的，在 Java 21 转正，所以在 Java 19 单独出文做详细介绍\n作用域值，Java 20 引入的，但是在 Java 21 还处于预览阶段，所以不做介绍，等将来转正后会详细介绍\n\n\n\nJava 8 新特性一JEP 101：类型推断优化简单理解泛型泛型是Java SE 1.5的新特性，泛型的本质是参数化类型，也就是说所操作的数据类型被指定为一个参数。通俗点将就是“类型的变量”。这种类型变量可以用在类、接口和方法的创建中。\n理解Java泛型最简单的方法是把它看成一种便捷语法，能节省你某些Java类型转换(casting)上的操作:\nList box &#x3D; new ArrayList();box.add(new Apple());Apple apple &#x3D;box.get(0);\n上面的代码自身已表达的很清楚: box是一个装有Apple对象的List。get方法返回一个Apple对象实例，这个过程不需要进行类型转换。没有泛型，上面的代码需要写成这样:\nApple apple &#x3D; (Apple)box.get(0);\n泛型的尴尬泛型的最大优点是提供了程序的类型安全同时可以向后兼容，但也有尴尬的地方，就是每次定义时都要写明泛型的类型，这样显示指定不仅感觉有些冗长，最主要是很多程序员不熟悉泛型，因此很多时候不能够给出正确的类型参数，现在通过编译器自动推断泛型的参数类型，能够减少这样的情况，并提高代码可读性。\njava7的泛型类型推断改进在以前的版本中使用泛型类型，需要在声明并赋值的时候，两侧都加上泛型类型。例如:\nMap&lt;String, String&gt; myMap = new HashMap&lt;String, String&gt;();\n\n你可能觉得:老子在声明变量的的时候已经指明了参数类型，为毛还要在初始化对象时再指定? 幸好，在Java SE 7中，这种方式得以改进，现在你可以使用如下语句进行声明并赋值:\nMap&lt;String, String&gt; myMap = new HashMap&lt;&gt;(); //注意后面的&quot;&lt;&gt;&quot;\n\n在这条语句中，编译器会根据变量声明时的泛型类型自动推断出实例化HashMap时的泛型类型。再次提醒一定要注意new HashMap后面的“&lt;&gt;”，只有加上这个“&lt;&gt;”才表示是自动类型推断，否则就是非泛型类型的HashMap，并且在使用编译器编译源代码时会给出一个警告提示。\n但是: Java SE 7在创建泛型实例时的类型推断是有限制的: 只有构造器的参数化类型在上下文中被显著的声明了，才可以使用类型推断，否则不行。例如: 下面的例子在java 7无法正确编译(但现在在java8里面可以编译，因为根据方法参数来自动推断泛型的类型):\nList&lt;String&gt; list = new ArrayList&lt;&gt;();list.add(&quot;A&quot;);// 由于addAll期望获得Collection&lt;? extends String&gt;类型的参数，因此下面的语句无法通过list.addAll(new ArrayList&lt;&gt;());\n\nJava8的泛型类型推断改进java8里面泛型的目标类型推断主要2个:\n1.支持通过方法上下文推断泛型目标类型\n2.支持在方法调用链路当中，泛型类型推断传递到最后一个方法\n让我们看看官网的例子\nclass List&lt;E&gt; &#123;   static &lt;Z&gt; List&lt;Z&gt; nil() &#123; ... &#125;;   static &lt;Z&gt; List&lt;Z&gt; cons(Z head, List&lt;Z&gt; tail) &#123; ... &#125;;   E head() &#123; ... &#125;&#125;\n根据JEP101的特性，我们在调用上面方法的时候可以这样写\n//通过方法赋值的目标参数来自动推断泛型的类型List&lt;String&gt; l = List.nil();//而不是显示的指定类型//List&lt;String&gt; l = List.&lt;String&gt;nil();//通过前面方法参数类型推断泛型的类型List.cons(42, List.nil());//而不是显示的指定类型//List.cons(42, List.&lt;Integer&gt;nil());\n\n以上是JEP101的特性内容了，Java作为静态语言的代表者，可以说类型系统相当丰富。导致类型间互相转换的问题困扰着每个java程序员，通过编译器自动推断类型的东西可以稍微缓解一下类型转换太复杂的问题。 虽然说是小进步，但对于我们天天写代码的程序员，肯定能带来巨大的作用，至少心情更愉悦了\nJEP 104：类型注解从java5开始加入这一特性，发展到现在已然是遍地开花，在很多框架中得到了广泛的使用，用来简化程序中的配置。那充满争议的类型注解究竟是什么? 复杂还是便捷?\n\n在java 8之前，注解只能是在声明的地方所使用，比如类，方法，属性；\n\njava 8里面，注解可以应用在任何地方，比如:\n\n\n创建类实例\nnew @Interned MyObject();\n\n类型映射\nmyString = (@NonNull String) str;\n\nimplements 语句中\nclass UnmodifiableList&lt;T&gt; implements @Readonly List&lt;@Readonly T&gt; &#123; … &#125;\n\nthrow exception声明\nvoid monitorTemperature() throws @Critical TemperatureException &#123; … &#125;\n\n需要注意的是，类型注解只是语法而不是语义，并不会影响java的编译时间，加载时间，以及运行时间，也就是说，编译成class文件的时候并不包含类型注解。\n类型注解的作用先看看下面代码\nCollections.emptyList().add(&quot;One&quot;);int i=Integer.parseInt(&quot;hello&quot;);System.console().readLine();\n\n上面的代码编译是通过的，但运行是会分别报UnsupportedOperationException； NumberFormatException；NullPointerException异常，这些都是runtime error；\n类型注解被用来支持在Java的程序中做强类型检查。配合插件式的check framework，可以在编译的时候检测出runtime error，以提高代码质量。这就是类型注解的作用了。\ncheck framework是第三方工具，配合Java的类型注解效果就是1+1&gt;2。它可以嵌入到javac编译器里面，可以配合ant和maven使用, 地址是http://types.cs.washington.edu/checker-framework/。 check framework可以找到类型注解出现的地方并检查，举个简单的例子:\nimport checkers.nullness.quals.*;public class GetStarted &#123;    void sample() &#123;        @NonNull Object ref = new Object();    &#125;&#125;\n\n使用javac编译上面的类\njavac -processor checkers.nullness.NullnessChecker GetStarted.java\n\n编译是通过，但如果修改成\n@NonNull Object ref = null;\n\nGetStarted.java:5: incompatible types.found   : @Nullable &lt;nulltype&gt;required: @NonNull Object        @NonNull Object ref = null;                              ^1 error\n\n类型注解向下兼容的解决方案如果你不想使用类型注解检测出来错误，则不需要processor，直接javac GetStarted.java是可以编译通过的，这是在java 8 with Type Annotation Support版本里面可以，但java 5,6,7版本都不行，因为javac编译器不知道@NonNull是什么东西，但check framework 有个向下兼容的解决方案，就是将类型注解nonnull用&#x2F;**&#x2F;注释起来，比如上面例子修改为\nimport checkers.nullness.quals.*;public class GetStarted &#123;    void sample() &#123;        /*@NonNull*/ Object ref = null;    &#125;&#125;\n\n这样javac编译器就会忽略掉注释块，但用check framework里面的javac编译器同样能够检测出nonnull错误。 通过类型注解+check framework我们可以看到，现在runtime error可以在编译时候就能找到。\n关于JSR 308JSR 308想要解决在Java 1.5注解中出现的两个问题:\n\n在句法上对注解的限制: 只能把注解写在声明的地方\n类型系统在语义上的限制: 类型系统还做不到预防所有的bug\n\nJSR 308 通过如下方法解决上述两个问题:\n\n对Java语言的句法进行扩充，允许注解出现在更多的位置上。包括: 方法接收器(method receivers，译注: 例public int size() @Readonly { … })，泛型参数，数组，类型转换，类型测试，对象创建，类型参数绑定，类继承和throws子句。其实就是类型注解，现在是java 8的一个特性\n通过引入可插拔的类型系统(pluggable type systems)能够创建功能更强大的注解处理器。类型检查器对带有类型限定注解的源码进行分析，一旦发现不匹配等错误之处就会产生警告信息。其实就是check framework\n\n对JSR308，有人反对，觉得更复杂更静态了，比如\n@NotEmpty List&lt;@NonNull String&gt; strings = new ArrayList&lt;@NonNull String&gt;()&gt;\n\n换成动态语言为\nvar strings = [&quot;one&quot;, &quot;two&quot;];\n\n有人赞成，说到底，代码才是“最根本”的文档。代码中包含的注解清楚表明了代码编写者的意图。当没有及时更新或者有遗漏的时候，恰恰是注解中包含的意图信息，最容易在其他文档中被丢失。而且将运行时的错误转到编译阶段，不但可以加速开发进程，还可以节省测试时检查bug的时间。\n总结并不是人人都喜欢这个特性，特别是动态语言比较流行的今天，所幸，java 8并不强求大家使用这个特性，反对的人可以不使用这一特性，而对代码质量有些要求比较高的人或公司可以采用JSR 308，毕竟代码才是“最基本”的文档，这句话我是赞同的。虽然代码会增多，但可以使你的代码更具有表达意义。对这个特性有何看法，大家各抒己见。。。。\nJEP 107：Stream APIStream API ( java.util.stream) 把真正的函数式编程风格引入到Java中。这是目前为止对Java类库最好的补充，因为Stream API可以极大提供Java程 序员的生产力，让程序员写出高效率、干净、简洁的代码。\nStream 是 Java8 中处理集合的关键抽象概念，它可以指定你希望对集合进行的操作，可以执行非常复杂的查找、过滤和映射数据等操作。使用 Stream API 对集合数据进行操作，就类似于使用 SQL 执行的数据库查询。 也可以使用 Stream API来并行执行操作。简言之，Stream API 提供了一种高效且易于使用的处理数据的方式。\n为什么要使用Stream API？实际开发中，项目中多数数据源都来自于Mysql，Oracle等。但现在数据源可以更多了，有MongDB，Radis等，而这些NoSQL的数据就需要Java层面去处理。\nStream 和 Collection 集合的区别：Collection 是一种静态的内存数据结构，而 Stream 是有关计算的。前者是主要面向内存，存储在内存中，后者主要是面向 CPU，通过 CPU 实现计算。\n什么是 Stream？是数据渠道，用于操作数据源（集合、数组等）所生成的元素序列。 “集合讲的是数据，Stream讲的是计算！”\n注意：\n\nStream关注的是对数据的运算，与CPU打交道，集合关注的是数据的存储，与内存打交道。\nStream 自己不会存储元素。\nStream 不会改变源对象。相反，他们会返回一个持有结果的新Stream。\nStream 操作是延迟执行的。这意味着他们会等到需要结果的时候才执行\n\nStream 操作的三个步骤\n创建 Stream\n\n一个数据源（如：集合、数组），获取一个流。\n\n\n中间操作\n\n一个中间操作链，对数据源的数据进行处理。\n\n\n终止操作(终端操作)\n\n一旦执行终止操作，就执行中间操作链，并产生结果，之后，不会再被使用。\n\n\n\n\n注意：\n\n\n一个中间操作链，对数据源的数据进行处理\n一旦执行终止操作，就执行中间操作链，并产生结果。之后，不会再被使用\n\n\n创建Stream实例\n创建 Stream方式一：通过集合\n\nJava8 中的 Collection 接口被扩展，提供了两个获取流的方法：\n\n\n\n方法\n描述\n\n\n\ndefault Stream stream()\n返回一个顺序流\n\n\ndefault Stream parallelStream()\n返回一个并行流\n\n\n代码演示：\n//创建 Stream方式一：通过集合    @Test    public void test1()&#123;        List&lt;Employee&gt; employees = EmployeeData.getEmployees();//        default Stream&lt;E&gt; stream() : 返回一个顺序流        Stream&lt;Employee&gt; stream = employees.stream();//        default Stream&lt;E&gt; parallelStream() : 返回一个并行流        Stream&lt;Employee&gt; parallelStream = employees.parallelStream();    &#125;\n\n\n创建 Stream方式二：通过数组\n\nJava8 中的 Arrays 的静态方法 stream() 可以获取数组流：\n\n\n\n方法\n描述\n\n\n\nstatic  Stream stream(T[] array)\n返回一个流\n\n\npublic static IntStream stream(int[] array)\n返回一个 int 型流\n\n\npublic static LongStream stream(long[] array)\n返回一个 long 型流\n\n\npublic static DoubleStream stream(double[] array)\n返回一个 double 型流\n\n\n代码演示：\n//创建 Stream方式二：通过数组    @Test    public void test2()&#123;        int[] arr = new int[]&#123;1,2,3,4,5,6&#125;;        //调用Arrays类的static &lt;T&gt; Stream&lt;T&gt; stream(T[] array): 返回一个流        IntStream stream = Arrays.stream(arr);        Employee e1 = new Employee(1001,&quot;Tom&quot;);        Employee e2 = new Employee(1002,&quot;Jerry&quot;);        Employee[] arr1 = new Employee[]&#123;e1,e2&#125;;        Stream&lt;Employee&gt; stream1 = Arrays.stream(arr1);    &#125;\n\n\n创建 Stream方式三：通过Stream的of()\n\n可以调用Stream类静态方法 of(), 通过显示值创建一个流。它可以接收任意数量的参数。\n\n\n\n方法\n描述\n\n\n\nspublic static Stream of(T… values)\n返回一个流\n\n\n代码演示：\n//创建 Stream方式三：通过Stream的of()    @Test    public void test3()&#123;        Stream&lt;Integer&gt; stream = Stream.of(1, 2, 3, 4, 5, 6);    &#125;\n\n\n创建 Stream方式四：创建无限流\n\n可以使用静态方法 Stream.iterate() 和 Stream.generate(), 创建无限流。\n\n\n\n方法\n描述\n\n\n\npublic static Stream iterate(final T seed, final UnaryOperator f)\n迭代\n\n\npublic static Stream generate(Supplier s)\n生成\n\n\n代码演示：\n    //创建 Stream方式四：创建无限流    @Test    public void test4()&#123;//      迭代//      public static&lt;T&gt; Stream&lt;T&gt; iterate(final T seed, final UnaryOperator&lt;T&gt; f)        //遍历前10个偶数        Stream.iterate(0, t -&gt; t + 2).limit(10).forEach(System.out::println);//      生成//      public static&lt;T&gt; Stream&lt;T&gt; generate(Supplier&lt;T&gt; s)        Stream.generate(Math::random).limit(10).forEach(System.out::println);    &#125;\n\nStream 的中间操作多个中间操作可以连接起来形成一个流水线，除非流水线上触发终止 操作，否则中间操作不会执行任何的处理！而在终止操作时一次性全部处理，称为“惰性求值”。\n\n筛选与切片\n\n\n\n\n方法\n描述\n\n\n\nfilter(Predicate p)\n接收 Lambda， 从流中排除某些元素\n\n\ndistinct()\n筛选，通过流所生成元素的 hashCode() 和 equals() 去除重复元素\n\n\nlimit(long maxSize)\n截断流，使其元素不超过给定数量\n\n\nskip(long n)\n跳过元素，返回一个扔掉了前 n个元素的流。若流中元素不足 n个，则返回一个空流。与 limit(n) 互补\n\n\n代码演示：\n//1-筛选与切片    @Test    public void test1()&#123;        List&lt;Employee&gt; list = EmployeeData.getEmployees();//        filter(Predicate p)——接收 Lambda ， 从流中排除某些元素。        Stream&lt;Employee&gt; stream = list.stream();        //练习：查询员工表中薪资大于7000的员工信息        stream.filter(e -&gt; e.getSalary() &gt; 7000).forEach(System.out::println);        System.out.println();//        limit(n)——截断流，使其元素不超过给定数量。        list.stream().limit(3).forEach(System.out::println);        System.out.println();//        skip(n) —— 跳过元素，返回一个扔掉了前 n 个元素的流。若流中元素不足 n 个，则返回一个空流。与 limit(n) 互补        list.stream().skip(3).forEach(System.out::println);        System.out.println();//        distinct()——筛选，通过流所生成元素的 hashCode() 和 equals() 去除重复元素        list.add(new Employee(1010,&quot;刘强东&quot;,40,8000));        list.add(new Employee(1010,&quot;刘强东&quot;,41,8000));        list.add(new Employee(1010,&quot;刘强东&quot;,40,8000));        list.add(new Employee(1010,&quot;刘强东&quot;,40,8000));        list.add(new Employee(1010,&quot;刘强东&quot;,40,8000));//        System.out.println(list);        list.stream().distinct().forEach(System.out::println);    &#125;\n\n\n映射\n\n\n\n\n方法\n描述\n\n\n\nmap(Function f)\n接收一个函数作为参数，该函数会被应用到每个元素上，并将其映射成一个新的元素。\n\n\nmapToDouble(ToDoubleFunction f)\n接收一个函数作为参数，该函数会被应用到每个元素上，产生一个新的 DoubleStream。\n\n\nmapToInt(ToIntFunction f)\n接收一个函数作为参数，该函数会被应用到每个元素上，产生一个新的 IntStream。\n\n\nmapToLong(ToLongFunction f)\n接收一个函数作为参数，该函数会被应用到每个元素上，产生一个新的 LongStream。\n\n\nflatMap(Function f)\n接收一个函数作为参数，将流中的每个值都换成另一个流，然后把所有流连接成一个流。\n\n\n代码演示：\n //映射    @Test    public void test2()&#123;//        map(Function f)——接收一个函数作为参数，将元素转换成其他形式或提取信息，该函数会被应用到每个元素上，并将其映射成一个新的元素。        List&lt;String&gt; list = Arrays.asList(&quot;aa&quot;, &quot;bb&quot;, &quot;cc&quot;, &quot;dd&quot;);        list.stream().map(str -&gt; str.toUpperCase()).forEach(System.out::println);//        练习1：获取员工姓名长度大于3的员工的姓名。        List&lt;Employee&gt; employees = EmployeeData.getEmployees();        Stream&lt;String&gt; namesStream = employees.stream().map(Employee::getName);        namesStream.filter(name -&gt; name.length() &gt; 3).forEach(System.out::println);        System.out.println();        //练习2：        Stream&lt;Stream&lt;Character&gt;&gt; streamStream = list.stream().map(StreamAPITest1::fromStringToStream);        streamStream.forEach(s -&gt;&#123;            s.forEach(System.out::println);        &#125;);        System.out.println();//        flatMap(Function f)——接收一个函数作为参数，将流中的每个值都换成另一个流，然后把所有流连接成一个流。        Stream&lt;Character&gt; characterStream = list.stream().flatMap(StreamAPITest1::fromStringToStream);        characterStream.forEach(System.out::println);    &#125;    //将字符串中的多个字符构成的集合转换为对应的Stream的实例    public static Stream&lt;Character&gt; fromStringToStream(String str)&#123;//aa        ArrayList&lt;Character&gt; list = new ArrayList&lt;&gt;();        for(Character c : str.toCharArray())&#123;            list.add(c);        &#125;       return list.stream();    &#125;    @Test    public void test3()&#123;        ArrayList list1 = new ArrayList();        list1.add(1);        list1.add(2);        list1.add(3);        ArrayList list2 = new ArrayList();        list2.add(4);        list2.add(5);        list2.add(6);//        list1.add(list2);        list1.addAll(list2);        System.out.println(list1);    &#125;\n\n\n排序\n\n\n\n\n方法\n描述\n\n\n\nsorted()\n产生一个新流，其中按自然顺序排序。\n\n\nsorted(Comparator com)\n产生一个新流，其中按比较器顺序排序。\n\n\n代码演示：\n//3-排序    @Test    public void test4()&#123;//        sorted()——自然排序        List&lt;Integer&gt; list = Arrays.asList(12, 43, 65, 34, 87, 0, -98, 7);        list.stream().sorted().forEach(System.out::println);        //抛异常，原因:Employee没有实现Comparable接口//        List&lt;Employee&gt; employees = EmployeeData.getEmployees();//        employees.stream().sorted().forEach(System.out::println);//        sorted(Comparator com)——定制排序        List&lt;Employee&gt; employees = EmployeeData.getEmployees();        employees.stream().sorted( (e1,e2) -&gt; &#123;           int ageValue = Integer.compare(e1.getAge(),e2.getAge());           if(ageValue != 0)&#123;               return ageValue;           &#125;else&#123;               return -Double.compare(e1.getSalary(),e2.getSalary());           &#125;        &#125;).forEach(System.out::println);    &#125;\n\nStream 的终止操作\n终端操作会从流的流水线生成结果。其结果可以是任何不是流的值，例 如：List、Integer，甚至是 void 。\n\n\n流进行了终止操作后，不能再次使用。\n\n\n匹配与查找\n\n\n\n\n方法\n描述\n\n\n\nallMatch(Predicate p)\n检查是否匹配所有元素。\n\n\nanyMatch(Predicate p)\n检查是否至少匹配一个元素。\n\n\nnoneMatch(Predicate p)\n检查是否没有匹配所有元素\n\n\nfindFirst()\n返回第一个元素\n\n\nfindAny()\n返回当前流中的任意元素\n\n\ncount()\n返回流中元素总数\n\n\nmax(Comparator c)\n返回流中最大值\n\n\nmin(Comparator c)\n返回流中最小值\n\n\nforEach(Consumer c)\n内部迭代(使用 Collection 接口需要用户去做迭代，称为外部迭代。相反，Stream API 使用内部迭代——它帮你把迭代做了)\n\n\n代码演示：\n //1-匹配与查找    @Test    public void test1()&#123;        List&lt;Employee&gt; employees = EmployeeData.getEmployees();//        allMatch(Predicate p)——检查是否匹配所有元素。//          练习：是否所有的员工的年龄都大于18        boolean allMatch = employees.stream().allMatch(e -&gt; e.getAge() &gt; 18);        System.out.println(allMatch);//        anyMatch(Predicate p)——检查是否至少匹配一个元素。//         练习：是否存在员工的工资大于 10000        boolean anyMatch = employees.stream().anyMatch(e -&gt; e.getSalary() &gt; 10000);        System.out.println(anyMatch);//        noneMatch(Predicate p)——检查是否没有匹配的元素。//          练习：是否存在员工姓“雷”        boolean noneMatch = employees.stream().noneMatch(e -&gt; e.getName().startsWith(&quot;雷&quot;));        System.out.println(noneMatch);//        findFirst——返回第一个元素        Optional&lt;Employee&gt; employee = employees.stream().findFirst();        System.out.println(employee);//        findAny——返回当前流中的任意元素        Optional&lt;Employee&gt; employee1 = employees.parallelStream().findAny();        System.out.println(employee1);    &#125;    @Test    public void test2()&#123;        List&lt;Employee&gt; employees = EmployeeData.getEmployees();        // count——返回流中元素的总个数        long count = employees.stream().filter(e -&gt; e.getSalary() &gt; 5000).count();        System.out.println(count);//        max(Comparator c)——返回流中最大值//        练习：返回最高的工资：        Stream&lt;Double&gt; salaryStream = employees.stream().map(e -&gt; e.getSalary());        Optional&lt;Double&gt; maxSalary = salaryStream.max(Double::compare);        System.out.println(maxSalary);//        min(Comparator c)——返回流中最小值//        练习：返回最低工资的员工        Optional&lt;Employee&gt; employee = employees.stream().min((e1, e2) -&gt; Double.compare(e1.getSalary(), e2.getSalary()));        System.out.println(employee);        System.out.println();//        forEach(Consumer c)——内部迭代        employees.stream().forEach(System.out::println);        //使用集合的遍历操作        employees.forEach(System.out::println);    &#125;\n\n\n归约\n\n\n\n\n方法\n描述\n\n\n\nreduce(T iden, BinaryOperator b)\n可以将流中元素反复结合起来，得到一个值。返回 T\n\n\nreduce(BinaryOperator b)\n可以将流中元素反复结合起来，得到一个值。返回 Optional\n\n\n代码演示：\n//2-归约    @Test    public void test3()&#123;//        reduce(T identity, BinaryOperator)——可以将流中元素反复结合起来，得到一个值。返回 T//        练习1：计算1-10的自然数的和        List&lt;Integer&gt; list = Arrays.asList(1,2,3,4,5,6,7,8,9,10);        Integer sum = list.stream().reduce(0, Integer::sum);        System.out.println(sum);//        reduce(BinaryOperator) ——可以将流中元素反复结合起来，得到一个值。返回 Optional&lt;T&gt;//        练习2：计算公司所有员工工资的总和        List&lt;Employee&gt; employees = EmployeeData.getEmployees();        Stream&lt;Double&gt; salaryStream = employees.stream().map(Employee::getSalary);//        Optional&lt;Double&gt; sumMoney = salaryStream.reduce(Double::sum);        Optional&lt;Double&gt; sumMoney = salaryStream.reduce((d1,d2) -&gt; d1 + d2);        System.out.println(sumMoney.get());    &#125;\n\n\n收集\n\n\n\n\n方法\n描述\n\n\n\ncollect(Collector c)\n将流转换为其他形式。接收一个 Collector接口的实现，用于给Stream中元素做汇总的方法\n\n\nCollector 接口中方法的实现决定了如何对流执行收集的操作(如收集到 List、Set、Map)。\n另外， Collectors 实用类提供了很多静态方法，可以方便地创建常见收集器实例，具体方法参考文档：https://www.matools.com/api/java8\n代码演示：\n//3-收集    @Test    public void test4()&#123;//        collect(Collector c)——将流转换为其他形式。接收一个 Collector接口的实现，用于给Stream中元素做汇总的方法//        练习1：查找工资大于6000的员工，结果返回为一个List或Set        List&lt;Employee&gt; employees = EmployeeData.getEmployees();        List&lt;Employee&gt; employeeList = employees.stream().filter(e -&gt; e.getSalary() &gt; 6000).collect(Collectors.toList());        employeeList.forEach(System.out::println);        System.out.println();        Set&lt;Employee&gt; employeeSet = employees.stream().filter(e -&gt; e.getSalary() &gt; 6000).collect(Collectors.toSet());        employeeSet.forEach(System.out::println);    &#125;\n\n","categories":["总结笔记"],"tags":["java8","类型推断","类型注解","java8-stream"]},{"title":"重温滕王阁","url":"/2017_05_11_tengwanggexu/","content":"读《滕王阁序》，我会在开始时同情王勃的遭遇；然而，读至“老当益壮，宁移白首之心，穷且益坚，不坠青云之志”时，我便渐渐由同情变为了赞赏与佩服。这篇骈文是才华横溢的才子王勃仕途终结之作，面对如此人生困境，他也能即兴诵出这等有气势的句子，实在让人可叹，可敬。今天，记录于此，留待后日继续欣赏。\n\n\n原文王勃\n豫章故郡，洪都新府。星分翼轸(zhěn)，地接衡庐。襟三江而带五湖，控蛮荆而引瓯（ōu）越。物华天宝，龙光射牛斗之墟；人杰地灵，徐孺下陈蕃之榻。雄州雾列，俊采星驰，台隍(huáng)枕夷夏之交，宾主尽东南之美。都督阎公之雅望，棨(qǐ )戟(jǐ)遥临；宇文新州之懿(yì)范，襜(chān )帷(wéi)暂驻。十旬休假，胜友如云；千里逢迎，高朋满座。腾蛟起凤，孟学士之词宗；紫电青霜，王将军之武库。家君作宰，路出名区；童子何知，躬逢胜饯。\n时维九月，序属三秋。潦（lǎo）水尽而寒潭清，烟光凝（ning)而暮山紫。俨(yǎn)骖騑(cān fēi)于上路，访风景于崇阿。临帝子之长洲，得仙人之旧馆。层峦耸翠，上出重霄；飞阁流丹，下临无地。鹤汀（tīng）凫(fú )渚，穷岛屿之萦(yíng)回；桂殿兰宫，即冈峦之体势。\n披绣闼（tà），俯雕甍(méng )。山原旷其盈视，川泽纡(yū)其骇瞩。闾(lǘ)阎(yán) 扑地，钟鸣鼎食之家；舸（gě)舰弥津，青雀黄龙之轴（zhú)。云销雨霁(jì)，彩彻区明（云衢）。落霞与孤鹜齐飞，秋水共长天一色。渔舟唱晚，响穷彭蠡（lǐ ）之滨；雁阵惊寒，声断衡阳之浦。\n遥襟俯畅，逸兴遄(chuán)飞。爽籁发而清风生，纤歌凝而白云遏(è)。睢(suī)园绿竹，气凌彭泽之樽(zūn)；邺(yè)水朱华，光照临川之笔。四美具，二难并。穷睇眄(dì miǎn)于中天，极娱游于暇日。天高地迥，觉宇宙之无穷；兴尽悲来，识盈虚之有数。望长安于日下，目吴会（kuài）于云间。地势极而南溟深，天柱高而北辰远。关山难越，谁悲失路之人；萍水相逢，尽是他乡之客。怀帝阍(hūn)而不见，奉宣室以何年？\n嗟乎！时运不齐，命途多舛(chuǎn)；冯唐易老，李广难封。屈贾谊（yì）于长沙，非无圣主；窜梁(liang)鸿于海曲，岂乏明时？所赖君子见机，达人知命。老当益壮，宁移白首之心；穷且益坚，不坠青云之志。酌贪泉而觉爽，处涸辙（hézhé）而尤欢。北海虽赊（shē），扶摇可接；东隅已逝，桑榆非晚。孟尝高洁，空余报国之情；阮籍猖狂，岂效穷途之哭？\n勃，三尺微命，一介书生，无路请缨，等终军之弱冠（guàn）；有怀投笔，慕宗悫（què）之长风。舍簪（zān）笏（hù）于百龄，奉晨昏于万里。非谢家之宝树，接孟氏之芳邻。他日趋庭，叨(tāo)陪鲤对；今晨捧袂(mèi)，喜托龙门。杨意不逢，抚凌云而自惜；钟期既遇，奏流水以何惭？\n呜呼！胜地不常，盛筵(yán)难再；兰亭已矣，梓(zi)泽(ze)丘墟。临别赠言，幸承恩于伟饯；登高作赋，是所望于群公。敢竭鄙怀，恭疏短引；一言均赋，四韵俱成；请洒潘江，各倾陆海云尔。\n译文汉代的豫章旧郡，现在称洪都府。它处在翼、轸二星的分管区域，与庐山和衡山接壤。以三江为衣襟，以五湖为腰带，控制楚地，连接瓯越。这里地上物产的精华，乃是天的宝物，宝剑的光气直射牛、斗二星之间；人有俊杰是因为地有灵秀之气，徐孺子竟然在太守陈蕃家下榻(世说新语记载，太守陈蕃赏识徐孺子，专门为其在家中设置榻，当徐孺子来的时候，就将榻放下来，徐孺子走了就将榻吊起来，此处应该是称赞滕王阁的东道主欣赏才俊，也有夸赞宾客的成分)。雄伟的州城像雾一样涌起，俊美的人才像流星一样飞驰。城池倚据在荆楚和华夏交接的地方，宴会上客人和主人都是东南一带的俊杰。声望崇高的阎都督公，（使）打着仪仗（的高人）远道而来；德行美好的宇文新州刺史，（让）驾着车马（的雅士）也在此暂时驻扎。正好赶上十日一休的假日，才华出众的朋友多得如云；迎接千里而来的客人，尊贵的朋友坐满宴席。文章的辞彩如蛟龙腾空、凤凰飞起，那是文词宗主孟学士；紫电和清霜这样的宝剑，出自王将军的武库里。家父做交趾县令，我探望父亲路过这个有名的地方；我一个小孩子知道什么，却有幸亲自遇到了这样盛大的宴会。\n时间是九月，季节为深秋。蓄积的雨水已经消尽，潭水寒冷而清澈，烟光雾气凝结，傍晚的山峦呈现出紫色。驾着豪华的马车行驶在高高的道路上，到崇山峻岭中观望风景。来到滕王营建的长洲上，看见他当年修建的楼阁。重叠的峰峦耸起一片苍翠，上达九霄；凌空架起的阁道上，朱红的油彩鲜艳欲滴，从高处往下看，地好像没有了似的。仙鹤野鸭栖止的水边平地和水中小洲，极尽岛屿曲折回环的景致；桂树与木兰建成的宫殿，随着冈峦高低起伏的态势。\n打开精美的阁门，俯瞰雕饰的屋脊，放眼远望辽阔的山原充满视野，迂回的河流湖泊使人看了惊叹。房屋排满地面，有不少官宦人家；船只布满渡口，都装饰着青雀黄龙的头形。云消雨散，阳光普照，天空明朗。落霞与孤独的野鸭一齐飞翔，秋天的江水和辽阔的天空浑然一色。渔船唱着歌傍晚回来，歌声响遍鄱阳湖畔；排成行列的大雁被寒气惊扰，叫声消失在衡阳的水边。\n远望的胸怀顿时舒畅，飘逸的兴致油然而生。排箫发出清脆的声音，引来阵阵清风；纤细的歌声仿佛凝住不散，阻止了白云的飘动。今日的宴会很像是当年睢园竹林的聚会，在座的诗人文士狂饮的气概压过了陶渊明；又有邺水的曹植咏荷花那样的才气，文采可以直射南朝诗人谢灵运。良辰、美景、赏心、乐事，四美都有，贤主、嘉宾，难得却得。放眼远望半空中，在闲暇的日子里尽情欢乐。天高地远，感到宇宙的无边无际；兴致已尽，悲随之来，认识到事物的兴衰成败有定数。远望长安在夕阳下，遥看吴越在云海间。地势偏远，南海深不可测；天柱高耸，北极星远远悬挂。雄关高山难以越过，有谁同情不得志的人?在座的各位如浮萍在水上相聚，都是客居异乡的人。思念皇宫却看不见，等待在宣室召见又是何年?\n唉!命运不顺畅，路途多艰险。冯唐容易老，李广封侯难。把贾谊贬到长沙，并非没有圣明的君主；让梁鸿到海边隐居，难道不是在政治昌明的时代?能够依赖的是君子察觉事物细微的先兆，通达事理的人知道社会人事的规律。老了应当更有壮志，哪能在白发苍苍时改变自己的心志?处境艰难反而更加坚强，不放弃远大崇高的志向。喝了贪泉的水，仍然觉得心清气爽；处在干涸的车辙中，还能乐观开朗。北海虽然遥远，乘着旋风仍可以到达；少年的时光虽然已经消逝，珍惜将来的岁月还不算晚。孟尝品行高洁，却空有一腔报国的热情；怎能效法阮籍狂放不羁，在无路可走时便恸哭而返?\n我，地位低下，一个书生。没有请缨报国的机会，虽然和终军的年龄相同；像班超那样有投笔从戎的胸怀，也仰慕宗悫“乘风破浪”的志愿。宁愿舍弃一生的功名富贵，到万里之外去早晚侍奉父亲。不敢说是谢玄那样的人才，却结识了诸位名家。过些天到父亲那里聆听教诲，一定要像孔鲤那样趋庭有礼，对答如流；今天举袖作揖谒见阎公，好像登上龙门一样。司马相如倘若没有遇到杨得意那样引荐的人，虽有文才也只能独自叹惋。既然遇到钟子期那样的知音，演奏高山流水的乐曲又有什么羞惭呢？\n唉!名胜的地方不能长存，盛大的宴会难以再遇。当年兰亭宴饮集会的盛况已成为陈迹了，繁华的金谷园也成为荒丘废墟。临别赠言，作为有幸参加这次盛宴的纪念；登高作赋，那就指望在座的诸公了。冒昧给大家献丑，恭敬地写下这篇小序，我的一首四韵小诗也已写成。请各位像潘岳、陆机那样，展现如江似海的文才吧。\n作者《滕王阁序》，体裁：辞赋、骈文（魏晋以来产生的一种文体，又称骈俪文。骈文是与散文相对而言的。其主要特点是以四六句式为主，讲究对仗，因句式两两相对，犹如两马并驾齐驱，故被称为骈体。在声韵上，则讲究运用平仄，韵律和谐；修辞上注重藻饰和用典。由于骈文注重形式技巧，故内容的表达往住受到束缚，但运用得当，也能增强文章的艺术效果）；作者：王勃；别名：秋日登洪府滕王阁饯别序；年代：唐朝。\n王勃（649～675年），唐代诗人。字子安。绛州龙门(今山西河津)人。王勃与杨炯、卢照邻、骆宾王以诗文齐名，并称“王杨卢骆”，亦称“初唐四杰”。王勃为隋末大儒王通的孙子（王通是隋末著名学者，号文中子），王通生二子，长名福郊，次名福峙，福峙即王勃之父，曾出任太常博士、雍州司功、交趾县令、六合县令、齐州长史等职。可知王勃生长于书香之家。\n王勃才华早露，未成年即被司刑太常伯刘祥道赞为神童，向朝廷表荐，对策高第，授朝散郎。乾封初(666年)为沛王李贤征为王府侍读，两年后因戏为《檄英王鸡》文，被高宗怒逐出府。随即出游巴蜀。咸亨三年(672年)补虢州参军，因擅杀官奴当诛，遇赦除名。其父亦受累贬为交趾令。上元二年(675年)或三年(676年)，王勃南下探亲，渡海溺水，惊悸而死。其诗力求摆脱齐梁的绮靡诗风，文也有名，著名的《滕王阁序》就出自他之手。\n写作背景滕王阁因滕王李元婴得名。李元婴是唐高祖李渊的幼子，唐太宗李世民的弟弟，骄奢淫逸，品行不端，毫无政绩可言。但他精通歌舞，善画蝴蝶，很有艺术才情。他修建滕王阁，也是为了歌舞享乐的需要。这座江南名楼建于唐朝繁盛时期，又因王勃的一篇《滕王阁序》而很快出名。韩愈在《新修滕王阁记》中说：“愈少时，则闻江南多临观之美，而滕王阁独为第一，有瑰伟绝特之称。”\n《滕王阁序》全称《秋日登洪府滕王阁饯别序》，又名《滕王阁诗序》《宴滕王阁序》，写于何时，有两种说法。唐末五代时人王定保的《唐摭言》说：“王勃著《滕王阁序》，时年十四。”那时，王勃的父亲可能任六合县(今属江苏)令，王勃赴六合经过洪州。又这篇序文中有“童子何知，躬逢胜饯”之语，也可佐证。元代辛文房《唐才子传》认为《滕王阁序》是上元二年(675)王勃前往交趾(在现在越南河内西北)看望父亲(那时他父亲任交趾县令)，路过南昌时所作。从这篇序文内容的博大、辞采的富赡来看，更像是成年作品。“童子”不一定就是指小孩，也可以是表示自己年轻无知的谦词。何况序文中有“无路请缨，等终军之弱冠”的话，“弱冠”是指二十岁。所以，关于写作时间，课文的注释解说采用后一种说法。\n《新唐书·文艺传》记滕王阁诗会为：“九月九日都督大宴滕王阁，宿命其婿作序以夸客，因出纸笔遍请客，莫敢当，至勃，泛然不辞。都督怒，起更衣，遣吏伺其文辄报。一再报，语益奇，乃矍然曰：‘天才也!’请遂成文，极欢罢。”可见当时王勃年轻气盛，才华横溢，挥毫泼墨，语惊四座的情景。\n关于王勃的生卒年，至今尚有歧说。杨炯《王勃集序》说他于唐高宗上元三年（676年）卒，年28岁。据此，王勃应生于唐太宗贞观二十三年（649年）。而王勃《春思赋》载：“咸亨二年（671年），余春秋二十有二。”据此推算，则当生于高宗永徽元年（65O年）。此为王勃自述，当可信，所以现在大多数学者认为王勃生于永徽元年（650年），卒于上元三年（676年），生年27岁。王勃是初唐诗坛上一位非常有才华的诗人，只活了27岁，确实令人痛惜。\n王勃自幼聪慧好学，为时人所公认。《旧唐书》本传谓王勃：“六岁解属文，构思无滞，词情英迈，与兄才藻相类，父友杜易简常称之曰：此王氏三珠树也。”又有杨炯《王勃集序》说：“九岁读颜氏《汉书》，撰《指瑕》十卷。十岁包综六经，成乎期月，悬然天得，自符音训。时师百年之学，旬日兼之，昔人千载之机，立谈可见。”太常伯刘公称王绩为神童。唐高宗麟德元年（664年），王勃上书右相刘祥道，中有“所以慷慨于君侯者，有气存乎心耳”之语，求刘祥道表荐。刘即表荐于朝，王勃乃应麟德三年（666年）制科，对策高第，被授予朝散郎之职。此时的王勃，才14岁，尚是一少年。\n沛王李贤闻王勃之名，召王勃为沛府修撰，十分爱重他。当时诸王经常斗鸡为乐，王勃闹着玩，写了一篇《檄周王鸡》，不料竟因此罹祸，唐高宗认为是使诸王闹矛盾，将王勃赶出沛王府。其实王勃此次受打击，并非真的因《檄周王鸡》而触怒高宗，而是因才高被嫉，所以杨炯《王勃集序》说他“临秀不容，寻反初服”。王勃被赶出沛王府后，便去游蜀，与杨炯等放旷诗酒，驰情于文场。《旧唐书·杨炯传》说：“炯与王勃、卢照邻、骆宾王以文词齐名，海内称为王杨卢骆，亦号为四杰。”\n初唐四杰，在中国文学史上是一个非常著名的集团。作为一个集团，他们反对六朝以来颓废绮丽的风气，提出一些革新意见，开始把诗文从宫廷引向市井，从台阁移到江山和边塞，题材扩大了，风格也较清新刚健，对于革除齐梁余风、开创唐诗新气象，起了重要的作用。讲中国文学史，尤其是唐代文学史，没有不讲到王杨卢骆的。\n王勃所遇到的第二次打击，是在虢州参军任上杀死自己所匿藏的官奴而犯罪。咸亨二年（671年）秋冬或第二年年初，王勃从蜀地返回长安参加科选。他的朋友凌季友当时为虢州司法，说虢州药物丰富，而他知医识药草，便为他在虢州谋得一个小小的参军之职。就在他任虢州参军期间，有个叫曹达的官奴犯罪，他将罪犯藏匿起来，后来又怕走漏风声，便杀死曹达以了其事，结果因此而犯了死罪。幸亏遇大赦，没有被处死。此事甚为蹊跷，王勃为什么要保护罪犯曹达，既藏匿保护又怎能将其杀死。据新旧《唐书》所载，王勃此次被祸，是因情才傲物，为同僚所嫉。官奴曹达事，有人怀疑为同僚设计构陷王勃，或者纯属诬陷，不无道理。总之王勃两次遭受打击，都与他的才华超人有关。\n这次被祸，虽遇赦未丢掉性命，但宣告了他仕途的终结，也连累了他的父亲。王福峙因儿子王勃犯罪，被贬为交趾县令，远谪到南荒之外。王勃远行到交趾去看望父亲，途中溺水而死，从而结束了他年轻的生命。王勃的死，是渡水时遇难不幸而死，还是自杀，无从查考，总归是怀着一腔愁愤离开人世的。\n;王勃诗文俱佳，不愧为四杰之首，在扭转齐梁余风、开创唐诗上功劳尤大，为后世留下了一些不朽名篇。他的五言律诗《送杜少府之任蜀州》，成为中国诗歌史上的杰作，久为人们所传诵，“海内存知己，天涯若比邻”已成为千古名句，至今常被人们引用。而王勃最为人所称道、千百年来被传为佳话的，是他在滕王阁即席所赋《滕王阁序》。对此事，《唐摭言》所记最详。\n上元二年（675年）秋，王勃前往交趾看望父亲，路过南昌时，正赶上都督阎伯屿新修滕王阁成，重阳日在滕王阁大宴宾客。王勃前往拜见，阎都督早闻他的名气，便请他也参加宴会。阎都督此次宴客，是为了向大家夸耀女婿孟学士的才学。让女婿事先准备好一篇序文，在席间当作即兴所作书写给大家看。宴会上，阎都督让人拿出纸笔，假意请诸人为这次盛会作序。大家知道他的用意，所以都推辞不写，而王勃以一个二十几岁的青年晚辈，竟不推辞，接过纸笔，当众挥笔而书。阎都督老大不高兴，拂衣而起，转入帐后，教人去看王勃写些什么。听说王勃开首写道“南昌故都，洪都新府”，都督便说：不过是老生常谈。又闻“星分翼轸，地接衡庐”，沉吟不语。等听到“落霞与孤骛齐飞，秋水共长天一色”，都督不得不叹服道：“此真天才，当垂不朽！”。《唐才子传》则记道：“勃欣然对客操觚，顷刻而就，文不加点，满座大惊。”\n《唐摭言》等书所记，或者有些夸张，但王勃《滕王阁序》，确实为不朽之名篇。王勃于南昌阎都督宴上赋《滕王阁序》的佳话。实乃中国文学史上最为动人的故事。《新唐书》本传说王勃“属文，初不精思，先磨墨数升，则酣饮，引被覆面卧，及寤，援笔成篇，不易一字。”唐人段成式《酉阳杂俎》也说；“王勃每为碑颂，先磨墨数升，引被覆面卧，忽起一笔数之，初不窜点，时人谓之腹稿。”据此可知王勃文思敏捷，滕王阁上即兴而赋千古名篇，并非虚传。王勃作为古代一位极富才华的作家，未及而立之年便逝去，实在是中国文学的一大损失。\n王勃虽然只活了27个春秋，但著述仍很多，曾撰《汉书指瑕》十卷，《周易发挥》五卷，《次论语》十卷，《舟中纂序》五卷，《千岁历》若干卷，可惜皆佚失。今所传者，唯《王子安集》16卷，也非全本。何林天教授点校整理的《重订新校王子安集》，收录了辑自日本的一些佚文，已由山西人民出版社出版\n艺术特色《滕王阁序》是唐人王勃用骈文写的佳作。此文内容丰富，情真意切；对仗工整，声律和谐；手法多变，格调迥异；骈散结合，语言华美。千百年来，一直被世人广为传颂。在高中教材中，此文一直为广大师生所喜爱。这里，仅就个人对文章的一点浅陋理解，将本文的艺术手法作以简要梳理。\n一、文思细密，层层扣题本文原题是《秋日登洪府滕王阁饯别序》，全文谋篇布局，无不统于题目之下。第一段概写洪州地势之雄伟，物产之华美，人才之优秀，宾主之尊贵，紧扣题目“洪府”二字。第二段先叙写清澈幽寒之水，青紫暮色之山；再写仙境般之长洲，桂殿般之楼阁，展示了一幅壮美秀丽的滕王阁秋景图，紧扣题目“秋日” 、“登滕王阁”六字。第三段写滕王阁及周围景色之美，再一次紧扣题目“滕王阁”。第四段集中笔墨写阁中宴会场面，赞文人雅士之气概和风采，特别是“四美俱，二难并”一句，对良辰、美景、赏心、乐事和贤主嘉宾作总结赞美，紧扣题目“饯”。后面三段，笔锋一转，由盛赞良辰美景、文人雅士转为慨叹人生艰难、命运多舛，表达了正视现实、奋发向上的态度。最后交代有幸参加盛会，理当应命作序，紧扣题目“饯”“序”二字。纵观全文，由地及人、由人及景、由景及情，步步递进，层层扣题。\n二、写景状物，手法多变王勃在文中运用灵活多变的手法描山绘水，将读者带入一种绝妙的佳境。    1、景情结合法。文章前半部分侧重写景，生动地展示了滕王阁壮美秀丽的图景，描绘了宴游唱和之欢乐。后半部分触景生情，反复抒发了作客他乡、怀才不遇的感慨，表达了对现实的不满。文中虽流露出相信时命运数的消极思想，但作者正视现实和奋发向上的进取精神在文中无处不在。可谓景中有情，情中见景。    2、色彩变化法。文章不惜笔墨，极力渲染，尽展色彩变化之美。如“紫电”、“青霜”、“耸翠”、“青雀”、“黄龙”、“白云”等，真可谓五光十色，摇曳生辉。特别是“潦水尽而寒潭清，烟光凝而暮山紫”一句，尽力表现出山光水色之色彩变化，上句用色淡雅，下句着色浓重，在对比中突出了秋日景象之色。    3、诗画统一法。文章第三段浓墨重彩地将滕王阁及周围景色推上了美的极致。文中写阁门美、屋宇美、山川美，写遍地宅舍、舸舰迷江、渔舟唱晚，真可谓是山清水秀、国富民强的江南风景图，特别是“落霞与孤鹜齐飞，秋水共长天一色”一句，既具有诗的意境，又兼备画的情调，想时像一幅充满诗意的大自然风景画，读时像一首妙趣绝伦的好诗，真是诗中有画，画中有诗。    4、虚实相间法。作者登高望远，不仅驰目四方，而且思接万里。文中既实写目之所见，又发挥想像，描摹目力难极之景。如为渲染阁中气氛，作者借助联想，让乐声唤来徐徐清风，让歌声阻遏高空行云，让雅士像陶渊明那样畅怀痛饮，让文人像谢灵运那般能诗善写。如此虚实相间，不仅使读者对所写之景有具体真实之感，又使读者视野开阔，目通万里。\n三、含蓄委婉，述志言情在大量的铺陈叙事之后，作者借含蓄委婉的笔法，腾挪跌宕之气势，宴游唱和之欢娱引出人世之艰难，仕途之崎岖，怀才之不遇，抒写了报国无门却奋发向上的执着态度。如第四自然段，兴尽悲来，“望长安于日下，指吴会于云间”委婉地抒发远离京城浪迹天涯之情。接着由关山难越，想到仕途失意，借屈原、贾谊、冯唐、李广的典故，抒发有志难伸的感慨，含蓄地表达了对所谓“圣君”“明时”的不满。“所赖君子安贪”三句，表达不因年华易逝和处境困顿而自暴自弃的精神，又以大鹏自比，表明扶摇直上九霄的凌云壮志。而后又借“失之东隅，收之桑榆”的说法，表明早年虽失意，但拯时匡世之信念并未泯灭。同时又反用“贪泉”“涸辙”“阮籍”之典，说明“出淤泥而不染”、“穷且益坚”之志。作者正是借多样的历史典故，委婉含蓄地述志言情。\n四、词藻华美，语约言丰唐初的骈体文，还有齐梁之余风，辞藻繁多，典故滥用，即以形式上的浮艳来掩盖内容上的空虚，但王勃的《滕王阁序》却用骈文的形式表达丰富的内容，再现了交织于内心的失望与希望、痛苦与追求、落魄与奋进的感情历程，真可谓辞藻华美，语约言丰。其中妙词佳句，今天还脍炙人口，广为流传。如“星分翼轸”、“物华天宝”、“人杰地灵”、“雄州雾列”、“俊采星驰”、“胜友如云”、“高朋满座”、“ 关山难越”、“萍水相逢”、“冯唐易老，李广难封”、“老当益壮”、“穷且益坚”、“不坠青云之志”、“落霞与孤鹜齐飞，秋水共长天一色”……读之如饮醇酒，久而弥笃。典故如陶渊明、曹植、谢灵运、冯唐、李广、贾谊、梁鸿、贪泉、涸辙、北海、阮籍……这些典故并未使人陷入浓云晦雾之中，而是清新疏朗，意味隽永。读之令人荡气回肠，感染力极强。\n总之，《滕王阁序》是古代骈文的精品，它既发挥了骈文特有的铺陈描写手法，又运用形散而神不散的散文之气于骈偶之中。字字绝妙，句句传神，章章生辉，使人读后有身临其境之感，正如都督阎公所言：“此真天才，当垂不朽矣！”它不但以其真实的情感和充实的内容区别于六朝及其以前那些无病呻吟或嘲风弄月者，而且打破了僵死陈旧的骈文格局和陈陈相因的文风，给骈文注入新的血液。\n读后感滕王阁序读后感（一）林中流淌的溪水，面前总会有巨石、横木拦住去路，河道也并非笔直，而是曲曲折折，总是阻止溪水前进。人生也正是这样。人所踏上的道路不是一帆风顺的，而是坎坷密布，荆棘丛生。时运不济，命途多舛。\n在大唐王朝强盛的唐高宗年间，从绛州龙门走出了一位才高八斗的文人。他就是初唐四杰之一的王勃。他才华早露，十四岁时即被授予官职。然而他却在仕途至终因才华横溢而遭受了两次打击。这也宣告了他仕途的终结。\n上元二年，滕王阁上，他即席作赋，写下了千古名篇《滕王阁序》，为历代传颂赞赏。今日，读《滕王阁序》，我会在开始时同情王勃的遭遇。然而，读至“老当益壮，宁移白首之心，穷且益坚，不坠青云之志”时，我便渐渐由同情变为了赞赏与佩服。读罢此文，我深有感触。\n林中的溪水虽身处曲折的河道中，面对拦路的艰难险阻，却毫不畏惧、毫不气馁，只是聚成一股又一股的水流冲向障碍，冲破障碍，流向远方。人在经历了失败、打击、挫折后，需要一种乐观向上的心态，拥有这种心态后，人就会变得不畏困难，像溪水一样勇于面对，勇于承担，勇于挑战，在摔倒之后满怀信心地再度站起，为追寻成功继续前行。\n王勃前往交趾看望自己被贬的父亲途中，心中还怀着两次打击给他留下的阴影。然而在《滕王阁序》却表现出了一种积极的壮怀。“酌贪泉而觉爽，处涸辙以犹欢”使人精神一振，感受到作者那种身处逆境却仍乐观向上的心情。西汉史学家司马迁惨遭酷刑，却最终完成了“史家绝唱”的《史记》。他在《报任安书》中写道盖西伯拘而演《周易》；仲尼厄而作《春秋》；屈原放逐，乃赋《离骚》；左丘失明，厥有《国语》；孙子膑脚，《兵法》修列；不韦迁蜀，世传《吕览》；韩非囚秦，《说难》、《孤愤》。《诗》三百篇，大氐贤圣发愤之所为作也。发明家爱迪生，失败两千多次后方才成功。音乐家贝多芬失聪，仍作出了一生中最伟大的音乐篇章。经历了痛苦的生命才能称其为人，真正的成功者都是从痛苦中超度出来的。古人在逆境，经历失败，这些不仅没有束缚他们的手脚，反而成就了他们的成功。若只是一味地感叹命运的不公，只会停滞不前，碌碌无为终此一生。\n人生失意后，重要的不是别人的雪中送炭，而是自身需要一种积极乐观的态度去面对一切。命运是无情的，即使是叱咤疆场的一代名将李广也没有得到命运之神的眷顾，终身未得封侯，自刎沙场。海伦凯勒曾说过：“对于无可挽回的事，就应想得开点，不要总强求不可能的结果。”真正重要的并不是在一个人的身上发生了什么，而是这个人如何去看待。人不能只为自己的命运叹息，而是应该努力去改变命运。而改变命运就需要有向困难挑战的勇气和永不放弃的信心。而这些就需要心态的乐观。拥有了乐观的心态，人的心胸会变得宽广，不会总因失败而痛苦，心中会产生希望，进而会产生动力，使人继续向成功迈进。心态是成功的基石，正如一位名人所言：“播下一种心态，收获一种思想；播下一种思想，收获一种行为；播下一种行为，收获一种习惯；播下一种习惯，收获一种性格；播下一种性格，收获一种命运。”一切的根源就是一种心态。人如果改变了心态，就能改变他的命运。积极的心态，能使人重新振作，重拾信心；积极的心态，能使人不畏挑战，勇往直前；积极的心态，能使人坚持不懈，持之以恒，积极的心态，能使人超越自我，走向成功。\n北海虽赊，扶摇可接，东隅已逝，桑榆非晚。林中的溪水，终有一天会流到广阔的大洋。人如果永怀积极乐观之心，终会铸成人生的辉煌。\n滕王阁序读后感（二）是夜，独坐于笼中，一盏青灯，半手残卷，摇头晃脑，念念有词，假马拉鬼了半天，也没见一只从天而将，衣袂飘飘的白狐，都是骗子，哪里来的什么白狐，算了，我也不是什么书生，遇不来也罢。\n没来由的瞌睡，怎么这么困呢，想想白天也没干嘛啊，砸路灯，抢银行，打飞机，抢登钓鱼岛，这些也都不是我干的啊。不过在这里还是要向那几个妄图登陆钓鱼岛宣誓主权的英雄们致敬，你们的爱国热血激励了我们的什么什么，尽管已经麻木与主权是为谁而宣，但你们仍旧是英雄。我党呢已经见怪不怪了，按兵不动，坚持敌不动，我不动；敌动了，我抗议的一贯作风，只要不侵犯我党利益一切好说。一不小心又扯远了。\n还是来说说解困，音乐已经不行了，听的都想吐了，就在灵魂即将出窍之际，又看到了这篇奇文，拿来一读，不得了啊，暑气顿消，困意全无啊，顿时神清气爽，夜不能寐啊。\n正是王勃的《滕王阁序》，千古一序，名不虚传啊：\n揽汉唐人文成一序，绝江山美景于片言。\n此序一出，有没有滕王阁都无所谓了，什么文以阁名，阁以文传才是真，这滕王阁本一歌舞之地，现在矗在南昌那的意义也就是为南昌市人民政府挣点钱罢了，要说慕名前去看下我看十有八九，说绝对点肯定会失望的，就像你看新闻联播，跟现实完全不搭界（其实看新闻联播也就节目开始主持人报个日期是靠谱的），这读滕王阁序也是一样啊，里面描写的景色跟现实估计也不好比。\n落霞与孤鹜齐飞，秋水共长天一色。渔舟唱晚，响穷彭蠡之滨；雁阵惊寒，声断衡阳之浦。如这般景色你就是请个高手来也PS不出此等画面，这般意境。况且每个人读出的景色画面也都不一样，完全是精神的享受了。说直白点是在意淫了。\n我怀着无比敬仰的心情，就着拼音注释读了三遍，人一下子就机灵了，不困了，真的\n何以解忧，唯有杜康，何以解困，首推此序啊。\n唉，王勃就是死的太早了。\n滕王阁序读后感（三）滕王阁，是中国古代四大名楼之一（另外三座是岳阳楼、黄鹤楼、蓬莱阁）。而如果没有王勃的这篇千古流传的《膝王阁序》，滕王阁的盛名自然会削减不少。王勃乃“初唐四杰”之一，少年时期便有“神童”之名，其才情在这风华绝代的《膝王阁序》里得到了充分的展现：用词华美瑰丽，用典琳琅满目，行文气势磅礴、收放白如，既歌咏了滕王阁的雄伟壮观、宾主的才华横溢以及滕王阁周围的绝妙胜景，也抒发了自己怀才不遇、愤慈悲凉而又不甘沉沦的屈湘。《滕王阁序》由此奠定了其在中国古代文学史上不朽的地位。\n就是这样一位满腹珠矶的才子，却得不到重用，郁郁寡欢，心情烦闷两个月后，王勃渡海去探望父亲时，不幸溺水身亡。\n王勃的悲剧并不是很个别的现象，自古以来文人就常常成为统治者的工具，甚至仅仅是摆设。即如李白，已经达到了诗歌创作的顶峰，(www.lz13.cn)亦不过在皇帝的赏识下进官当一个御用文人，一个招之即来、抨之即去的“宠物”李白很不满，但又有什么办法呢？除了写下类似‘’仰天大笑出门去，我辈岂是蓬篙人“的诗句来一抒豪情，他并没有什么办法摆脱这种屈辱的地位。\n但几千年来中国文人的悲剧命运仅仅在于统治者吗？显然不是。文人往往自命不凡，的确，在知识普及程度很低的古代，一个饱读诗书的文人是可以在精神层面上俯视芸芸众生的。但这种精神上的距离，在实际生活中也使知识分子与人民大众有了隔膜像白居易那样写诗要让老婆婆也能明白的文人实属风毛麟角。脱离群众的后果是什么呢？上不能为统治者所用，下不屑与劳苦大众为伍。多少文人就这样落得个孤家寡人，潦倒而终。\n还有，文人往往受”学而优则仕“的影响，把做官作为人生理想。殊不知，官场险恶，风云莫测。电视连续剧《铁齿铜牙纪晓岚》中，和坤有一段经典台词：”……纪先生您在文海遨游，而我却在宦海打滚儿。文海偶而有点小风小浪，宦海却永远是血雨腥风。“看，文坛与官场就有这样的差别！有些文人学会了政治权谋，摇身一变成为政治家，如王安石；另一些文人”保持本色“，除了几根硬骨头和一肚皮学问外，别无长处，于是官是做不下去的，如陶渊明。可悲的是，文人对统治者，在野则口诛笔伐，对官场黑暗也深恶痛绝，但朝廷一开始吸纳文人，绝大多数文人又趋之若鹜。明末张献忠举兵人蜀，长刀一挥，血流成河。可张献忠建立的”大西“政权一宣布”开科取士“，立刻有8000多名文人从四面八方赶到成都试图谋取功名，孰料全变成了张军的刀下之鬼。\n千年一叹―中国文人的命运！\n","categories":["哲学思考"],"tags":["滕王阁","王勃","滕王阁序"]},{"title":"Spring、rpc、orm常用框架总结","url":"/2021_10_21_spring_rpc_orm/","content":"本文主要介绍 Java 中常用的应用框架，重点讲解如下三部分内容。\n\nSpring 框架中的主要知识点；\nNIO 框架 Netty 以及基于 Netty 实现的主流 RPC 框架 Motan、Dubbo 和 gRPC；\nORM 框架 MyBatis。\n\n常用框架汇总先来看常用框架的知识点汇总，如下图所示。\n\n\n如上图所示，左上方是 Spring 系列。很多研发人员把 Spring 看作心目中最好的 Java 项目，没有之一。Spring 系列包含非常多的项目，可以满足 Java 开发中的方方面面。那么来看几个常用的 Spring 框架。\nSpringSpring Framework，也就是我们常说的 Spring 框架，包括了 IoC 依赖注入，Context 上下文、 Bean 管理、SpringMVC 等众多功能模块，其他 Spring 项目比如 Spring Boot 也会依赖 Spring 框架。\nSpring Boot 的目标是简化 Spring 应用和服务的创建、开发与部署，简化了配置文件，使用嵌入式 Web 服务器，含有诸多开箱即用的微服务功能，可以和 Spring Cloud 联合部署。Spring Boot 的核心思想是约定大于配置，应用只需要很少的配置即可，简化了应用开发模式。\nSpring Data 是一个数据访问及操作的工具集，封装了多种数据源的操作能力，包括：JDBC、Redis、MongoDB 等。\nSpring Cloud 是一套完整的微服务解决方案，是一系列不同功能的微服务框架的集合。Spring Cloud 基于 Spring Boot，简化了分布式系统的开发，集成了服务发现、配置管理、消息总线、负载均衡、断路器、数据监控等各种服务治理能力。比如sleuth提供了全链路追踪能力，Netflix套件提供了hystrix熔断器、zuul网关等众多的治理组件。config 组件提供了动态配置能力，bus组件支持使用 RabbitMQ、Kafka、ActiveMQ 等消息队列，实现分布式服务之间的事件通信。\nSpring Security 用于快速构建安全的应用程序和服务，在 Spring Boot 和 Spring Security OAuth2 的基础上，可以快速实现常见安全模型，如单点登录，令牌中继和令牌交换。这里可以了解一下 OAuth2 授权机制和 JWT 认证方式。OAuth2 是一种授权机制，规定了完备的授权、认证流程。JWT 全称是 JSON Web Token，是一种把认证信息包含在 token 中的认证实现，OAuth2 授权机制中就可以应用 JWT 来作为认证的具体实现方法。\nStrutsStruts 是曾经非常火爆的 Web 组合 SSH 中的控制层。我们知道 Web 服务一般都采用 MVC 分层模型构建，就是 Model 层负责内部数据模型，Controller 负责请求的分发控制，View 层负责返回给用户展示的视图。Struts 实现的就是其中控制层的角色。\nStruts 采用 Filter 实现，针对类进行拦截，每次请求就会创建一个 Action。不过使用 Struts 的 SSH 组合已经逐渐被使用 SpringMVC 的 SSM 组合代替，也就是 SpringMVC+Spring+MyBatis的组合，一方面原因是由于 Struts 对几次安全漏洞的处理，让大家对 Struts 的信心受到影响；另一方面，SpringMVC 更加灵活，不需要额外配置，不存在和 Spring 整合等问题，使用更加方便。所以建议以 SSM 框架的学习为主。\nORMORM 就是对象关系匹配，解决面向对象与关系数据库存在的互不匹配的问题。简单来说，就是把关系数据库中的数据转换成面向对象程序中的对象。常用的 ORM 框架有 Hibernate 和 MyBatis，也就是 SSH 组合和 SSM 组合中的 H 与 M。\n来看一下 Hibernate 和 MyBatis 的特点和区别。\n\nHibernate 对数据库结构提供了完整的封装，实现了 POJO 对象与数据库表之间的映射，能够自动生成并执行 SQL 语句。只要定义了 POJO 到数据库表的映射关系，就可以通过 Hibernate 提供的方法完成数据库操作。Hibernate 符合 JPA 规范，就是 Java 持久层 API。\nMyBatis 通过映射配置文件，将 SQL 所需的参数和返回的结果字段映射到指定对象，MyBatis 不会自动生成 SQL，需要自己定义 SQL 语句，不过更方便对 SQL 语句进行优化。\n\n总结起来，Hibernate 配置要比 MyBatis 复杂的多，学习成本也比 MyBatis 高。MyBatis，简单、高效、灵活，但是需要自己维护 SQL；Hibernate 功能强大、全自动、适配不同数据库，但是非常复杂，灵活性稍差。\nNettyNetty 是一个高性能的异步事件驱动的网络通信框架，Netty 对 JDK 原生 NIO 进行封装，简化了网络服务的开发。\n另外，同类型的框架还有 MINA、Grizzly，不过目前使用得相对较少，一般不会在面试题目中出现，可以作为兴趣简单了解。\nRPCRPC 服务，Motan、Dubbo、gRPC 都是比较常用的高性能 RPC 框架，可以提供完善的服务治理能力，Java 版本的通信层都是基于前面提到的 Netty 实现。\n其他框架此外，Jersy 和 RESTEasy 都是可以快速开发 RESTful 服务的框架。与 SpringMVC 相比，这两个框架都是基于 JAX-RS 标准，而 SpringMVC 基于 Servlet，使用自己构建的 API，是两个不同的标准。\nShiro 框架是一个与 Spring Security 类似的开源的权限管理框架，用于访问授权、认证、加密及会话管理。能够支持单机与分布式 session 管理。相比 Security，Shiro更加简单易用\n详解 Spring 框架对于 Spring 框架，讲解中涉及的流程与实现默认都是基于最新的 5.x 版本。先来看 Spring 中的几个重要概念。\nIoCIoC，也就是控制反转，如下图，拿公司招聘岗位来举例。假设一个公司有产品、研发、测试等岗位。如果是公司根据岗位要求，逐个安排人选，如图中向下的箭头，这是正向流程。如果反过来，不用公司来安排候选人，而是由第三方猎头来匹配岗位和候选人，然后进行推荐，如图中向上的箭头，这就是控制反转。\n\n\n在 Spring 中，对象的属性是由对象自己创建的，就是正向流程；如果属性不是对象创建，而是由 Spring 来自动进行装配，就是控制反转。这里的 DI 也就是依赖注入，就是实现控制反转的方式。正向流程导致了对象于对象之间的高耦合，IoC 可以解决对象耦合的问题，有利于功能的复用，能够使程序的结构变得非常灵活\nContext 和 BeanSpring 进行 IoC 实现时使用的两个概念：Context 上下文和 Bean。如下图所示，所有被 Spring 管理的、由 Spring 创建的、用于依赖注入的对象，就叫作一个 Bean。Spring 创建并完成依赖注入后，所有 Bean 统一放在一个叫作 Context 的上下文中进行管理。\n\n\nAOPAOP，就是面向切面编程。如下图所示，一般我们的程序执行流程是从 Controller 层调用 Service 层、然后 Service 层调用 DAO 层访问数据，最后在逐层返回结果。这个是图中向下箭头所示的按程序执行顺序的纵向处理。\n\n\n但是，我们思考这样一个问题，一个系统中会有多个不同的服务，例如用户服务、商品信息服务等等，每个服务的Controller层都需要验证参数，都需要处理异常，如果按照图中红色的部分，对不同服务的纵向处理流程进行横切，在每个切面上完成通用的功能，例如身份认证、验证参数、处理异常等等、这样就不用在每个服务中都写相同的逻辑了，这就是 AOP 思想解决的问题。AOP 以功能进行划分，对服务顺序执行流程中的不同位置进行横切，完成各服务共同需要实现的功能\n组件再来看到 Spring 框架，下图中列出了 Spring 框架主要包含的组件。这张图来自 Spring4.x 的文档。目前最新的 5.x 版本中右面的 Portlet 组件已经被废弃掉，同时增加了用于异步响应式处理的 WebFlux 组件。这里不需要对所有的组件都详细了解，只需要重点了解最常用的几个组件实现，以及知道每个组件用来实现哪一类功能就可以了。\n\n\n图中红框框住的是比较重要的组件，Core 组件是 Spring 所有组件的核心；Bean 组件和 Context 组件我刚才提到了，是实现 IoC 和依赖注入的基础；AOP 组件用来实现面向切面编程；Web 组件包括 SpringMVC，是 Web 服务的控制层实现\n动态代理和静态代理接下来是 Spring 中机制和概念相关的知识点，如下图所示。\n\n\nAOP 的实现是通过代理模式，在调用对象的某个方法时，执行插入的切面逻辑。实现的方式有动态代理，也叫运行时增强，比如 JDK 代理、CGLIB；静态代理是在编译时进行织入或类加载时进行织入，比如 AspectJ。关于 AOP 还需要了解一下对应的 Aspect、pointcut、advice 等注解和具体使用方式。\nPlaceHolder 动态替换PlaceHolder 动态替换主要需要了解替换发生的时间，是在 Bean Definition 创建完成后，Bean 初始化之前，是通过 BeanFactoryPostProcessor 接口实现的。主要实现方式有 PropertyPlaceholderConfigurer 和 PropertySourcesPlaceholderConfigurer。这两个类实现逻辑不一样，Spring Boot 使用 PropertySourcesPlaceholderConfigurer 实现。\n事务事务，需要了解 Spring 中对事务规定的隔离类型和事务传播类型。这里要知道事务的隔离级别是由具体的数据库来实现的，在数据库部分会作详细介绍。事务的传播类型，可以重点了解最常用的 REQUIRED 和 SUPPORTS类型\n核心接口&#x2F;类再来看图右上方需要重点掌握的核心类。\n\nApplicationContext 保存了 IoC 的整个应用上下文，可以通过其中的 BeanFactory 获取到任意到 Bean；\nBeanFactory 主要的作用是根据 Bean Definition 来创建具体的 Bean；\nBeanWrapper 是对 Bean 的包装，一般情况下是在 Spring IoC 内部使用，提供了访问 Bean 的属性值、属性编辑器注册、类型转换等功能，方便 IoC 容器用统一的方式来访问 Bean 的属性；\nFactoryBean 通过 getObject 方法返回实际的 Bean 对象，例如 Motan 框架中 referer 对 service 的动态代理就是通过 FactoryBean 来实现的。\n\nScopeBean 的 Scope 是指 Bean 的作用域，默认情况下是单例模式，这也是使用最多的一种方式；多例模式，即每次从 BeanFactory 中获取 Bean 都会创建一个新的 Bean。Request、Session、Global-session 是在 Web 服务中使用的 Scope。\n\nRequest 每次请求都创建一个实例；\nSession 是在一个会话周期内保证只有一个实例；\nGlobal-session 在 5.x 版本中已经不再使用，同时增加了 Application 和 Websocket 两种Scope，分别保证在一个 ServletContext 与一个 WebSocket 中只创建一个实例。\n\n还可以了解一下 Spring 的事件机制，知道 Spring 定义的五种标准事件，了解如何自定义事件和实现对应的 ApplicationListener 来处理自定义事件。\n应用下面来看 Spring 应用相关的知识点，如下图所示。\n\n\n首先要熟练掌握常用注解的使用。\n\n类型类的注解，包括 Controller、Service 等，可以重点了解一下 Component 和 Bean 注解的区别：\n\n@Component 注解在类上使用表明这个类是个组件类，需要 Spring 为这个类创建 Bean\n@Bean 注解使用在方法上，告诉 Spring 这个方法将会返回一个 Bean 对象，需要把返回的对象注册到 Spring 的应用上下文中。\n\n\n设置类注解可以重点了解 @Autowire 和 @Qualifier 以及 byType、byName 等不同的自动装配机制\n\nWeb 类主要以了解为主，关注 @RequestMapping、@GetMapping、@PostMapping 等路径匹配注解，以及 @PathVariable、@RequestParam 等参数获取注解。\n\n功能类的注解，包括 @ImportResource 引用配置、@ComponentScan 注解自动扫描、@Transactional 事务注解等等，这里不一一介绍了。\n\n\n如上图右边所示，Spring 应用部分，还需要了解配置 Spring 的几种方式：XML 文件配置、注解配置和使用 API 进行配置。\n自动装配机制需要了解按类型匹配进行自动装配，按 Bean 名称进行自动装配，构造器中的自动装配和自动检测等主要的四种方式。\n最后还可以了解一下 List、Set、Map 等集合类属性的配置方式以及内部 Bean 的使用。\nContext 初始化流程如下图所示，左侧是三种类型的 Context：\n\nXML 配置方式的 Context；\nSpring Boot 的 Context；\nWeb 服务的 Context。\n\n\n\n\n不论哪种 Context，创建后都会调用到 AbstractApplicationContext 类的 refresh 方法，流程如下。\n\n首先对刷新进行准备，包括设置开始时间、设置激活状态、初始化 Context 环境中的占位符，这个动作根据子类的需求由子类来执行，然后验证是否缺失必要的 properties。\n刷新并获得内部的 Bean Factory\n对 BeanFactory 进行准备工作，比如设置类加载器和后置处理器、配置不进行自动装配的类型、注册默认的环境 Bean。\n为 Context 的子类提供后置处理 BeanFactory 的扩展能力。如果子类想在 Bean 定义加载完成后，开始初始化上下文之前做一些特殊逻辑，可以复写这个方法。\n执行 Context 中注册的 Bean Factory 后缀处理器。这里有两种后置处理器，一种是可以注册 Bean 的后缀处理器，另一种是针对 BeanFactory 进行处理的后置处理器。执行的顺序是，先按优先级执行可注册 Bean 的处理器，在按优先级执行针对 BeanFactory的处理器。对 Spring Boot 来说，这一步会进行注解 Bean Definition 的解析。流程如图右侧所示，由 ConfigurationClassPostProcessor 触发、由 ClassPathBeanDefinitionScanner 解析并注册到 BeanFactory。\n按优先级顺序在 BeanFactory 中注册 Bean的后缀处理器，Bean 后置处理器可以在 Bean 初始化前、后执行处理。\n初始化消息源，消息源用来支持消息的国际化\n初始化应用事件广播器。事件广播器用来向 ApplicationListener 通知各种应用产生的事件，是一个标准的观察者模式\n是留给子类的扩展步骤，用来让特定的 Context 子类初始化其他的 Bean。\n把实现了 ApplicationListener 的 Bean 注册到事件广播器，并对广播器中的早期未广播事件进行通知。\n冻结所有 Bean 描述信息的修改，实例化非延迟加载的单例 Bean。\n完成上下文的刷新工作，调用 LifecycleProcessor 的 onFresh() 方法以及发布 ContextRefreshedEvent 事件\n在 finally 中，执行第十三步，重置公共的缓存，比如 ReflectionUtils 中的缓存、 AnnotationUtils 中的缓存等等；\n\n至此，Spring 的 Context 初始化完成。由于篇幅和时间的关系，这里介绍了最主要的主流程，建议课后阅读源码来复习这个知识点，补全细节。\nBean 生命周期面试中经常问到 Bean 的生命周期，如下图，我们先看绿色的部分，Bean 的创建过程。\n\n\n\n调用 Bean 的构造方法创建 Bean；\n通过反射调用 setter 方法进行属性的依赖注入\n如果实现 BeanNameAware 接口的话，会设置 Bean 的 name；\n如果实现了 BeanFactoryAware，会把 BeanFactory 设置给 Bean\n如果实现了 ApplicationContextAware，会给 Bean 设置 ApplictionContext；\n如果实现了 BeanPostProcessor 接口，则执行前置处理方法；\n实现了 InitializingBean 接口的话，执行 afterPropertiesSet 方法；\n执行自定义的 init 方法；\n执行 BeanPostProcessor 接口的后置处理方法。\n\n以上就完成了 Bean 的创建过程。而在使用完 Bean 需要销毁时，会先执行 DisposableBean 接口的 destroy 方法，然后在执行自定义的 destroy 方法。这部分也建议阅读源码加深理解\n扩展接口在对 Spring 进行定制化功能扩展时，可以选择一些扩展点，如下图所示。\n\n\n\nBeanFactoryPostProcessor 是 BeanFactory 后置处理器，支持在 BeanFactory 标准初始化完成后，对 BeanFactory 进行一些额外处理。讲 Context 初始化流程时介绍过，这时所有的 Bean 的描述信息已经加载完毕，但是还没有进行 Bean 初始化。例如前面提到的 PropertyPlaceholderConfigurer，就是在这个扩展点上对 Bean 属性中的占位符进行替换。\nBeanDefinitionRegistryPostProcessor，它扩展自BeanFactoryPostProcessor，在执行 BeanFactoryPostProcessor 的功能前，提供了可以添加 Bean Definition 的能力，允许在初始化一般 Bean 前，注册额外的 Bean。例如可以在这里根据 Bean 的 Scope 创建一个新的代理 Bean。\nBeanPostProcessor，提供了在 Bean 初始化之前和之后插入自定义逻辑的能力。与 BeanFactoryPostProcessor 的区别是处理的对象不同，BeanFactoryPostProcessor 是对 BeanFactory 进行处理，BeanPostProcessor 是对 Bean 进行处理。\n\n上面这三个扩展点，可以通过实现 Ordered 和PriorityOrdered 接口来指定执行顺序。实现 PriorityOrdered 接口的 processor 会先于实现 Ordered 接口的执行。\n\nApplicationContextAware，可以获得 ApplicationContext 及其中的 Bean，当需要在代码中动态获取 Bean 时，可以通过实现这个接口来实现。\nInitializingBean，可以在 Bean 初始化完成，所有属性设置完成后执行特定逻辑，例如对自动装配对属性进行验证等。\nDisposableBean，用于在 Bean 被销毁前执行特定的逻辑，例如做一些回收工作等。\nApplicationListener，用来监听 Spring 的标准应用事件或者自定义事件。\n\nSpring Boot下面来看 Spring Boot 相关的知识点，如下图所示。\n\n\n首先是 Spring Boot 启动流程的主要步骤：\n\n要配置 Environment。\n准备 Context 上下文，包括执行 ApplicationContext 的后置处理、初始化 Initializer、通知Listener 处理 ContextPrepared 和 ContextLoaded 事件\n执行 refreshContext，也就是前面介绍过的 AbstractApplicationContext 类的 refresh 方法。\n\n然后要知道在 Spring Boot 中有两种上下文，一种是 bootstrap, 另外一种是 application。其中，bootstrap 是应用程序的父上下文，会先于 applicaton 加载。bootstrap 主要用于从额外的资源来加载配置信息，还可以在本地外部配置文件中解密属性。bootstrap 里面的属性会优先加载，默认也不能被本地相同配置覆盖。\n再来看 Spring Boot 的注解。\n需要知道 @SpringBootApplication 包含了 @ComponentScan、@EnableAutoConfiguration、@SpringBootConfiguration 三个注解，而 @SpringBootConfiguration 注解包含了 @Configuration 注解。也就是 Spring Boot 的自动配置功能。@Conditional 注解就是控制自动配置的生效条件的注解，例如 Bean 或 Class 存在、不存在时进行配置，当满足条件时进行配置等。\n最后，了解一下 Spring Boot 的几个特色模块。\n\nStarter 是 Spring Boot 提供的无缝集成功能的一种方式，使用某个功能时开发者不需要关注各种依赖库的处理，不需要具体的配置信息，由 Spring Boot 自动配置进行 Bean的创建。例如需要使用 Web 功能时，只需要在依赖中引入 Spring-boot-starter-web 即可。\nActuator 是用来对应用程序进行监视和管理，通过 RESTful API 请求来监管、审计、收集应用的运行情况。\nDevTools 提供了一系列开发工具的支持，来提高开发效率。例如热部署能力等。\nCLI 就是命令行接口，是一个命令行工具，支持使用 Groovy 脚本，可以快速搭建 Spring 原型项目。\n\n详解 Netty下面我们来看 Netty 相关的知识点，如下图所示。\n\n\n特点如上图左侧所示，首先了解 Netty 的特点。\n\nNetty 是一个高性能的异步事件驱动的 NIO 框架，它对消息的处理采用串行无锁化设计，提供了对 TCP、UDP 和文件传输的支持。\nNetty 内置了多种 encoder 和 decoder 实现来解决 TCP 粘包问题。\nNetty 处理消息时使用了池化的缓冲池 ByteBufs，提高性能。\n结合内存零 copy 机制，减少了对象的创建，降低了 GC 的压力。\n\n主要概念需要掌握 Netty 中的一些对象概念，比如将 Socket 封装成 Channel 对象，在 Channel 读写消息时，使用 ChannelHandler 对消息进行处理，一组 Handler 顺序链接组成 ChannelPipeline 的责任链模式。一个 Channel 产生的所有事件交给一个单线程的 EventLoop 事件处理器来进行串行处理。而 Bootstrap 对象的主要作用是配置整个 Netty 程序串联起各个组件，是一个 Netty 应用的起点。\n零内存复制要了解 Netty 的内存零 copy 技术。包括使用堆外内存来避免在 Socket 读写时缓冲数据在堆外与堆内进行频繁复制；使用 CompositeByteBuf 来减少多个小的 buffer 合并时产生的内存复制；使用 FileRegion 实现文件传输的零拷贝等。\n粘包与半包要了解 TCP 协议下粘包与半包等产生原因，知道 Netty 提供的多个 decoder 是用什么方式解决这个问题的。例如 FixedLengthFrameDecoder 用来解决固定大小数据包的粘包问题、LineBasedFrameDecoder 适合对文本进行按行分包、DelimiterBasedFrameDecoder 适合按特殊字符作为分包标记的场景、LengthFieldBasedFrameDecoder 可以支持复杂的自定义协议分包等等。\nNetty3 和 Netty4简单了解一下 Netty3 和 Netty4 的区别，其中主要的就是两个版本的线程处理模型完全不同， Netty4 处理得更加优雅。其他的以 Netty4 的特点为主即可。\n线程模型Netty 线程模型采用“服务端监听线程”和“IO 线程”分离的方式，如下图，左侧 Boss 线程组负责监听事件，创建 Socket 并绑定到 Worker 线程组。\n\n\n\nWorker 线程组负责 IO 处理。线程组由 EventLoopGroup 实现，其中包含了多个 EventLoop 事件处理器，每个 EventLoop 包含一个处理线程。通常情况下在 NIO 非阻塞模式下，Netty 为每个 Channel 分配一个 EventLoop，并且它的整个生命周期中的事件都由这个 EventLoop 来处理。一个 EventLoop 可以绑定多个 Channel。\n如上图右侧所示，EventLoop 的处理模型，Netty4 中 Channel 的读写事件都是由 Worker 线程来处理。请求处理中最主要的就是 ChannelPipeline，其中包含了一组 ChannelHandler。这些 Handler 组成了责任链模式，依次对 Channel 中的消息进行处理。一般接收消息时，Pipeline 处理完成会把消息提交到业务线程池进行处理，当业务线程处理完成时，会封装成 Task，提交回 Channel 对应的 EventLoop 来写回返回值。\n详解 RPCRPC 是远程过程调用的简写，RPC 与 HTTP 一样都可以实现远程服务的调用，但是使用方式上有很大的区别。它能够像使用本地方法一样调用远程的方法。\n交互流程如下图所示，来看 RPC 的交互流程。图中绿色的模块是 RPC 中最主要的三个角色。左边的是 Client 端，就是请求的发起方，也可以叫作 Consumer 或者 Referer。右边的模块是 Server 端，就是提供服务实现的一方，也叫作 Provider。\n\n\n为了保持较高的性能，Client 端一般都是直接请求远端的 Server 节点。因此，RPC 框架需要自动的服务注册与发现的能力，上方的绿色的注册中心就是用来动态维护可用服务节点信息的模块。\n图中的箭头代表交互流程。当 Server 提供服务时，向注册中心注册服务信息，告诉注册中心可以提供哪些服务。同时与注册中心保持心跳或者维持长链接，来维持 Server 可用状态，具体方式与注册中心的实现有关，例如 ZK 使用长链接推送方式而 Consul 使用心跳方式。\n如上图所示，当 Client 需要使用服务时，会先向注册中心订阅服务，获得可用的 Server 节点，并保存在 Client 本地。当 Server 节点发生变更时会通知 Client 更新本地 Server 节点信息。Client 按某种负载均衡策略直接请求 Server 使用服务。注意：注册中心只参与服务节点的注册与变更通知，并不会参与具体请求的处理。\n另外一般的 RPC 框架都提供了完整的服务治理能力，因此会有额外的管理模块和信息采集模块来监控、管理服务。如图中灰色的模块所示。\n开源框架来看三款比较有特色的主流 RPC 框架，如下图所示。\n\n\nDubbo 是阿里开源的 RPC 框架，提供完善的服务治理能力，可以快速为 Java 服务提供 RPC 能力。Dubbo 提供了随机、轮询、最少调用优先等多种负载均衡策略，提供对 ZK 等多种注册中心等支持，能够自动完成服务的注册与发现。Dubbo 提供可视化的管理后台，方便对服务状态进行监控和管理。Dubbo 的数据通信默认使用我 Netty 来实现，拥有非常不错的性能。\n微博开源的轻量级服务治理框架 Motan。Motan 的特点是轻量级，提供强大灵活的扩展能力，Motan 提供了多语言支持，目前支持 Java、PHP、Lua、Golang 等多语言交互，目前 Python 和 C++ 的客户端也在研发中。Motan 通过 Agent 代理方式，实现了的跨语言 ServiceMesh 的支持。ServiceMesh 被誉为下一代微服务，在课时 10 还会重点介绍。Motan Java 版本的通信层也是通过 Netty 来实现的，基于 TCP 的私有协议进行通信\nGoogle 开源的 gRPC。gRPC 默认使用 Protobuf 进行消息序列化，非常适合多语言服务之间进行交互。虽然 gRPC 本身支持的服务治理能力并不强，但拥有非常灵活的插件扩展能力，可以方便的实现自定义的服务治理能力。gRPC 基于 HTTP2 协议，能够支持链接复用，并且提供了流式调用能力，也支持从服务端进行推送消息的能力。\n详解 MyBatis特点下面我们来看 ORM 框架 MyBatis，它的知识结构图如下所示。首先要了解它的特点，可以和 Hibernate 来对比进行理解。\n\n\nMyBatis 的优点：\n\nMyBatis 是原生SQL，不像 Hibernate 的 HQL 需要额外的学习成本；\nMyBatis 的 SQL 语句与代码进行了解耦合，这与 Hibernate 是一致的；\nMyBatis 功能简单，学习成本比较低，使用的门槛也非常低，可以快速上手；\nMyBatis SQL调优比较灵活，而 Hibernate，SQL 语句是自动生成的，当有复杂语句需要进行优化时就比较难处理。\n\nMyBatis 的缺点：\n\n相比 Hibernate 这样的全自动 ORM 框架，不能自动生成 SQL 语句，编写 SQL 的工作量比较大，尤其是字段多、关联表多的情况下\n另外一个缺点就是 SQL 语句依赖于具体数据库，导致数据库迁移性差，而 Hibernate 则拥有良好的数据库可移植性。\n\n缓存MyBatis 提供了两级缓存。MyBatis 的一级缓存的存储作用域是 Session，会对同一个 Session 中执行语句的结果进行缓存，来提高再次执行时的效率。MyBatis 内部通过 HashMap 实现存储缓存，一级缓存是默认开启的。\nMyBatis 的二级缓存的作用域是一个 Mapper 的 namespace，在同一个 namespace 中查询 SQL 时可以从缓存中获取数据。二级缓存能够跨 SqlSession 生效，并且可自定义存储源，比如 Ehcache。MyBatis 的二级缓存可以设置剔除策略、刷新间隔、缓存数量等参数来进行优化。\n应用\nMyBatis 提供 #{} 的变量占位符，来支持 SQL 预编译，防止 SQL 注入。\n获取自增主键的 id 可以通过 keyProperty 配置和使用 selectKey 两种方式来实现。\n要记住动态 SQL 常用的几个标签，例如 foreach、where、if、choose、trim 等等。\n\n主要对象需要理解 MyBatis 的主要对象有哪些，它们的作用是什么，举例如下。\n\nSqlSessionFactory 是用来创建 SqlSession 的工厂类，一个 SqlSessionFactory 对应配置文件中的一个环境，也就是一个数据库配置。\n对数据库的操作必须在 SqlSession 中进行，SqlSession 非线程安全，每一次操作完数据库后都要调用 Close 对其进行关闭。\nSqlSession 通过内部的 Executor 来执行增删改查操作。\nStatementHandler 用来处理 SQL 语句预编译，设置参数等。\nParameterHandler 用来设置预编译参数。\nResultSetHandler 用来处理结果集。\nTypeHandler 进行数据库类型和 JavaBean 类型的互相映射。\n\n插件机制MyBatis 的插件机制是通过拦截器组成责任链来对 Executor、StatementHandler、ParameterHandler、ResultSetHandler 这四个作用点进行定制化处理。另外可以了解一下基于插件机制实现的 PageHelper 分页插件\n处理流程如下图所示，MyBatis 的处理流程。\n\n\n在执行 SQL 时，首先会从 SqlSessionFactory 中创建一个新的 SqlSession。\nSQL 语句是通过 SqlSession 中的 Executor 来执行，Executor 根据 SqlSession 传递的参数执行 query() 方法，然后创建一个 StatementHandler 对象，将必要的参数传递给 StatementHandler，由 StatementHandler 来完成对数据库的查询。\nStatementHandler 调用 ParameterHandler 的 setParameters 方法，把用户传递的参数转换成 JDBC Statement 所需要的参数， 调用原生 JDBC 来执行语句。\n最后由 ResultSetHandler 的 handleResultSets 方法将 JDBC 返回的 ResultSet 结果集转换成对象集，并逐级返回结果，完成一次 SQL 语句执行。\n考察点下面是需要注意的面试考察点。\n\n首先要掌握 Spring 的核心概念 IoC、AOP 以及具体的实现方式。\n要重点掌握 SpringContext 的初始化流程、Bean 的生命周期。\n以应用为主，了解常用注解的作用和使用方式。\n要了解一下 Spring Boot 相关的知识点，目前使用 Spring Boot 的项目越来越多，建议根据前面列出的知识点来学习。\n要理解 Netty 的线程模型和消息处理的pipeline机制。\n要理解 RPC 的交互流程及常用 RPC 框架的特点。\n要了解 MyBatis 或者 Hibernate 这样的 ORM 框架解决了什么问题，了解框架的实现原理。\n\n这一节课的内容比较多，前面提到的核心机制、核心流程，建议阅读源码加深理解。提供一个小技巧，在学习时可以通过断点调试的方式，结合给出的流程图来阅读源码。\n加分项\n本课时涉及考察点大多是以应用能力为主的，但是如果你阅读过源码，能突出对底层实现细节的掌握能力，一定会另面试官刮目相看。\n除了应用之外，最好能理解框架的理念，例如理解 Spring 的控制反转与 AOP 思想。\n能够知道框架最新版本的实现和发展方向，保持对新技术的兴趣和敏感。例如了解 Spring 的 Web Flux 响应式编程的实现与应用，关注 Spring Cloud 的应用等等。\n如果能在应用的基础上有调优的经验，会让你在面试时更加突出。例如你有 Netty 的调优经验，知道要尽量减少 IO 线程占用，把可以后置的处理放到业务线程池中进行。\n\n真题汇总最后汇总一些相关的面试真题作为参考，以及需要注意的地方，如下所示。\n\n\n第 1 题，除了说出 SSH 框架是 Struct+Spring+Hibernate，SSM 是指的 Spring MVC+Spring+MyBatis，另外要重点说一下 SpringMVC 和 Struts 的区别，以及 MyBatis 和 Hibernate 的区别。\n第 4 题，要答出是通过 BeanFactoryPostProcessor 后置处理器进行的占位符替换，如果自定义处理，可以扩展 PropertyPlaceholderConfigurer 或 PropertySourcesPlaceholderConfigurer 来实现。\n第 5 题，大致可以分为：从 HandlerMapping 中查找 Handler、执行 Handler、执行完成给适配器返回 ModelAndView、视图解析、返回视图，这些步骤。建议通过调试来阅读源码，补充细节、增加理解\n第 6 题，可以从构造器循环依赖和 setter 循环依赖两部分来回答，构造器循环通过使用创建中 Bean 标示池，来判断是否产生了循环创建；setter 循环依赖通过引入 ObjectFactory 来解决。\n\n\n第 7 题，题目给出的就是执行顺序。\n第 8 题，可以从 Channel、Socket、EventLoop、ChannelPipeline 等对象展开介绍。\n第 9 题，可以从下面几方面回答：\n\n 使用方式，HTTP 使用 Client 方式进行远程调用，RPC 使用动态代理的方式实现远程调用；\n 请求模型，HTTP 一般会经过 DNS 解析、4−7 层代理等中间环节，而 RPC 一般是点对点直连；\n 服务治理能力，RPC 提供更加丰富的服务治理功能，例如熔断、负载均衡等；\n 语言友好性，HTTP 对跨语言服务之间交互更加友好。\n","categories":["总结笔记"],"tags":["spring","sringboot","mybatis","orm","rpc","netty"]},{"title":"消息队列与数据库总结","url":"/2021_10_23_msg_mysql/","content":"本文主要讲解消息队列与数据库相关的知识，重点讲解三部分知识点：\n\n Kafka 的架构与消息交互流程；\n 数据库事务的 4 大特性和分类；\n MySQL 相关的内容，比如索引、MySQL 调优等。\n\n\n消息队列与数据库知识点先来看看相关知识点汇总，如下图。首先为了防止歧义进行说明，本课时中提到的“队列“就是指“消息队列“。\n\n\n消息队列来看消息队列的应用场景，也就是队列能解决哪些问题。\n\n 队列可以对应用进行解耦合，应用之间不用直接调用。\n 可以通过队列来传递消息，完成通信。\n 队列也可以用来执行异步任务，任务提交方无需等待结果。\n 队列的另一个作用是削峰填谷，在突发流量时，可以通过队列做缓冲，不会对后端服务产生较大压力，当峰值过去时，可以逐渐消费堆积的数据，填平流量低谷。\n 消息队列一般还提供了一写多读的能力，可以用来做消息的多播与广播。\n\n\n关于队列还需要知道两个主要的消息协议。\n\n JMS 是 Java 的消息服务，规定了 Java 使用消息服务的 API，在前面 Spring 的课时提到过，Spring 提供了支持 JMS 的组件。\n AMQP 是高级消息队列协议，是应用层协议的一个开放标准，AMQP 不从 API 层进行限定，而是直接定义网络交换的数据格式，因此支持跨语言的能力，例如 RabbitMQ 就使用了 AMQP 实现。\n\n\n再来对比几个常用的消息队列。\n\n RabbitMQ\n\n使用 Erlang 开发的开源消息队列，通过 Erlang 的 Actor 模型实现了数据的稳定可靠传输。支持 AMQP、XMPP、SMTP 等多种协议，因此也比较重量级。由于采用 Broker 代理架构，发送给客户端时先在中心队列排队，疑似 RabbitMQ的单机吞吐量在万级，不算很高。\n\n ActiveMQ\n\n可以部署于代理模式和 P2P 模式，支持多种协议，单机吞吐量在万级，但是 ActiveMQ 不够轻巧，对于队列较多的情况支持不是很好。并且有较低概率丢失消息。\n\n RocketMQ\n\n阿里开源的消息中间件，单机能够支持 10w 级的吞吐量，使用 Java 开发，具有高吞吐量、高可用性的特点、适合在大规模分布式系统中应用。\n\n Kafka\n\n\n\n由 Scala 开发的高性能跨语言分布式消息队列，单机吞吐量可以到达 10w 级，消息延迟在 ms 级。Kafka 是完全的分布式系统，Broker、Producer、Consumer 都原生自动支持分布式，依赖于 ZooKeeper 做分布式协调。Kafka 支持一写多读，消息可以被多个客户端消费，消息有可能会重复，但是不会丢失。本课时后面会对 Kafka 的架构进行详细介绍。\n数据库中间件数据库中间件一般提供了读写分离、数据库水平扩展的能力。下面主要介绍两个中间件。\n一是 Sharding-Sphere，它是一个开源的分布式数据库中间件解决方案，由 Sharding-JDBC、Sharding-Proxy、Sharding-Sidecar 这几个独立产品组成，适用不同使用场景。这几个产品都提供标准化的数据分片、读写分离、柔性事务和数据治理功能，可适用于如 Java 同构、异构语言、容器、云原生等各种多样化的应用场景。 目前 Sharding-Sphere 已经进入 Apache 孵化，发展速度很快，可以重点关注。\n二是 Mycat，它也提供了分库分表等能力，Mycat 基于 Proxy 代理模式，后端可以支持 MySQL、Oracle、DB2 等不同数据库实现，不过代理方式对性能会有一定影响\n其他还有一些数据库中间件例如 Vitess 等，使用不算广泛，了解即可\n数据库对于数据库相关知识点，首先需要知道不同类型的数据库。\n关系型数据库常用的关系型数据库主要是 Oracle 和 MySQL。Oracle 功能强大，主要缺点就是贵。MySQL 是互联网行业中最流行的数据库，这不仅仅是因为 MySQL 免费，可以说关系数据库场景中你需要的功能，MySQL 都能很好得满足。后面的详解部分会详细介绍 MySQL 的一些知识点。\nMariaDB 是 MySQL 的分支，由开源社区维护，MariaDB 虽然被看作 MySQL 的替代品，但与 MySQL 相比，它在扩展功能、存储引擎上都有非常好的改进，后续可以关注。\nPostgreSQL也叫 PGSQL，PGSQL 类似于 Oracle 的多进程框架，可以支持高并发的应用场景。PG 几乎支持所有的 SQL 标准，支持类型相当丰富。PG 更加适合严格的企业应用场景，而 MySQL 更适合业务逻辑相对简单、数据可靠性要求较低的互联网场景。\nNoSQLNoSQL，就是 Not only SQL，一般指非关系型数据库。\nRedis 就是非关系型数据库，它提供了持久化能力，支持多种数据类型。Redis 适用于数据变化快且数据大小可预测的场景。\nMongoDB 是一个基于分布式文件存储的数据库，将数据存储为一个文档，数据结构由键值对组成。MongoDB 比较适合表结构不明确，且数据结构可能不断变化的场景，不适合有事务和复杂查询的场景。\nHBase 是建立在 HDFS，也就是 Hadoop 文件系统之上的分布式面向列的数据库，类似于谷歌的大表设计，HBase 可以快速随机访问海量结构化数据。在表中它由行排序，一个表有多个列族以及每一个列族可以有任意数量的列。 HBase 依赖 HDFS 可以实现海量数据的可靠存储，适用于数据量大，写多读少，不需要复杂查询的场景。\nCassandra 是一个高可靠的大规模分布式存储系统。支持分布式的结构化 key-value 存储，以高可用性为主要目标。适合写多的场景，适合做一些简单查询，不适合用来做数据分析统计。\nPika 是一个可持久化的大容量类 Redis 存储服务， 兼容五种主要数据结构的大部分命令。Pika 使用磁盘存储，主要解决 Redis 大容量存储的成本问题。\nNewSQLNewSQL 数据库也越来越被大家关注，NewSQL 是指新一代关系型数据库。比较典型的有TiDB。\nTiDB 是开源的分布式关系数据库，几乎完全兼容 MySQL，能够支持水平弹性扩展、ACID 事务、标准 SQL、MySQL 语法和 MySQL 协议，具有数据强一致的高可用特性。既适合在线事务处理，也适合在线分析处理。\n另外一个比较著名的 NewSQL 是蚂蚁金服的 OceanBase。OB 是可以满足金融级的可靠性和数据一致性要求的数据库系统。需要使用事务，并且数据量比较大的时候，就比较适合使用 OB。不过目前 OB 已经商业化，不再开源。\n最后来看数据库的范式。目前关系数据库有六种范式：第一范式、第二范式、第三范式、巴斯-科德范式（BCNF）、第四范式和第五范式。范式级别越高对数据表的要求越严格。\n\n 要求最低的第一范式只要求表中字段不可用在拆分。\n 第二范式在第一范式的基础上要求每条记录由主键唯一区分，记录中所有属性都依赖于主键。\n 第三范式在第二范式的基础上，要求所有属性必须直接依赖主键，不允许间接依赖。\n\n \n 一般说来，数据库只需满足第三范式就可以了。\n详解 Kafka架构下面来学习 Kafka 的架构。先结合如下的架构图来了解 Kafka 中的几个概念。\n\n\n首先 Kafka 消息队列由三个角色组成，左面的是消息的生产方 Producer；中间是 Kafka 集群， Kafka 集群由多台 Kafka server 组成，每个 Server 称为一个 Broker，也就是消息代理；右面的是消息的消费方 Consumer。\nKafka 中消息是按照 Topic 进行划分的，一个 Topic 就是一个 Queue。在实际应用中，不同业务数据就可以设置为不同的 Topic。一个 Topic 可以有多个消费方，当生产方在某个 Topic 发出一条消息后，所有订阅了这个 Topic 的消费方都可以收到这条消息。\n为了提高并行能力，Kafka 为每个 Topic 维护了多个 Partition 分区，每个分区可以看作一份追加类型的日志。 每个分区中的消息保证 ID 唯一且有序，新消息不断追加到尾部。Partition 实际存储数据时，会对按大小进行分段（Segment），来保证总是对较小的文件进行写操作，提高性能，方便管理。\n如图中间部分，Partition 分布于多个 Broker 上。图中绿色的模块表示 Topic1 被分为了 3 个 Partition。每个 Partition 会被复制多份存在于不同的 Broker 上，如图中红色的模块，这样可以保证主分区出现问题时进行容灾。每个 Broker 可以保存多个 Topic 的多个 Partition。\nKafka 只保证一个分区内的消息有序，不能保证一个 Topic 的不同分区之间的消息有序。为了保证较高的处理效率，所有的消息读写都是在主 Partition 中进行，其他副本分区只会从主分区复制数据。Kafka 会在 ZooKeeper 上针对每个 Topic 维护一个称为 ISR（in-sync replica），就是已同步的副本集。如果某个主分区不可用了，Kafka 就会从 ISR 集合中选择一个副本作为新的主分区。\n消息发布&#x2F;消费流程Kafka 通过对消费方进行分组管理来支持消息一写多读，流程如下图所示。\n\n\n来看图中的例子，这个 Topic 分为 4 个 Partition，就是图中绿色的 P1到 P4，上部的生产方根据规则选择一个 Partition 进行写入，默认规则是轮询策略。也可以由生产方指定 Partition 或者指定 key 来根据 Hash 值选择 Partition。\n消息的发送有三种方式：同步、异步以及 oneway。\n\n 同步模式下后台线程中发送消息时同步获取结果，这也是默认模式。\n 异步的模式允许生产者批量发送数据，可以极大的提高性能，但是会增加丢失数据的风险。\n oneway 模式只发送消息不需要返回发送结果，消息可靠性最低，但是低延迟、高吞吐，适用于对可靠性要求不高的场景。\n\n\n\n来看消息的消费，Consumer 按照 Group 来消费消息，Topic 中的每一条消息可以被多个 Consumer Group 消费，如上图中的 GroupA 和 GroupB。Kafka 确保每个 Partition 在一个 Group 中只能由一个 Consumer 消费。Kafka 通过 Group Coordinator 来管理 Consumer 实际负责消费哪个 Partition，默认支持 Range 和轮询分配。\nKafka 在 ZK 中保存了每个 Topic 中每个 Partition 在不同 Group 的消费偏移量 offset，通过更新偏移量保证每条消息都被消费\n注意：用多线程来读取消息时，一个线程相当于一个 Consumer 实例。当 Consumer 的数量大于分区的数量的时候，有的 Consumer 线程会读取不到数据。\n详解数据库事务特性数据库的特性是面试时考察频率非常高的题目，来看看数据库的 ACID 四大特性，如下图\n\n\n第一个原子性，指事务由原子的操作序列组成，所有操作要么全部成功，要么全部失败回滚。\n第二个事务的一致性，指事务的执行不能破坏数据库数据的完整性和一致性，一个事务在执行之前和执行之后，数据库都必须处以一致性状态。比如在做多表操作时，多个表要么都是事务后新的值，要么都是事务前的旧值\n第三个事务的隔离性，指多个用户并发访问数据库时，数据库为每个用户执行的事务，不能被其他事务的操作所干扰，多个并发事务之间要相互隔离。事务的隔离级别在后文中介绍。\n第四个事务的持久性，指一个事务一旦提交并执行成功，那么对数据库中数据的改变就是永久性的，即便是在数据库系统遇到故障的情况下也不会丢失提交事务的操作。\n并发问题在介绍数据的隔离级别之前，先看看没有隔离性的情况下数据库会出现哪些并发问题，如下图左侧部分所示。\n\n\n脏读是指在一个事务处理过程里读取了另一个未提交的事务中的数据，例如，账户 A 转帐给 B 500元，B 余额增加后但事务还没有提交完成，此时如果另外的请求中获取的是 B 增加后的余额，这就发生了脏读，因为事务如果失败回滚时，B 的余额就不应该增加。\n不可重复读是指对于数据库中某个数据，一个事务范围内多次查询返回了不同的数据值，这是由于在多次查询之间，有其他事务修改了数据并进行了提交。\n幻读是指一个事务中执行两次完全相同的查询时，第二次查询所返回的结果集跟第一个查询不相同。与不可重复读的区别在于，不可重复读是对同一条记录，两次读取的值不同。而幻读是记录的增加或删除，导致两次相同条件获取的结果记录数不同。\n隔离级别事务的四种隔离级别可以解决上述几种并发问题。如上图右侧内容所示，由上到下，四种隔离级别由低到高。\n第一个隔离级别是读未提交，也就是可以读取到其他事务未提交的内容，这是最低的隔离级别，这个隔离级别下，前面提到的三种并发问题都有可能发生。\n第二个隔离级别是读已提交，就是只能读取到其他事务已经提交的数据。这个隔离级别可以解决脏读问题。\n第三个隔离级别是可重复读，可以保证整个事务过程中，对同数据的多次读取结果是相同的。这个级别可以解决脏读和不可重复读的问题。MySQL 默认的隔离级别就是可重复读。\n最后一个隔离级别是串行化，这是最高的隔离级别，所有事务操作都依次顺序执行。这个级别会导致并发度下降，性能最差。不过这个级别可以解决前面提到的所有并发问题\n事务分类接下来看事务的分类，如下图。\n\n\n第一个是扁平化事务，在扁平事务中，所有的操作都在同一层次，这也是我们平时使用最多的一种事务。它的主要限制是不能提交或者回滚事务的某一部分，要么都成功，要么都回滚。\n为了解决第一种事务的弊端，就有了第二种带保存点的扁平事务。它允许事务在执行过程中回滚到较早的状态，而不是全部回滚。通过在事务中插入保存点，当操作失败后，可以选择回滚到最近的保存点处。\n第三种事务是链事务，可以看做是第二种事务的变种。它在事务提交时，会将必要的上下文隐式传递给下一个事务，当事务失败时就可以回滚到最近的事务。不过，链事务只能回滚到最近的保存点，而带保存点的扁平化事务是可以回滚到任意的保存点。\n第四种事务是嵌套事务，由顶层事务和子事务构成，类似于树的结构。一般顶层事务负责逻辑管理，子事务负责具体的工作，子事务可以提交，但真正提交要等到父事务提交，如果上层事务回滚，那么所有的子事务都会回滚。\n最后一种类型是分布式事务。是指分布式环境中的扁平化事务。\n常用的分布式事务解决方案如上图右侧所示，下面进行简要介绍。\n第一个分布式事务解决方案是 XA 协议，是保证强一致性的刚性事务。实现方式有两段式提交和三段式提交。两段式提交需要有一个事务协调者来保证所有的事务参与者都完成了第一阶段的准备工作。如果协调者收到所有参与者都准备好的消息，就会通知所有的事务执行第二阶段提交。一般场景下两段式提交已经能够很好得解决分布式事务了，然而两阶段在即使只有一个进程发生故障时，也会导致整个系统存在较长时间的阻塞。三段式提交通过增加 pre-commit 阶段来减少前面提到的系统阻塞的时间。三段式提交很少在实际中使用，简单了解就可以了。\n第二个分布式解决方案是 TCC，是满足最终一致性的柔性事务方案。TCC 采用补偿机制，核心思想是对每个操作，都要注册对应的确认和补偿操作。它分为三个阶段：Try 阶段主要对业务系统进行检测及资源预留；Confirm 阶段对业务系统做确认提交；Cancel 阶段是在业务执行错误，执行回滚，释放预留的资源。\n第三种方案是消息一致性方案。基本思路是将本地操作和发送消息放在一个事务中，保证本地操作和消息发送要么都成功要么都失败。下游应用订阅消息，收到消息后执行对应操作。\n第四种方案可以了解一下阿里云中的全局事务服务 GTS，对应的开源版本是 Fescar。Fescar 基于两段式提交进行改良，剥离了分布式事务方案对数据库在协议支持上的要求。使用 Fescar 的前提是分支事务中涉及的资源，必须是支持 ACID 事务的关系型数据库。分支的提交和回滚机制，都依赖于本地事务来保障。 Fescar 的实现目前还存在一些局限，比如事务隔离级别最高支持到读已提交级别。\n详解 MySQL下面来学习互联网行业使用最为广泛的关系型数据库 MySQL，它的知识点结构图如下所示。\n\n\n常用 SQL 语句对于手写常用 SQL 语句，没有什么特殊的技巧，根据所列的语句类型多做一些练习就好。\n数据类型要知道 MySQL 都提供哪些基本的数据类型，不同数据类型占用的空间大小。可以按给出的分类进行记忆，不一一罗列。\n引擎介绍 MySQL 中主要的存储引擎。\n\nMyISAM 是 MySQL 官方提供的存储引擎，其特点是支持全文索引，查询效率比较高，缺点是不支持事务、使用表级锁。\nInnoDB 在 5.5 版本后成为了 MySQL 的默认存储引擎，特点是支持 ACID 事务、支持外键、支持行级锁提高了并发效率。\nTokuDB 是第三方开发的开源存储引擎，有非常快的写速度，支持数据的压缩存储、可以在线添加索引而不影响读写操作。但是因为压缩的原因，TokuDB 非常适合访问频率不高的数据或历史数据归档，不适合大量读取的场景。\n\n锁MySQL 中的锁，上面也提到了，MyIASAM 使用表级锁，InnoDB 使用行级锁。\n\n表锁开销小，加锁快，不会出现死锁；但是锁的粒度大，发生锁冲突的概率高，并发访问效率比较低。\n行级锁开销大，加锁慢，有可能会出现死锁，不过因为锁定粒度最小，发生锁冲突的概率低，并发访问效率比较高。\n共享锁也就是读锁，其他事务可以读，但不能写。MySQL 可以通过 lock in share mode 语句显示使用共享锁。\n排他锁就是写锁，其他事务不能读取，也不能写。对于 UPDATE、DELETE 和 INSERT 语句，InnoDB 会自动给涉及的数据集加排他锁，或者使用 select for update 显示使用排他锁。\n\n存储过程与函数MySQL 的存储过程与函数都可以避免开发人员重复编写相同的 SQL 语句，并且存储过程和函数都是在 MySQL 服务器中执行的，可以减少客户端和服务器端的数据传输\n存储过程能够实现更复杂的功能，而函数一般用来实现针对性比较强的功能，例如特殊策略求和等。存储过程可以执行包括修改表等一系列数据库操作，而用户定义函数不能用于执行修改全局数据库状态的操作。\n存储过程一般是作为一个独立的部分来执行，而函数可以作为查询语句的一个部分来调用。SQL 语句中不能使用存储过程，但可以使用函数。存储过程一般与数据库实现绑定，使用存储过程会降低程序的可移植性，应谨慎使用。\n新特性此外，可以去了解 MySQL8.0 的一些新特性，例如：\n\n默认字符集格式改为了 UTF-8；\n增加了隐藏索引的功能，隐藏后的索引不会被查询优化器使用，可以使用这个特性用于性能调试；\n支持了通用表表达式，使复杂查询中的嵌入表语句更加清晰；\n新增了窗口函数的概念，可以用来实现新的查询方式。\n\n其中，窗口函数与 SUM、COUNT 等集合函数类似，但不会将多行查询结果合并，而是将结果放在多行中，即窗口函数不需要 GROUP BY。\n索引来看 MySQL 的索引，索引可以大幅增加数据库的查询的性能，在实际业务场景中，或多或少都会使用到。但是索引也是有代价的，首先需要额外的磁盘空间来保存索引；其次，对于插入、更新、删除等操作由于更新索引会增加额外的开销，因此索引比较适合用在读多写少的场景\n首先学习 MySQL 索引类型。\n\n唯一索引，就是索引列中的值必须是唯一的，但是允许出现空值。这种索引一般用来保证数据的唯一性，比如保存账户信息的表，每个账户的 ID 必须保证唯一，如果重复插入相同的账户 ID 时 MySQL 返回异常。\n主键索引是一种特殊的唯一索引，但是它不允许出现空值。\n普通索引，与唯一索引不同，它允许索引列中存在相同的值。例如学生的成绩表，各个学科的分数是允许重复的，就可以使用普通索引。\n联合索引，就是由多个列共同组成的索引。一个表中含有多个单列的索引并不是联合索引，联合索引是对多个列字段按顺序共同组成一个索引。应用联合索引时需要注意最左原则，就是 where 查询条件中的字段必须与索引字段从左到右进行匹配。比如，一个用户信息表，用姓名和年龄组成了联合索引，如果查询条件是“姓名等于张三“，那么满足最左原则；如果查询条件是“年龄大于 20“，由于索引中最左的字段是姓名不是年龄，所以不能使用这个索引。\n全文索引，前面提到了，MyISAM 引擎中实现了这个索引，在 5.6 版本后 InnoDB 引擎也支持了全文索引，并且在 5.7.6 版本后支持了中文索引。全文索引只能在 CHAR、VARCHAR、TEXT 类型字段上使用，底层使用倒排索引实现。要注意对于大数据量的表，生成全文索引会非常消耗时间也非常消耗磁盘空间。\n\n然后来看索引的实现。\nB+ 树实现，B+ 树比较适合用作 &gt; 或 &lt; 这样的范围查询，是 MySQL 中最常使用的一种索引实现。\nR-Tree 是一种用于处理多维数据的数据结构，可以对地理数据进行空间索引。不过实际业务场景中使用的比较少。\nHash 是使用散列表来对数据进行索引，Hash 方式不像 B-Tree 那样需要多次查询才能定位到记录，因此 Hash 索引的效率高于 B-Tree，但是不支持范围查找和排序等功能。实际使用的也比较少。\nFullText 就是前面提到的全文索引，是一种记录关键字与对应文档关系的倒排索引\n调优MySQL 的调优也是研发人员需要掌握的一项技能，一般 MySQL 调优有如下图所示的四个纬度。\n\n\n\n第一个纬度是针对数据库设计、表结构设计以及索引设置纬度进行的优化；\n第二个纬度是对我们业务中使用的 SQL 语句进行优化，例如调整 where 查询条件；\n第三个纬度是对 MySQL 服务的配置进行优化，例如对链接数的管理，对索引缓存、查询缓存、排序缓存等各种缓存大小进行优化；\n第四个纬度是对硬件设备和操作系统设置进行优化，例如调整操作系统参数、禁用 swap、增加内存、升级固态硬盘等等。\n\n这四个纬度从优化的成本角度来讲，从左到右优化成本逐渐升高；从优化效果角度来看，从右到左优化的效果更高。\n对于研发人员来说，前两个纬度与业务息息相关，因此需要重点掌握，后两个纬度更适合 DBA 进行深入学习，简单了解就好。\n那么，重点来看前两个纬度，要点如下图所示。\n\n\n先看到图中左边的模块，关于表结构和索引的优化，应该掌握如下原则。\n\n要在设计表结构时，考虑数据库的水平与垂直扩展能力，提前规划好未来1年的数据量、读写量的增长，规划好分库分表方案。比如设计用户信息表，预计1年后用户数据10亿条，写QPS约5000，读QPS30000，可以设计按UID纬度进行散列，分为4个库每个库32张表，单表数据量控制在KW级别。\n要为字段选择合适的数据类型，在保留扩展能力的前提下，优先选用较小的数据结构。例如保存年龄的字段，要使用TINYINT而不要使用INT。\n可以将字段多的表分解成多个表，必要时增加中间表进行关联。假如一张表有40～50个字段显然不是一个好的设计。\n一般来说，设计关系数据库时需要满足第三范式，但为了满足第三范式，我们可能会拆分出多张表。而在进行查询时需要对多张表进行关联查询，有时为了提高查询效率，会降低范式的要求，在表中保存一定的冗余信息，也叫做反范式。但要注意反范式一定要适度。\n要擅用索引，比如为经常作为查询条件的字段创建索引、创建联合索引时要根据最左原则考虑索引的复用能力，不要重复创建索引；要为保证数据不能重复的字段创建唯一索引等等。不过要注意索引对插入、更新等写操作是有代价的，不要滥用索引，比如像性别这样唯一很差的字段就不适合建立索引。\n列字段尽量设置为not null。MySQL难以对使用null的列进行查询优化，允许null会使索引、索引统计和值更加复杂，允许null值的列需要更多的存储空间，还需要MySQL内部进行特殊处理。\n再看到如图右边所示的模块，对SQL语句进行优化的原则\n要找到最需要优化的SQL语句。要么是使用最频繁的语句，要么是优化后提高最明显的语句，可以通过查询 MySQL的慢查询日志来发现需要进行优化的SQL语句；\n要学会利用 MySQL 提供的分析工具。例如使用 Explain 来分析语句的执行计划，看看是否使用了索引，使用了哪个索引，扫描了多少记录，是否使用文件排序等等。或者利用 Profile 命令来分析某个语句执行过程中各个分步的耗时。\n要注意使用查询语句是要避免使用 SELECT *，而是应该指定具体需要获取的字段。原因一是可以避免查询出不需要使用的字段，二是可以避免查询列字段的元信息。\n是尽量使用 prepared statements，一个是它性能更好，另一个是可以防止 SQL 注入。\n要尽量使用索引扫描来进行排序，也就是尽量在有索引的字段上进行排序操作。\n\n考察点\n必须了解消息队列、数据库的基本原理、使用场景以及常用队列、数据库的特点。比如消息队列适用于异步处理和削峰填谷的场景；Kafka 在提供高可用性的前提下实现了 0 消息丢失的高性能分布式队列服务；MySQL 提供了多种引擎可以支持事务型与非事务型的关系对象库服务等等。\n要了解 Kafka 的架构和消息处理流程，明白 Kafka 是如何通过 Partition 来保证并发能力与冗余灾备的；了解消费组是如何保证每个 Consumer 实例不会获取到重复消息的\n要深刻理解数据库事务的 ACID 特性，了解并发事务可能导致的并发问题和不同的数据库隔离级别如何解决这些并发问题。\n要牢牢掌握常用的 MySQL 语句，比如 WHERE 条件查询语句、JOIN 关联语句、ORDER BY 排序语句等等。还要熟悉常用的自带函数，例如 SUM、COUNT 等等。\n了解 MySQL 数据库不同引擎的特点及不同类型的索引实现。比如知道最常使用的 InnoDB 非常擅长事务处理，MyISAM 比较适合非事务的简单查询场景。比如知道 MySQL 的唯一索引、联合索引、全文索引等不同索引类型，以及最常使用等 B+ 树索引实现等等。\n\n加分项如果想要在面试中获得更好的表现，还应该了解下面这些加分项。\n\n要了解新特性，不论是 Kafka 还是 MySQL，都要了解一下新版本特性。例如 MySQL8.0 中提供了窗口函数来支持新的查询方式；支持通用表表达式，使复杂查询中的嵌入表语句更加清晰等等。\n要知道数据库表设计原则，如果有过线上业务数据库的设计经验就更好了，就能够知道如何对容量进行评估，也知道适当分库分表来保证未来服务的可扩展性，这会对面试起到积极的影响。\n最好有过数据库调优经验，例如明明建立了索引的语句，但是查询效率还是很慢，通过 Explain 分析发现表中有多个索引，MySQL 的优化器选用了错误的索引，导致查询效率偏低，然后通过在 SQL 语句中使用 use index 来指定索引解决。\n有过 Kafka 等主流消息队列使用经验，并且知道应该如何在业务场景下进行调优。例如日志推送的场景，对小概率消息丢失可以容忍，可以设置异步发送消息。而对应金融类业务，需要设置同步发送消息，并设置最高的消息可靠性，把 request.required.acks 参数设置为 -1。\n\n真题汇总\n\n\n第 2 题，可以从消息的发送者保证投递到消息队列、消息对象自身的高可用、消费方处理完成后修改 offset 这三个方面来保证消息的可靠性。这个题目可以结合 Kafka 的消息发送同步、异步，消息可靠性配置来回答。\n第 3 题可以从两个方面解决消息重复：一个是通过对消息处理实现幂等，消除消息重复的影响；另一个是使用 Redis 来进行消息去重，避免重复消息的处理。\n第 4 题可以从创建索引、减少关联查询、优化 SQL 查询条件等方面展开。\n第 6 题可以从 MySQL 调优部分讲解的相关原则这个角度来回答。\n\n","categories":["总结笔记"],"tags":["kafka","mysql","mysql调优"]},{"title":"Nginx总结","url":"/2022_04_13_nginx/","content":"Nginx 是开源、高性能、高可靠的 Web 和反向代理服务器，而且支持热部署，几乎可以做到 7 * 24 小时不间断运行，即使运行几个月也不需要重新启动，还能在不间断服务的情况下对软件版本进行热更新。性能是 Nginx 最重要的考量，其占用内存少、并发能力强、能支持高达 5w 个并发连接数，最重要的是， Nginx 是免费的并可以商业化，配置使用也比较简单。\nNginx 特点高并发、高性能；\n模块化架构使得它的扩展性非常好；\n异步非阻塞的事件驱动模型这点和 Node.js 相似；\n相对于其它服务器来说它可以连续几个月甚至更长而不需要重启服务器使得它具有高可靠性；\n热部署、平滑升级；\n完全开源，生态繁荣；\nNginx 作用Nginx 的最重要的几个使用场景：\n静态资源服务，通过本地文件系统提供服务；\n反向代理服务，延伸出包括缓存、负载均衡等；\nAPI 服务， OpenResty ；\n对于前端来说 Node.js 并不陌生， Nginx 和 Node.js 的很多理念类似， HTTP 服务器、事件驱动、异步非阻塞等，且 Nginx 的大部分功能使用 Node.js 也可以实现，但 Nginx 和 Node.js 并不冲突，都有自己擅长的领域。 Nginx 擅长于底层服务器端资源的处理（静态资源处理转发、反向代理，负载均衡等）， Node.js 更擅长上层具体业务逻辑的处理，两者可以完美组合。\n用一张图表示：\n\nNginx 安装这里基于docker安装：\n# 先起个容器，为了拷贝配置文件docker run -p 80:80 --name mynginx  -d nginx# 拷贝配置docker cp mynginx:/etc/nginx/nginx.conf ./conf/nginx.confdocker cp mynginx:/etc/nginx/conf.d ./conf/conf.ddocker cp mynginx:/usr/share/nginx/html ./html# 删除容器docker stop mynginxdocker rm mynginx# 启动容器docker run -p 80:80 --name mynginx -v $(pwd)/conf/nginx.conf:/etc/nginx/nginx.conf:ro -v $(pwd)/conf/conf.d:/etc/nginx/conf.d:ro -v $(pwd)/logs:/var/log/nginx:rw -v $(pwd)/html:/usr/share/nginx/html:rw -d nginx:latest\n验证地址： http://127.0.0.1/\nWelcome to nginx!If you see this page, the nginx web server is successfully installed and working. Further configuration is required.For online documentation and support please refer to nginx.org.Commercial support is available at nginx.com.Thank you for using nginx.\n\nNginx 核心配置\\nginx\\conf\\nginx.conf\n# main段配置信息user  nginx;                        # 运行用户，默认即是nginx，可以不进行设置worker_processes  auto;             # Nginx 进程数，一般设置为和 CPU 核数一样error_log  /var/log/nginx/error.log notice;         # Nginx 的错误日志存放目录pid        /var/run/nginx.pid;                      # Nginx 服务启动时的 pid 存放位置# events段配置信息events &#123;    use epoll;                      # 使用epoll的I/O模型(如果你不知道Nginx该使用哪种轮询方法，会自动选择一个最适合你操作系统的)    worker_connections  1024;       # 每个进程允许最大并发数&#125;# http段配置信息# 配置使用最频繁的部分，代理、缓存、日志定义等绝大多数功能和第三方模块的配置都在这里设置http &#123;    include       /etc/nginx/mime.types;        # 文件扩展名与类型映射表    default_type  application/octet-stream;     # 默认文件类型    # 设置日志模式    log_format  main  &#x27;$remote_addr - $remote_user [$time_local] &quot;$request&quot; &#x27;                      &#x27;$status $body_bytes_sent &quot;$http_referer&quot; &#x27;                      &#x27;&quot;$http_user_agent&quot; &quot;$http_x_forwarded_for&quot;&#x27;;    # Nginx访问日志存放位置    access_log  /var/log/nginx/access.log  main;    sendfile        on;     # 开启高效传输模式    #tcp_nopush     on;     # 减少网络报文段的数量    keepalive_timeout  65;  # 保持连接的时间，也叫超时时间，单位秒    #gzip  on;    include /etc/nginx/conf.d/*.conf;    # 加载子配置项&#125;\n\n\\nginx\\conf\\conf.d\\default.conf\n# server段配置信息server &#123;    listen       80;        # 配置监听的端口    listen  [::]:80;        # 配置监听的端口    server_name  localhost; # 配置的域名    #access_log  /var/log/nginx/host.access.log  main;    # location段配置信息    location / &#123;        root   /usr/share/nginx/html;       # 网站根目录        index  index.html index.htm;        # 默认首页文件        deny 172.168.22.11;                 # 禁止访问的ip地址，可以为all        allow 172.168.33.44;                # 允许访问的ip地址，可以为all    &#125;    #error_page  404              /404.html;    # 默认40x对应的访问页面    # redirect server error pages to the static page /50x.html    #    error_page   500 502 503 504  /50x.html;    # 默认50x对应的访问页面    location = /50x.html &#123;        root   /usr/share/nginx/html;    &#125;&#125;\n\n\nmain 全局配置，对全局生效；\n\nevents 配置影响 Nginx 服务器与用户的网络连接；\n\nhttp 配置代理，缓存，日志定义等绝大多数功能和第三方模块的配置；\n\nserver 配置虚拟主机的相关参数，一个 http 块中可以有多个 server 块；\n\nlocation 用于配置匹配的 uri ；\n\nupstream 配置后端服务器具体地址，负载均衡配置不可或缺的部分；\n\n\n用一张图清晰的展示它的层级结构：\n\n配置文件 main 段核心参数useruser USERNAME [GROUP]user nginx lion; # 用户是nginx;组是lion\n\npid指定运行 Nginx master 主进程的 pid 文件存放路径。\npid /opt/nginx/logs/nginx.pid # master主进程的的pid存放在nginx.pid的文件\n\nworker_rlimit_nofile_number指定 worker 子进程可以打开的最大文件句柄数。\nworker_rlimit_nofile 20480; # 可以理解成每个worker子进程的最大连接数量。\n\nworker_rlimit_core指定 worker 子进程异常终止后的 core 文件，用于记录分析问题。\nworker_rlimit_core 50M; # 存放大小限制working_directory /opt/nginx/tmp; # 存放目录\n\nworker_processes_number指定 Nginx 启动的 worker 子进程数量。\nworker_processes 4; # 指定具体子进程数量worker_processes auto; # 与当前cpu物理核心数一致\n\nworker_cpu_affinity将每个 worker 子进程与我们的 cpu 物理核心绑定。\nworker_cpu_affinity 0001 0010 0100 1000; # 4个物理核心，4个worker子进程\n\n将每个 worker 子进程与特定 CPU 物理核心绑定，优势在于，避免同一个 worker 子进程在不同的 CPU 核心上切换，缓存失效，降低性能。但其并不能真正的避免进程切换。\nworker_priority指定 worker 子进程的 nice 值，以调整运行 Nginx 的优先级，通常设定为负值，以优先调用 Nginx。\nworker_priority -10; # 120-10=110，110就是最终的优先级\n\nLinux 默认进程的优先级值是120，值越小越优先； nice  定范围为 -20 到 +19 。\n[备注] 应用的默认优先级值是120加上 nice 值等于它最终的值，这个值越小，优先级越高。\nworker_shutdown_timeout指定 worker 子进程优雅退出时的超时时间。\nworker_shutdown_timeout 5s;\n\ntimer_resolutionworker 子进程内部使用的计时器精度，调整时间间隔越大，系统调用越少，有利于性能提升；反之，系统调用越多，性能下降。\ntimer_resolution 100ms;\n在 Linux 系统中，用户需要获取计时器时需要向操作系统内核发送请求，有请求就必然会有开销，因此这个间隔越大开销就越小。\ndaemon指定 Nginx 的运行方式，前台还是后台，前台用于调试，后台用于生产。\ndaemon off; # 默认是on，后台运行模式\n\n配置文件 events 段核心参数useNginx 使用何种事件驱动模型。\nuse method; # 不推荐配置它，让nginx自己选择method 可选值为：select、poll、kqueue、epoll、/dev/poll、eventport\n\nworker_connectionsworker 子进程能够处理的最大并发连接数。\nworker_connections 1024 # 每个子进程的最大连接数为1024\n\naccept_mutex是否打开负载均衡互斥锁。\naccept_mutex on # 默认是off关闭的，这里推荐打开\n\nserver_name 指令指定虚拟主机域名。\nserver_name name1 name2 name3# 示例：server_name www.nginx.com;\n\n域名匹配的四种写法：\n精确匹配： server_name www.nginx.com ;\n左侧通配： server_name *.nginx.com ;\n右侧统配： server_name  www.nginx.* ;\n正则匹配： server_name ~^www.nginx.*$ ;\n匹配优先级：「精确匹配 &gt; 左侧通配符匹配 &gt; 右侧通配符匹配 &gt; 正则表达式匹配」\nserver_name 配置实例：\n\n配置本地  DNS 解析 vim &#x2F;etc&#x2F;hosts （ linux 系统）\n\n# 添加如下内容，其中 121.42.11.34 是阿里云服务器IP地址121.42.11.34 www.nginx-test.com121.42.11.34 mail.nginx-test.com121.42.11.34 www.nginx-test.org121.42.11.34 doc.nginx-test.com121.42.11.34 www.nginx-test.cn121.42.11.34 fe.nginx-test.club\n\n[注意] 这里使用的是虚拟域名进行测试，因此需要配置本地 DNS 解析，如果使用阿里云上购买的域名，则需要在阿里云上设置好域名解析。\n\n配置阿里云 Nginx ，vim &#x2F;etc&#x2F;nginx&#x2F;nginx.conf\n\n# 这里只列举了http端中的sever端配置# 左匹配server &#123; listen 80; server_name *.nginx-test.com; root /usr/share/nginx/html/nginx-test/left-match/; location / &#123;  index index.html; &#125;&#125;# 正则匹配server &#123; listen 80; server_name ~^.*\\.nginx-test\\..*$; root /usr/share/nginx/html/nginx-test/reg-match/; location / &#123;  index index.html; &#125;&#125;# 右匹配server &#123; listen 80; server_name www.nginx-test.*; root /usr/share/nginx/html/nginx-test/right-match/; location / &#123;  index index.html; &#125;&#125;# 完全匹配server &#123; listen 80; server_name www.nginx-test.com; root /usr/share/nginx/html/nginx-test/all-match/; location / &#123;  index index.html; &#125;&#125;\n\n\n访问分析\n\n当访问 www.nginx-test.com 时，都可以被匹配上，因此选择优先级最高的“完全匹配”；\n当访问 mail.nginx-test.com 时，会进行“左匹配”；\n当访问 www.nginx-test.org 时，会进行“右匹配”；\n当访问 doc.nginx-test.com 时，会进行“左匹配”；\n当访问 www.nginx-test.cn 时，会进行“右匹配”；\n当访问 fe.nginx-test.club 时，会进行“正则匹配”；\nroot指定静态资源目录位置，它可以写在 http 、 server 、 location 等配置中。\nroot path例如：location /image &#123; root /opt/nginx/static;&#125;当用户访问 www.test.com/image/1.png 时，实际在服务器找的路径是 /opt/nginx/static/image/1.png\n\n[注意] root 会将定义路径与 URI 叠加， alias 则只取定义路径。\nalias它也是指定静态资源目录位置，它只能写在 location 中。\nlocation /image &#123; alias /opt/nginx/static/image/;&#125;当用户访问 www.test.com/image/1.png 时，实际在服务器找的路径是 /opt/nginx/static/image/1.png\n\n[注意] 使用 alias 末尾一定要添加 &#x2F; ，并且它只能位于 location 中。\nlocation配置路径。\nlocation [ = | ~ | ~* | ^~ ] uri &#123; ...&#125;\n\n匹配规则：\n&#x3D; 精确匹配；\n~ 正则匹配，区分大小写；\n~* 正则匹配，不区分大小写；\n^~ 匹配到即停止搜索；\n匹配优先级： &#x3D;  &gt; ^~  &gt;  ~  &gt; ~*  &gt; 不带任何字符。\n实例：\nserver &#123;  listen 80;  server_name www.nginx-test.com;    # 只有当访问 www.nginx-test.com/match_all/ 时才会匹配到/usr/share/nginx/html/match_all/index.html  location = /match_all/ &#123;      root /usr/share/nginx/html      index index.html  &#125;    # 当访问 www.nginx-test.com/1.jpg 等路径时会去 /usr/share/nginx/images/1.jpg 找对应的资源  location ~ \\.(jpeg|jpg|png|svg)$ &#123;   root /usr/share/nginx/images;  &#125;    # 当访问 www.nginx-test.com/bbs/ 时会匹配上 /usr/share/nginx/html/bbs/index.html  location ^~ /bbs/ &#123;   root /usr/share/nginx/html;    index index.html index.htm;  &#125;&#125;\n\nlocation 中的反斜线location /test &#123; ...&#125;location /test/ &#123; ...&#125;\n\n不带 &#x2F; 当访问 www.nginx-test.com/test 时， Nginx 先找是否有 test 目录，如果有则找 test 目录下的 index.html ；如果没有 test 目录， nginx  则会找是否有 test 文件。\n带 &#x2F; 当访问 www.nginx-test.com/test 时， Nginx 先找是否有 test 目录，如果有则找 test 目录下的 index.html ，如果没有它也不会去找是否存在 test 文件。\nreturn停止处理请求，直接返回响应码或重定向到其他 URL ；执行 return 指令后， location 中后续指令将不会被执行。\nreturn code [text];return code URL;return URL;例如：location / &#123; return 404; # 直接返回状态码&#125;location / &#123; return 404 &quot;pages not found&quot;; # 返回状态码 + 一段文本&#125;location / &#123; return 302 /bbs ; # 返回状态码 + 重定向地址&#125;location / &#123; return https://www.baidu.com ; # 返回重定向地址&#125;\n\nrewrite根据指定正则表达式匹配规则，重写 URL 。\n语法：rewrite 正则表达式 要替换的内容 [flag];上下文：server、location、if示例：rewirte /images/(.*\\.jpg)$ /pic/$1; # $1是前面括号(.*\\.jpg)的反向引用\n\nflag 可选值的含义：\n\nlast 重写后的 URL 发起新请求，再次进入 server 段，重试 location 的中的匹配；\n\nbreak 直接使用重写后的 URL ，不再匹配其它 location 中语句；\n\nredirect 返回302临时重定向；\n\npermanent 返回301永久重定向；\n\n\nserver&#123;  listen 80;  server_name fe.lion.club; # 要在本地hosts文件进行配置  root html;  location /search &#123;   rewrite ^/(.*) https://www.baidu.com redirect;  &#125;    location /images &#123;   rewrite /images/(.*) /pics/$1;  &#125;    location /pics &#123;   rewrite /pics/(.*) /photos/$1;  &#125;    location /photos &#123;    &#125;&#125;\n\n按照这个配置我们来分析：\n\n当访问 fe.lion.club&#x2F;search 时，会自动帮我们重定向到 https://www.baidu.com。\n\n当访问 fe.lion.club&#x2F;images&#x2F;1.jpg 时，第一步重写 URL 为 fe.lion.club&#x2F;pics&#x2F;1.jpg ，找到 pics 的 location ，继续重写 URL 为 fe.lion.club&#x2F;photos&#x2F;1.jpg ，找到 &#x2F;photos 的 location 后，去 html&#x2F;photos 目录下寻找 1.jpg 静态资源。\n\n\nif 指令语法：if (condition) &#123;...&#125;上下文：server、location示例：if($http_user_agent ~ Chrome)&#123;  rewrite /(.*)/browser/$1 break;&#125;\n\ncondition 判断条件：\n\n$variable 仅为变量时，值为空或以0开头字符串都会被当做 false 处理；\n\n&#x3D; 或 !&#x3D; 相等或不等；\n\n~ 正则匹配；\n\n! ~ 非正则匹配；\n\n~* 正则匹配，不区分大小写；\n\n-f 或 ! -f 检测文件存在或不存在；\n\n-d 或 ! -d 检测目录存在或不存在；\n\n-e 或 ! -e 检测文件、目录、符号链接等存在或不存在；\n\n-x 或 ! -x 检测文件可以执行或不可执行；\n\n\n实例：\nserver &#123;  listen 8080;  server_name localhost;  root html;    location / &#123;   if ( $uri = &quot;/images/&quot; )&#123;     rewrite (.*) /pics/ break;    &#125;  &#125;&#125;\n\n当访问 localhost:8080&#x2F;images&#x2F; 时，会进入 if 判断里面执行 rewrite 命令。\nautoindex用户请求以 &#x2F; 结尾时，列出目录结构，可以用于快速搭建静态资源下载网站。\nautoindex.conf 配置信息：\nserver &#123;  listen 80;  server_name fe.lion-test.club;    location /download/ &#123;    root /opt/source;        autoindex on; # 打开 autoindex，，可选参数有 on | off    autoindex_exact_size on; # 修改为off，以KB、MB、GB显示文件大小，默认为on，以bytes显示出⽂件的确切⼤⼩    autoindex_format html; # 以html的方式进行格式化，可选参数有 html | json | xml    autoindex_localtime off; # 显示的⽂件时间为⽂件的服务器时间。默认为off，显示的⽂件时间为GMT时间  &#125;&#125;\n\n当访问 fe.lion.com&#x2F;download&#x2F; 时，会把服务器 &#x2F;opt&#x2F;source&#x2F;download&#x2F; 路径下的文件展示出来，如下图所示：\n\n变量Nginx 提供给使用者的变量非常多，但是终究是一个完整的请求过程所产生数据， Nginx 将这些数据以变量的形式提供给使用者。\n下面列举些项目中常用的变量：\n\n实例演示 var.conf ：\nserver&#123; listen 8081; server_name var.lion-test.club; root /usr/share/nginx/html; location / &#123;  return 200 &quot;remote_addr: $remote_addrremote_port: $remote_portserver_addr: $server_addrserver_port: $server_portserver_protocol: $server_protocolbinary_remote_addr: $binary_remote_addrconnection: $connectionuri: $urirequest_uri: $request_urischeme: $schemerequest_method: $request_methodrequest_length: $request_lengthargs: $argsarg_pid: $arg_pidis_args: $is_argsquery_string: $query_stringhost: $hosthttp_user_agent: $http_user_agenthttp_referer: $http_refererhttp_via: $http_viarequest_time: $request_timehttps: $httpsrequest_filename: $request_filenamedocument_root: $document_root&quot;; &#125;&#125;\n\n当我们访问 http://var.lion-test.club:8081/test?pid=121414&amp;cid=sadasd  时，由于 Nginx 中写了 return 方法，因此 chrome 浏览器会默认为我们下载一个文件，下面展示的就是下载的文件内容：\nremote_addr: 27.16.220.84remote_port: 56838server_addr: 172.17.0.2server_port: 8081server_protocol: HTTP/1.1binary_remote_addr: 茉connection: 126uri: /test/request_uri: /test/?pid=121414&amp;cid=sadasdscheme: httprequest_method: GETrequest_length: 518args: pid=121414&amp;cid=sadasdarg_pid: 121414is_args: ?query_string: pid=121414&amp;cid=sadasdhost: var.lion-test.clubhttp_user_agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/88.0.4324.182 Safari/537.36http_referer: http_via: request_time: 0.000https: request_filename: /usr/share/nginx/html/test/document_root: /usr/share/nginx/html\n\nNginx 的配置还有非常多，以上只是罗列了一些常用的配置，在实际项目中还是要学会查阅文档。\nNginx 应用核心概念代理是在服务器和客户端之间假设的一层服务器，代理将接收客户端的请求并将它转发给服务器，然后将服务端的响应转发给客户端。\n不管是正向代理还是反向代理，实现的都是上面的功能。\n\n正向代理正向代理，意思是一个位于客户端和原始服务器(origin server)之间的服务器，为了从原始服务器取得内容，客户端向代理发送一个请求并指定目标(原始服务器)，然后代理向原始服务器转交请求并将获得的内容返回给客户端。\n正向代理是为我们服务的，即为客户端服务的，客户端可以根据正向代理访问到它本身无法访问到的服务器资源。\n正向代理对我们是透明的，对服务端是非透明的，即服务端并不知道自己收到的是来自代理的访问还是来自真实客户端的访问。\n反向代理反向代理（Reverse Proxy）方式是指以代理服务器来接受internet上的连接请求，然后将请求转发给内部网络上的服务器，并将从服务器上得到的结果返回给internet上请求连接的客户端，此时代理服务器对外就表现为一个反向代理服务器。\n反向代理是为服务端服务的，反向代理可以帮助服务器接收来自客户端的请求，帮助服务器做请求转发，负载均衡等。\n反向代理对服务端是透明的，对我们是非透明的，即我们并不知道自己访问的是代理服务器，而服务器知道反向代理在为他服务。\n反向代理的优势：\n\n隐藏真实服务器；\n\n负载均衡便于横向扩充后端动态服务；\n\n动静分离，提升系统健壮性；\n\n\n那么“动静分离”是什么？负载均衡又是什么？\n动静分离动静分离是指在 web 服务器架构中，将静态页面与动态页面或者静态内容接口和动态内容接口分开不同系统访问的架构设计方法，进而提示整个服务的访问性和可维护性。\n\n一般来说，都需要将动态资源和静态资源分开，由于 Nginx 的高并发和静态资源缓存等特性，经常将静态资源部署在 Nginx 上。如果请求的是静态资源，直接到静态资源目录获取资源，如果是动态资源的请求，则利用反向代理的原理，把请求转发给对应后台应用去处理，从而实现动静分离。\n使用前后端分离后，可以很大程度提升静态资源的访问速度，即使动态服务不可用，静态资源的访问也不会受到影响。\n负载均衡一般情况下，客户端发送多个请求到服务器，服务器处理请求，其中一部分可能要操作一些资源比如数据库、静态资源等，服务器处理完毕后，再将结果返回给客户端。\n这种模式对于早期的系统来说，功能要求不复杂，且并发请求相对较少的情况下还能胜任，成本也低。随着信息数量不断增长，访问量和数据量飞速增长，以及系统业务复杂度持续增加，这种做法已无法满足要求，并发量特别大时，服务器容易崩。\n很明显这是由于服务器性能的瓶颈造成的问题，除了堆机器之外，最重要的做法就是负载均衡。\n请求爆发式增长的情况下，单个机器性能再强劲也无法满足要求了，这个时候集群的概念产生了，单个服务器解决不了的问题，可以使用多个服务器，然后将请求分发到各个服务器上，将负载分发到不同的服务器，这就是负载均衡，核心是「分摊压力」。 Nginx 实现负载均衡，一般来说指的是将请求转发给服务器集群。\n举个具体的例子，晚高峰乘坐地铁的时候，入站口经常会有地铁工作人员大喇叭“请走 B 口， B 口人少车空….”，这个工作人员的作用就是负载均衡。\n\nNginx 实现负载均衡的策略：\n\n轮询策略：默认情况下采用的策略，将所有客户端请求轮询分配给服务端。这种策略是可以正常工作的，但是如果其中某一台服务器压力太大，出现延迟，会影响所有分配在这台服务器下的用户。\n\n最小连接数策略：将请求优先分配给压力较小的服务器，它可以平衡每个队列的长度，并避免向压力大的服务器添加更多的请求。\n\n最快响应时间策略：优先分配给响应时间最短的服务器。\n\n客户端 ip 绑定策略：来自同一个 ip 的请求永远只分配一台服务器，有效解决了动态网页存在的 session 共享问题。\n\n\nNginx 实战配置在配置反向代理和负载均衡等等功能之前，有两个核心模块是我们必须要掌握的，这两个模块应该说是 Nginx 应用配置中的核心，它们分别是： upstream 、proxy_pass 。\nupstream用于定义上游服务器（指的就是后台提供的应用服务器）的相关信息。\n\n语法：upstream name &#123; ...&#125;上下文：http示例：upstream back_end_server&#123;  server 192.168.100.33:8081&#125;\n\n在 upstream 内可使用的指令：\n\nserver 定义上游服务器地址；\n\nzone 定义共享内存，用于跨 worker 子进程；\n\nkeepalive 对上游服务启用长连接；\n\nkeepalive_requests 一个长连接最多请求 HTTP 的个数；\n\nkeepalive_timeout 空闲情形下，一个长连接的超时时长；\n\nhash 哈希负载均衡算法；\n\nip_hash 依据 IP 进行哈希计算的负载均衡算法；\n\nleast_conn 最少连接数负载均衡算法；\n\nleast_time 最短响应时间负载均衡算法；\n\nrandom 随机负载均衡算法；\n\n\nserver定义上游服务器地址。\n语法：server address [parameters]上下文：upstream\n\nparameters 可选值：\n\nweight&#x3D;number 权重值，默认为1；\n\nmax_conns&#x3D;number 上游服务器的最大并发连接数；\n\nfail_timeout&#x3D;time 服务器不可用的判定时间；\n\nmax_fails&#x3D;numer 服务器不可用的检查次数；\n\nbackup 备份服务器，仅当其他服务器都不可用时才会启用；\n\ndown 标记服务器长期不可用，离线维护；\n\n\nkeepalive限制每个 worker 子进程与上游服务器空闲长连接的最大数量。\nkeepalive connections;上下文：upstream示例：keepalive 16;\n\nkeepalive_requests单个长连接可以处理的最多 HTTP 请求个数。\n语法：keepalive_requests number;默认值：keepalive_requests 100;上下文：upstream\n\nkeepalive_timeout空闲长连接的最长保持时间。\n语法：keepalive_timeout time;默认值：keepalive_timeout 60s;上下文：upstream\n\n配置实例upstream back_end&#123; server 127.0.0.1:8081 weight=3 max_conns=1000 fail_timeout=10s max_fails=2;  keepalive 32;  keepalive_requests 50;  keepalive_timeout 30s;&#125;\n\nproxy_pass用于配置代理服务器。\n语法：proxy_pass URL;上下文：location、if、limit_except示例：proxy_pass http://127.0.0.1:8081proxy_pass http://127.0.0.1:8081/proxy\n\nURL 参数原则\n\nURL 必须以 http 或 https 开头；\n\nURL 中可以携带变量；\n\nURL 中是否带 URI ，会直接影响发往上游请求的 URL ；\n\n\n接下来让我们来看看两种常见的 URL 用法：\n\nproxy_pass http://192.168.100.33:8081\n\nproxy_pass http://192.168.100.33:8081/\n\n\n这两种用法的区别就是带 &#x2F; 和不带 &#x2F; ，在配置代理时它们的区别可大了：\n\n不带 &#x2F; 意味着 Nginx 不会修改用户 URL ，而是直接透传给上游的应用服务器；\n\n带 &#x2F; 意味着 Nginx 会修改用户 URL ，修改方法是将 location 后的 URL 从用户 URL 中删除；\n\n\n不带 &#x2F; 的用法：\nlocation /bbs/&#123;  proxy_pass http://127.0.0.1:8080;&#125;\n\n分析：\n\n用户请求 URL ： &#x2F;bbs&#x2F;abc&#x2F;test.html\n\n请求到达 Nginx 的 URL ： &#x2F;bbs&#x2F;abc&#x2F;test.html\n\n请求到达上游应用服务器的 URL ： &#x2F;bbs&#x2F;abc&#x2F;test.html\n\n\n带 &#x2F; 的用法：\nlocation /bbs/&#123;  proxy_pass http://127.0.0.1:8080/;&#125;\n\n分析：\n\n用户请求 URL ： &#x2F;bbs&#x2F;abc&#x2F;test.html\n\n请求到达 Nginx 的 URL ： &#x2F;bbs&#x2F;abc&#x2F;test.html\n\n请求到达上游应用服务器的 URL ： &#x2F;abc&#x2F;test.html\n\n\n并没有拼接上 &#x2F;bbs ，这点和 root 与 alias 之间的区别是保持一致的。\n配置反向代理这里为了演示更加接近实际，作者准备了两台云服务器，它们的公网 IP 分别是： 121.42.11.34 与 121.5.180.193 。\n我们把 121.42.11.34 服务器作为上游服务器，做如下配置：\n# /etc/nginx/conf.d/proxy.confserver&#123;  listen 8080;  server_name localhost;    location /proxy/ &#123;    root /usr/share/nginx/html/proxy;    index index.html;  &#125;&#125;# /usr/share/nginx/html/proxy/index.html&lt;h1&gt; 121.42.11.34 proxy html &lt;/h1&gt;\n\n配置完成后重启 Nginx 服务器\n把 121.5.180.193 服务器作为代理服务器，做如下配置：\n# /etc/nginx/conf.d/proxy.confupstream back_end &#123;  server 121.42.11.34:8080 weight=2 max_conns=1000 fail_timeout=10s max_fails=3;  keepalive 32;  keepalive_requests 80;  keepalive_timeout 20s;&#125;server &#123;  listen 80;  server_name proxy.lion.club;  location /proxy &#123;   proxy_pass http://back_end/proxy;  &#125;&#125;\n\n本地机器要访问 proxy.lion.club 域名，因此需要配置本地 hosts ，通过命令：vim &#x2F;etc&#x2F;hosts 进入配置文件，添加如下内容：\n121.5.180.193 proxy.lion.club\n\n\n分析：\n\n当访问 proxy.lion.club&#x2F;proxy 时通过 upstream 的配置找到 121.42.11.34:8080 ；\n\n因此访问地址变为 http://121.42.11.34:8080/proxy ；\n\n连接到 121.42.11.34 服务器，找到 8080 端口提供的 server ；\n\n通过 server 找到 &#x2F;usr&#x2F;share&#x2F;nginx&#x2F;html&#x2F;proxy&#x2F;index.html 资源，最终展示出来。\n\n\n配置负载均衡配置负载均衡主要是要使用 upstream 指令。\n我们把 121.42.11.34 服务器作为上游服务器，做如下配置（ &#x2F;etc&#x2F;nginx&#x2F;conf.d&#x2F;balance.conf ）：\nserver&#123;  listen 8020;  location / &#123;   return 200 &#x27;return 8020 \\n&#x27;;  &#125;&#125;server&#123;  listen 8030;  location / &#123;   return 200 &#x27;return 8030 \\n&#x27;;  &#125;&#125;server&#123;  listen 8040;  location / &#123;   return 200 &#x27;return 8040 \\n&#x27;;  &#125;&#125;\n\n配置完成后：\n\nnginx -t 检测配置是否正确；\n\nnginx -s reload 重启 Nginx 服务器；\n\n执行 ss -nlt 命令查看端口是否被占用，从而判断 Nginx 服务是否正确启动。\n\n\n把 121.5.180.193 服务器作为代理服务器，做如下配置（ &#x2F;etc&#x2F;nginx&#x2F;conf.d&#x2F;balance.conf ）：\nupstream demo_server &#123;  server 121.42.11.34:8020;  server 121.42.11.34:8030;  server 121.42.11.34:8040;&#125;server &#123;  listen 80;  server_name balance.lion.club;    location /balance/ &#123;   proxy_pass http://demo_server;  &#125;&#125;\n\n配置完成后重启 Nginx 服务器。并且在需要访问的客户端配置好 ip 和域名的映射关系。\n# /etc/hosts121.5.180.193 balance.lion.club\n\n在客户端机器执行 curl http://balance.lion.club/balance/ 命令：\n接下来，我们再来了解下 Nginx 的其它分发策略。\nhash 算法通过制定关键字作为 hash key ，基于 hash 算法映射到特定的上游服务器中。关键字可以包含有变量、字符串。\nupstream demo_server &#123;  hash $request_uri;  server 121.42.11.34:8020;  server 121.42.11.34:8030;  server 121.42.11.34:8040;&#125;server &#123;  listen 80;  server_name balance.lion.club;    location /balance/ &#123;   proxy_pass http://demo_server;  &#125;&#125;\n\nhash $request_uri 表示使用 request_uri 变量作为 hash 的 key 值，只要访问的 URI 保持不变，就会一直分发给同一台服务器。\nip_hash根据客户端的请求 ip 进行判断，只要 ip 地址不变就永远分配到同一台主机。它可以有效解决后台服务器 session 保持的问题。\nupstream demo_server &#123;  ip_hash;  server 121.42.11.34:8020;  server 121.42.11.34:8030;  server 121.42.11.34:8040;&#125;server &#123;  listen 80;  server_name balance.lion.club;    location /balance/ &#123;   proxy_pass http://demo_server;  &#125;&#125;\n\n最少连接数算法各个 worker 子进程通过读取共享内存的数据，来获取后端服务器的信息。来挑选一台当前已建立连接数最少的服务器进行分配请求。\n语法：least_conn;上下文：upstream;\n示例：\nupstream demo_server &#123;  zone test 10M; # zone可以设置共享内存空间的名字和大小  least_conn;  server 121.42.11.34:8020;  server 121.42.11.34:8030;  server 121.42.11.34:8040;&#125;server &#123;  listen 80;  server_name balance.lion.club;    location /balance/ &#123;   proxy_pass http://demo_server;  &#125;&#125;\n\n最后你会发现，负载均衡的配置其实一点都不复杂。\n配置缓存缓存可以非常有效的提升性能，因此不论是客户端（浏览器），还是代理服务器（ Nginx ），乃至上游服务器都多少会涉及到缓存。可见缓存在每个环节都是非常重要的。下面让我们来学习 Nginx 中如何设置缓存策略。\nproxy_cache存储一些之前被访问过、而且可能将要被再次访问的资源，使用户可以直接从代理服务器获得，从而减少上游服务器的压力，加快整个访问速度。\n语法：proxy_cache zone | off ; # zone 是共享内存的名称默认值：proxy_cache off;上下文：http、server、location\n\nproxy_cache_path设置缓存文件的存放路径。\n语法：proxy_cache_path path [level=levels] ...可选参数省略，下面会详细列举默认值：proxy_cache_path off上下文：http\n\n参数含义：\n\npath 缓存文件的存放路径；\n\nlevel path 的目录层级；\n\nkeys_zone 设置共享内存；\n\ninactive 在指定时间内没有被访问，缓存会被清理，默认10分钟；\n\n\nproxy_cache_key设置缓存文件的 key 。\n语法：proxy_cache_key默认值：proxy_cache_key $scheme$proxy_host$request_uri;上下文：http、server、location\n\nproxy_cache_valid配置什么状态码可以被缓存，以及缓存时长。\n语法：proxy_cache_valid [code...] time;上下文：http、server、location配置示例：proxy_cache_valid 200 304 2m;; # 说明对于状态为200和304的缓存文件的缓存时间是2分钟\n\nproxy_no_cache定义相应保存到缓存的条件，如果字符串参数的至少一个值不为空且不等于“ 0”，则将不保存该响应到缓存。\n语法：proxy_no_cache string;上下文：http、server、location示例：proxy_no_cache $http_pragma    $http_authorization;\n\nproxy_cache_bypass定义条件，在该条件下将不会从缓存中获取响应。\n语法：proxy_cache_bypass string;上下文：http、server、location示例：proxy_cache_bypass $http_pragma    $http_authorization;\n\nupstream_cache_status 变量它存储了缓存是否命中的信息，会设置在响应头信息中，在调试中非常有用。\nMISS: 未命中缓存HIT： 命中缓存EXPIRED: 缓存过期STALE: 命中了陈旧缓存REVALIDDATED: Nginx验证陈旧缓存依然有效UPDATING: 内容陈旧，但正在更新BYPASS: X响应从原始服务器获取\n\n配置实例我们把 121.42.11.34 服务器作为上游服务器，做如下配置（ &#x2F;etc&#x2F;nginx&#x2F;conf.d&#x2F;cache.conf ）：\nserver &#123;  listen 1010;  root /usr/share/nginx/html/1010;  location / &#123;   index index.html;  &#125;&#125;server &#123;  listen 1020;  root /usr/share/nginx/html/1020;  location / &#123;   index index.html;  &#125;&#125;\n\n把 121.5.180.193 服务器作为代理服务器，做如下配置（ &#x2F;etc&#x2F;nginx&#x2F;conf.d&#x2F;cache.conf ）：\nproxy_cache_path /etc/nginx/cache_temp levels=2:2 keys_zone=cache_zone:30m max_size=2g inactive=60m use_temp_path=off;upstream cache_server&#123;  server 121.42.11.34:1010;  server 121.42.11.34:1020;&#125;server &#123;  listen 80;  server_name cache.lion.club;  location / &#123;    proxy_cache cache_zone; # 设置缓存内存，上面配置中已经定义好的    proxy_cache_valid 200 5m; # 缓存状态为200的请求，缓存时长为5分钟    proxy_cache_key $request_uri; # 缓存文件的key为请求的URI    add_header Nginx-Cache-Status $upstream_cache_status # 把缓存状态设置为头部信息，响应给客户端    proxy_pass http://cache_server; # 代理转发  &#125;&#125;\n\n缓存就是这样配置，我们可以在 &#x2F;etc&#x2F;nginx&#x2F;cache_temp 路径下找到相应的缓存文件。\n「对于一些实时性要求非常高的页面或数据来说，就不应该去设置缓存，下面来看看如何配置不缓存的内容。」\nserver &#123;  listen 80;  server_name cache.lion.club;  # URI 中后缀为 .txt 或 .text 的设置变量值为 &quot;no cache&quot;  if ($request_uri ~ \\.(txt|text)$) &#123;   set $cache_name &quot;no cache&quot;  &#125;    location / &#123;    proxy_no_cache $cache_name; # 判断该变量是否有值，如果有值则不进行缓存，如果没有值则进行缓存    proxy_cache cache_zone; # 设置缓存内存    proxy_cache_valid 200 5m; # 缓存状态为200的请求，缓存时长为5分钟    proxy_cache_key $request_uri; # 缓存文件的key为请求的URI    add_header Nginx-Cache-Status $upstream_cache_status # 把缓存状态设置为头部信息，响应给客户端    proxy_pass http://cache_server; # 代理转发  &#125;&#125;\n\nHTTPS在学习如何配置 HTTPS 之前，我们先来简单回顾下 HTTPS 的工作流程是怎么样的？它是如何进行加密保证安全的？\nHTTPS 工作流程\n客户端（浏览器）访问 https://www.baidu.com 百度网站；\n\n百度服务器返回 HTTPS 使用的 CA 证书；\n\n浏览器验证 CA 证书是否为合法证书；\n\n验证通过，证书合法，生成一串随机数并使用公钥（证书中提供的）进行加密；\n\n发送公钥加密后的随机数给百度服务器；\n\n百度服务器拿到密文，通过私钥进行解密，获取到随机数（公钥加密，私钥解密，反之也可以）；\n\n百度服务器把要发送给浏览器的内容，使用随机数进行加密后传输给浏览器；\n\n此时浏览器可以使用随机数进行解密，获取到服务器的真实传输内容；\n\n\n这就是 HTTPS 的基本运作原理，使用对称加密和非对称机密配合使用，保证传输内容的安全性。\n配置证书下载证书的压缩文件，里面有个 Nginx  文件夹，把 xxx.crt 和 xxx.key 文件拷贝到服务器目录，再进行如下配置：\nserver &#123;  listen 443 ssl http2 default_server;   # SSL 访问端口号为 443  server_name lion.club;         # 填写绑定证书的域名(我这里是随便写的)  ssl_certificate /etc/nginx/https/lion.club_bundle.crt;   # 证书地址  ssl_certificate_key /etc/nginx/https/lion.club.key;      # 私钥地址  ssl_session_timeout 10m;  ssl_protocols TLSv1 TLSv1.1 TLSv1.2; # 支持ssl协议版本，默认为后三个，主流版本是[TLSv1.2]   location / &#123;    root         /usr/share/nginx/html;    index        index.html index.htm;  &#125;&#125;\n如此配置后就能正常访问 HTTPS 版的网站了。\n配置跨域 CORS同源策略限制了从同一个源加载的文档或脚本如何与来自另一个源的资源进行交互。这是一个用于隔离潜在恶意文件的重要安全机制。通常不允许不同源间的读操作。\n如果两个页面的协议，端口（如果有指定）和域名都相同，则两个页面具有相同的源。\n下面给出了与 URL http://store.company.com/dir/page.html 的源进行对比的示例:\nhttp://store.company.com/dir2/other.html 同源https://store.company.com/secure.html 不同源，协议不同http://store.company.com:81/dir/etc.html 不同源，端口不同http://news.company.com/dir/other.html 不同源，主机不同\n\n不同源会有如下限制：\n\nWeb 数据层面，同源策略限制了不同源的站点读取当前站点的 Cookie 、 IndexDB 、 LocalStorage 等数据。\n\nDOM 层面，同源策略限制了来自不同源的 JavaScript 脚本对当前 DOM 对象读和写的操作。\n\n网络层面，同源策略限制了通过 XMLHttpRequest 等方式将站点的数据发送给不同源的站点。\n\n\nNginx 解决跨域的原理例如：\n\n前端 server 的域名为： fe.server.com\n\n后端服务的域名为： dev.server.com\n\n\n现在我在 fe.server.com 对 dev.server.com 发起请求一定会出现跨域。\n现在我们只需要启动一个 Nginx 服务器，将 server_name 设置为 fe.server.com 然后设置相应的 location 以拦截前端需要跨域的请求，最后将请求代理回 dev.server.com 。如下面的配置：\nserver &#123; listen      80; server_name  fe.server.com; location / &#123;  proxy_pass dev.server.com; &#125;&#125;\n\n这样可以完美绕过浏览器的同源策略： fe.server.com 访问 Nginx 的 fe.server.com 属于同源访问，而 Nginx 对服务端转发的请求不会触发浏览器的同源策略。\n配置开启 gzip 压缩GZIP 是规定的三种标准 HTTP 压缩格式之一。目前绝大多数的网站都在使用 GZIP 传输 HTML 、CSS 、 JavaScript 等资源文件。\n对于文本文件， GZiP 的效果非常明显，开启后传输所需流量大约会降至 1&#x2F;4~1&#x2F;3 。\n并不是每个浏览器都支持 gzip 的，如何知道客户端是否支持 gzip 呢，请求头中的 Accept-Encoding 来标识对压缩的支持。\n\n启用 gzip 同时需要客户端和服务端的支持，如果客户端支持 gzip 的解析，那么只要服务端能够返回 gzip 的文件就可以启用 gzip 了,我们可以通过 Nginx 的配置来让服务端支持 gzip 。下面的 respone 中 content-encoding:gzip ，指服务端开启了 gzip 的压缩方式。\n\n在 &#x2F;etc&#x2F;nginx&#x2F;conf.d&#x2F;  文件夹中新建配置文件 gzip.conf ：\n# # 默认off，是否开启gzipgzip on; # 要采用 gzip 压缩的 MIME 文件类型，其中 text/html 被系统强制启用；gzip_types text/plain text/css application/json application/x-javascript text/xml application/xml application/xml+rss text/javascript;# ---- 以上两个参数开启就可以支持Gzip压缩了 ---- ## 默认 off，该模块启用后，Nginx 首先检查是否存在请求静态文件的 gz 结尾的文件，如果有则直接返回该 .gz 文件内容；gzip_static on;# 默认 off，nginx做为反向代理时启用，用于设置启用或禁用从代理服务器上收到相应内容 gzip 压缩；gzip_proxied any;# 用于在响应消息头中添加 Vary：Accept-Encoding，使代理服务器根据请求头中的 Accept-Encoding 识别是否启用 gzip 压缩；gzip_vary on;# gzip 压缩比，压缩级别是 1-9，1 压缩级别最低，9 最高，级别越高压缩率越大，压缩时间越长，建议 4-6；gzip_comp_level 6;# 获取多少内存用于缓存压缩结果，16 8k 表示以 8k*16 为单位获得；gzip_buffers 16 8k;# 允许压缩的页面最小字节数，页面字节数从header头中的 Content-Length 中进行获取。默认值是 0，不管页面多大都压缩。建议设置成大于 1k 的字节数，小于 1k 可能会越压越大；# gzip_min_length 1k;# 默认 1.1，启用 gzip 所需的 HTTP 最低版本；gzip_http_version 1.1;\n\n其实也可以通过前端构建工具例如 webpack 、rollup 等在打生产包时就做好 Gzip 压缩，然后放到 Nginx 服务器中，这样可以减少服务器的开销，加快访问速度。\n关于 Nginx 的实际应用就学习到这里，相信通过掌握了 Nginx 核心配置以及实战配置，之后再遇到什么需求，我们也能轻松应对。接下来，让我们再深入一点学习下 Nginx 的架构。\nNginx 架构进程结构多进程结构 Nginx 的进程模型图：\n\n多进程中的 Nginx 进程架构如下图所示，会有一个父进程（ Master Process ），它会有很多子进程（ Child Processes ）。\n\nMaster Process 用来管理子进程的，其本身并不真正处理用户请求。\n\n某个子进程 down 掉的话，它会向 Master 进程发送一条消息，表明自己不可用了，此时 Master 进程会去新起一个子进程。\n\n某个配置文件被修改了 Master 进程会去通知 work 进程获取新的配置信息，这也就是我们所说的热部署。\n\n子进程间是通过共享内存的方式进行通信的。\n\n\n配置文件重载原理reload 重载配置文件的流程：\n\n向 master 进程发送 HUP 信号（ reload 命令）；\n\nmaster 进程检查配置语法是否正确；\n\nmaster 进程打开监听端口；\n\nmaster 进程使用新的配置文件启动新的 worker 子进程；\n\nmaster 进程向老的 worker 子进程发送 QUIT 信号；\n\n老的 worker 进程关闭监听句柄，处理完当前连接后关闭进程；\n\n整个过程 Nginx 始终处于平稳运行中，实现了平滑升级，用户无感知；\n\n\nNginx 模块化管理机制Nginx 的内部结构是由核心部分和一系列的功能模块所组成。这样划分是为了使得每个模块的功能相对简单，便于开发，同时也便于对系统进行功能扩展。Nginx 的模块是互相独立的,低耦合高内聚。\n\n","categories":["总结笔记"],"tags":["nginx"]},{"title":"Redis总结","url":"/2022_05_01_redis/","content":"Redis是基于内存数据库，操作效率高，提供丰富的数据结构（Redis底层对数据结构还做了优化），可用作数据库，缓存，消息中间件等。本文从数据结构，到集群，到常见问题逐步深入了解Redis。\n高性能\n单线程模型\n\n基于内存操作\n\nepoll多路复用模型\n\n高效的数据存储结构\n\n\n\nredis的单线程指的是数据处理使用的单线程，实际上它主要包含\n\nIO线程：处理网络消息收发\n主线程：处理数据读写操作，包括事务、Lua脚本等\n持久化线程：执行RDB或AOF时，使用持久化线程处理，避免主线程的阻塞\n过期键清理线程：用于定期清理过期键\n\n\n至于redis为什么使用单线程处理数据，是因为redis基于内存操作，并且有高效的数据类型，它的性能瓶颈并不在CPU计算，主要在于网络IO，而网络IO在后来的版本中也被独立出来了IO线程，因此它能快速处理数据，单线程反而避免了多线程所带来的并发和资源争抢的问题。\n全局数据存储Redis底层存储基于全局Hash表，存储结构和Java的HashMap类似（数组+链表方式）\n\nrehashRedis 默认使用了两个全局哈希表：哈希表 1 和哈希表 2。一开始，当你刚插入数据时，默认使用哈希表 1，此时的哈希表 2 并没有被分配空间。随着数据逐步增多，Redis 开始执行 rehash\n\n给哈希表 2 分配更大的空间，例如是当前哈希表 1 大小的两倍；\n\n把哈希表 1 中的数据重新进行打散映射到hash表2中；这个过程采用渐进式hash 即拷贝数据时，Redis 仍然正常处理客户端请求，每处理一个请求时，从哈希表 1 中的第一个索引位置开始，顺带着将这个索引位置上的所有 entries 拷贝到哈希表 2 中；等处理下一个请求时，再顺带拷贝哈希表 1 中的下一个索引位置的 entries\n\n释放哈希表 1 的空间。\n\n\n数据类型查看存储编码类型：object encoding key\n127.0.0.1:6379[1]&gt; keys *1) &quot;1&quot;127.0.0.1:6379[1]&gt; OBJECT ENCODING 1&quot;int&quot;127.0.0.1:6379[1]&gt;\n\n1. stringstring是最常用的类型，它的底层存储结构是SDS\n\n存储结构redis的string分三种情况对对象编码，目的是为了节省内存空间：\nrobj *tryObjectEncodingEx(robj *o, int try_trim)\n\n\nif: value长度小于20字节且可以转换为整数（long类型），编码为OBJ_ENCODING_INT，其中若数字在0到10000之间，还可以使用内存共享的数字对象\n\nelse if: 若value长度小于OBJ_ENCODING_EMBSTR_SIZE_LIMIT（44字节），编码为OBJ_ENCODING_EMBSTR\n\nelse: 保持编码为OBJ_ENCODING_RAW\n\n\n常用命令SET key valueMSET key value [key value ...]SETNX key value #常用作分布式锁GET keyMGET key [key ...]DEL key [key ...]EXPIRE key secondsINCR keyDECR keyINCRBY key incrementDECRBY key increment\n\n常用场景\n简单键值对\n自增计数器\n\nINCR作为主键的问题\n缺陷：若数据量大的情况下，大量使用INCR来自增主键会让redis的自增操作频繁，影响redis的正常使用\n\n优化：每台服务可以使用INCRBY一次性获取一百或者一千或者多少个id段来慢慢分配，这样能大量减少redis的incr命令所带来的消耗\n\n\n2. list\n存储结构redis的list首先会按紧凑列表存储（listPack），当紧凑列表的长度达到list_max_listpack_size之后，会转换为双向链表\n// 1.LPUSH/RPUSH/LPUSHX/RPUSHX这些命令的统一入口void pushGenericCommand(client *c, int where, int xx)// 2.追加元素，并尝试转换紧凑列表void listTypeTryConversionAppend(robj *o, robj **argv, int start, int end, beforeConvertCB fn, void *data)// 3.尝试转换紧凑列表static void listTypeTryConversionRaw(robj *o, list_conv_type lct, robj **argv, int start, int end, beforeConvertCB fn, void *data)// 4.尝试转换紧凑列表// 若紧凑列表的长度达到list_max_listpack_size之后，则转换static void listTypeTryConvertQuicklist(robj *o, int shrinking, beforeConvertCB fn, void *data)\n\n当redis进行list元素移除时\n// 1.移除list元素的统一入口void listElementsRemoved(client *c, robj *key, int where, robj *o, long count, int signal, int *deleted)// 2.尝试转换void listTypeTryConversion(robj *o, list_conv_type lct, beforeConvertCB fn, void *data)// 3.尝试转换static void listTypeTryConversionRaw(robj *o, list_conv_type lct, robj **argv, int start, int end, beforeConvertCB fn, void *data)// 4.尝试转换双向链表// 若双向链表中只剩一个节点，且是压缩节点，则对双向链表转换为紧凑列表static void listTypeTryConvertQuicklist(robj *o, int shrinking, beforeConvertCB fn, void *data)\n\n以下参数可在redis.conf配置\nlist_max_listpack_size：默认-2\n常用命令LPUSH key value [value ...]RPUSH key value [value ...]LPOP keyRPOP keyLRANGE key start stopBLPOP key [key ...] timeout #从key列表头弹出一个元素，若没有元素，则阻塞等待timeout秒，0则一直阻塞等待BRPOP key [key ...] timeout #从key列表尾弹出一个元素，若没有元素，则阻塞等待timeout秒，0则一直阻塞等待\n\n组合数据结构\n根据list的特性，可以组成实现以下常用的数据结构\n\nStack（栈）：LPUSH + LPOP\n\nQueue（队列）：LPUSH + RPOP\n\nBlocking MQ（阻塞队列）：LPUSH + BRPOP\n\n\nredis实现数据结构的意义在于分布式环境的实现\n常用场景\n缓存有序列表结构\n\n构建分布式数据结构（栈、队列等）\n\n\n3. hash\n存储结构redis的hash首先会按紧凑列表存储（listPack），当紧凑列表的长度达到hash_max_listpack_entries或添加的元素大小超过hash_max_listpack_value之后，会转换为Hash表\n// 1.添加hash元素void hsetCommand(client *c)void hsetnxCommand(client *c)// 2.尝试转换Hash表// 若紧凑列表的长度达到hash_max_listpack_entries// 或添加的元素大小超过hash_max_listpack_value// 则进行转换void hashTypeTryConversion(robj *o, robj **argv, int start, int end)// 3.尝试转换Hash表void hashTypeConvert(robj *o, int enc)// 4.转换Hash表void hashTypeConvertListpack(robj *o, int enc)\n\n以下参数可在redis.conf配置\n\nhash_max_listpack_value：默认64\n\n\nhash_max_listpack_entries：默认512\n\n常用命令HSET key field valueHSETNX key field valueHMSET key field value [field value ...]HGET key fieldHMGET key field [field ...]HDEL key field [field ...]HLEN keyHGETALL keyHINCRBY key field increment\n\n常用场景对象缓存\n4. set\n存储结构\nredis的set添加元素时，若存储对象是整形数字且集合小于set_max_intset_entries，则存储为OBJ_ENCODING_INTSET，若集合长度小于set_max_listpack_entries时，存储为紧凑列表。否则，存储为Hash表\n\n// 1.添加set元素void saddCommand(client *c)// 2.1.创建set表// 若存储对象是整形数字且集合小于set_max_listpack_entries，则存储为OBJ_ENCODING_INTSET// 若集合长度小于set_max_listpack_entries时，存储为紧凑列表// 否则存储为Hash表robj *setTypeCreate(sds value, size_t size_hint)// 2.2 尝试转换set表// 如果编码是OBJ_ENCODING_LISTPACK（紧凑列表），且集合长度大于set_max_listpack_entries// 或编码是OBJ_ENCODING_INTSET（整形集合），且集合长度大于set_max_intset_entries// 则进行转换为Hash表void setTypeMaybeConvert(robj *set, size_t size_hint)// 2.3 添加元素int setTypeAdd(robj *subject, sds value)int setTypeAddAux(robj *set, char *str, size_t len, int64_t llval, int str_is_sds)// 2.4 若整形数组添加元素，长度超过set_max_intset_entries，则转换为Hash表static void maybeConvertIntset(robj *subject)\n\n以下参数可在redis.conf配置\n\nset_max_intset_entries：默认512\n\n\nset_max_listpack_entries：默认128\n\n常用命令SADD key member [member ...]SREM key member [member ...]SMEMBERS keySCARD keySISMEMBERS key memberSRANDMEMBER key [count]SPOP key [count]SRANDOMEMBER key [count]SINTER key [key ...] #交集运算SINTERSTORE destination key [key ...] #将交集结果存入新集合destinationSUNION key [key ...] #并集运算SUNIONSTORE destination key [key ...] #将并集结果存入新集合destinationSDIFF key [key ...] #差集运算SDIFFSTORE destination key [key ...] #将差集结果存入新集合destination\n\n常用场景\n缓存无序集合\n\n需要求交集并集差集的场景\n\n\n5. sorted set\n存储结构根据情况可能创建紧凑列表或跳表\n// 1.添加元素void zaddCommand(client *c)void zaddGenericCommand(client *c, int flags)// 2.1 创建元素// 若集合长度&lt;=zset_max_listpack_entries 并且值的长度&lt;=zset_max_listpack_value，则创建紧凑列表// 否则创建跳表节点robj *zsetTypeCreate(size_t size_hint, size_t val_len_hint)// 2.2 添加元素// 若集合是紧凑列表，且集合元素超过zset_max_listpack_entries// 或当前添加的元素长度超过zset_max_listpack_value// 则将紧凑列表转换为跳表int zsetAdd(robj *zobj, double score, sds ele, int in_flags, int *out_flags, double *newscore)\n\n以下参数可在redis.conf配置\n\nzset_max_listpack_entries：默认128\n\n\nzset_max_listpack_value：默认64\n\n跳表仅在以下情况转换回压缩列表\n\n使用命令georadius时，判断元素长度若小于等于zset_max_listpack_entries，并且最大元素的长度小于等于zset_max_listpack_value\n\nvoid georadiusGeneric(client *c, int srcKeyIndex, int flags)\n\n\n使用命令zunion&#x2F;zinter&#x2F;zdiff命令（求并集交集差集）时，判断元素长度若小于等于zset_max_listpack_entries，并且最大元素的长度小于等于zset_max_listpack_value\n\nvoid zunionInterDiffGenericCommand(client *c, robj *dstkey, int numkeysIndex, int op, int cardinality_only)\n\n常用命令ZADD key score member [[score member]...]ZREM key member [member ...]ZSCORE key memberZINCRBY key increment memberZCARD keyZRANGE key start stop [WITHSCORES]ZREVRANGE key start stop [WITHSCORES]ZUNIONSTORE destkey numkeys key [key ...] # 并集计算ZINTERSTORE destkey numkeys key [key ...] # 交集计算\n\n常用场景排行榜\n底层数据结构RedisObject&#123;   unsigned type:4;//类型 五种对象类型   unsigned encoding:4;//编码   void *ptr;//指向底层实现数据结构的指针   int refcount;//引用计数   unsigned lru:24;//记录最后一次被命令程序访问的时间&#125;robj;\n\n\ntype ：表示对象的类型，占4个比特；目前包括REDIS_STRING(字符串)、REDIS_LIST (列表)、REDIS_HASH(哈希)、REDIS_SET(集合)、REDIS_ZSET(有序集合)。\n\nencoding：占4个比特，Redis支持的每种类型，都有至少两种内部编码，例如对于字符串，有int、embstr、raw三种编码。通过encoding属性，Redis可以根据不同的使用场景来为对象设置不同的编码，大大提高了Redis的灵活性和效率。以列表对象为例，有紧凑列表和双端链表两种编码方式；如果列表中的元素较少，Redis倾向于使用紧凑列表进行存储，因为紧凑列表占用内存更少，而且比双端链表可以更快载入；当列表对象元素较多时，紧凑列表就会转化为更适合存储大量元素的双端链表。\n\nptr：指针指向具体的数据。\n\nrefcount：记录的是该对象被引用的次数，类型为整型。主要用于对象的引用计数和内存回收。Redis中被多次使用的对象(refcount&gt;1)，称为共享对象。Redis为了节省内存，当有一些对象重复出现时，新的程序不会创建新的对象，而是仍然使用原来的对象。这个被重复使用的对象，就是共享对象。目前共享对象仅支持整数值的字符串对象。共享对象只能是整数值的字符串对象，但是5种类型都可能使用共享对象。Redis服务器在初始化时，会创建10000个字符串对象，值分别是0~9999的整数值；\n\nlru：Redis 对象头中的 lru 字段，在 LRU 算法下和 LFU 算法下使用方式并不相同。\n\n在 LRU 算法中，Redis 对象头的 24 bits 的 lru 字段是用来记录 key 的访问时间戳，因此在 LRU 模式下，Redis可以根据对象头中的 lru 字段记录的值，来比较最后一次 key 的访问时间长，从而淘汰最久未被使用的 key。\n\n在 LFU 算法中，Redis对象头的 24 bits 的 lru 字段被分成两段来存储，高 16bit 存储 ldt(Last Decrement Time)，低 8bit 存储 logc(Logistic Counter)。\n\n一个redisObject对象的大小为16字节：4bit+4bit+24bit+4Byte+8Byte&#x3D;16Byte\n\n\nSDS 简单动态字符串(Simple Dynamic String)typedef char *sds;struct __attribute__ ((__packed__)) sdshdr5 &#123; // 对应的字符串长度小于 1&lt;&lt;5 32字节   unsigned char flags; /* 3 lsb of type, and 5 msb of string length intembstr*/   char buf[];&#125;;struct __attribute__ ((__packed__)) sdshdr8 &#123; // 对应的字符串长度小于 1&lt;&lt;8 256   uint8_t len; /* used */ //目前字符创的长度 用1字节存储   uint8_t alloc; //已经分配的总长度 用1字节存储   unsigned char flags; //flag用3bit来标明类型，类型后续解释，其余5bit目前没有使用 embstr raw   char buf[]; //柔性数组，以&#x27;\\0&#x27;结尾&#125;;struct __attribute__ ((__packed__)) sdshdr16 &#123; // 对应的字符串长度小于 1&lt;&lt;16   uint16_t len; /*已使用长度，用2字节存储*/   uint16_t alloc; /* 总长度，用2字节存储*/   unsigned char flags; /* 3 lsb of type, 5 unused bits */   char buf[];&#125;;struct __attribute__ ((__packed__)) sdshdr32 &#123; // 对应的字符串长度小于 1&lt;&lt;32   uint32_t len; /*已使用长度，用4字节存储*/   uint32_t alloc; /* 总长度，用4字节存储*/   unsigned char flags;/* 低3位存储类型, 高5位预留 */   char buf[];/*柔性数组，存放实际内容*/&#125;;struct __attribute__ ((__packed__)) sdshdr64 &#123; // 对应的字符串长度小于 1&lt;&lt;64   uint64_t len; /*已使用长度，用8字节存储*/   uint64_t alloc; /* 总长度，用8字节存储*/   unsigned char flags; /* 低3位存储类型, 高5位预留 */   char buf[];/*柔性数组，存放实际内容*/&#125;;\n\n字符串类型的内部编码有3种:\n\nint：8个字节的长整型。字符串值是整型时，这个值使用long整型表示。\n\nembstr：**&lt;&#x3D;44字节的字符串。embstr与raw都使用redisObject和sds保存数据，区别在于，embstr的使用只分配一次内存空间（因此redisObject和sds是连续的），而raw需要分配两次内存空间（分别为redisObject和sds分配空间）。因此与raw相比，embstr的好处在于创建时少分配一次空间，删除时少释放一次空间，以及对象的所有数据连在一起，寻找方便。而embstr的坏处也很明显，如果字符串的长度增加需要重新分配内存时，整个redisObject和sds都需要重新分配空间**，因此redis中的embstr实现为只读。\n\nraw：大于44个字节的字符串\n\n\nembstr和raw进行区分的长度，是44；是因为redisObject的长度是16字节，sds的长度是4+字符串长度；因此当字符串长度是44时，embstr的长度正好是16+4+44 &#x3D;64，jemalloc正好可以分配64字节的内存单元。\n压缩列表zipListziplist 被设计成一种内存紧凑型的数据结构，占用一块连续的内存空间，不仅可以利用 CPU 缓存，而且会针对不同长度的数据，进行相应编码，这种方法可以有效地节省内存开销。\nziplist 是一个特殊双向链表，不像普通的链表使用前后指针关联在一起，它是存储在连续内存上的。\n/* 创建一个空的 ziplist. */unsigned char *ziplistNew(void) &#123;    unsigned int bytes = ZIPLIST_HEADER_SIZE+ZIPLIST_END_SIZE;    unsigned char *zl = zmalloc(bytes);    ZIPLIST_BYTES(zl) = intrev32ifbe(bytes);    ZIPLIST_TAIL_OFFSET(zl) = intrev32ifbe(ZIPLIST_HEADER_SIZE);    ZIPLIST_LENGTH(zl) = 0;    zl[bytes-1] = ZIP_END;    return zl;&#125;\n\n\n\nzlbytes: 32 位无符号整型，记录 ziplist 整个结构体的占用空间大小。当然了也包括 zlbytes 本身。这个结构有个很大的用处，就是当需要修改 ziplist 时候不需要遍历即可知道其本身的大小。这和SDS中记录字符串的长度有相似之处。\n\nzltail: 32 位无符号整型, 记录整个 ziplist 中最后一个 entry 的偏移量。所以在尾部进行 POP 操作时候不需要先遍历一次。\n\nzllen: 16 位无符号整型, 记录 entry 的数量， 所以只能表示 2^16。但是 Redis 作了特殊的处理：当实体数超过 2^16 ,该值被固定为 2^16 - 1。所以这种时候要知道所有实体的数量就必须要遍历整个结构了。\n\nentry: 真正存数据的结构。\n\nzlend: 8 位无符号整型, 固定为 255 (0xFF)。为 ziplist 的结束标识。\n\n\nzipList缺陷\nziplist 在更新或者新增时候，如空间不够则需要对整个列表进行重新分配。当新插入的元素较大时，可能会导致后续元素的 prevlen 占用空间都发生变化，从而引起「连锁更新」问题，导致每个元素的空间都要重新分配，造成访问压缩列表性能的下降。\nziplist 节点的 prevlen 属性会根据前一个节点的长度进行不同的空间大小分配：\n\n如果前一个节点的长度小于 254 字节，那么 prevlen 属性需要用 1 字节的空间来保存这个长度值。\n\n如果前一个节点的长度大于等于 254 字节，那么 prevlen 属性需要用 5 字节的空间来保存这个长度值。\n\n\n假设有这样的一个 ziplist，每个节点都是等于 253 字节的。新增了一个大于等于 254 字节的新节点，由于之前的节点 prevlen 长度是 1 个字节。\n为了要记录新增节点的长度所以需要对节点 1 进行扩展，由于节点 1 本身就是 253 字节，再加上扩展为 5 字节的 pervlen 则长度超过了 254 字节，这时候下一个节点又要进行扩展了\n\nzipList特性\n\nziplist 为了节省内存，采用了紧凑的连续存储。所以在修改操作下并不能像一般的链表那么容易，需要从新分配新的内存，然后复制到新的空间。\n\nziplist 是一个双向链表，可以在时间复杂度为 O(1) 从下头部、尾部进行 pop 或 push。\n\n新增或更新元素可能会出现连锁更新现象。\n\n不能保存过多的元素，否则查询效率就会降低。\n\n\n紧凑列表listPackRedis7.0之后采用listPack全面替代zipList\n在 Redis5.0 出现了 listpack，目的是替代压缩列表，其最大特点是 listpack 中每个节点不再包含前一个节点的长度，压缩列表每个节点正因为需要保存前一个节点的长度字段，就会有连锁更新的隐患。\nunsigned char *lpNew(size_t capacity) &#123;    unsigned char *lp = lp_malloc(capacity &gt; LP_HDR_SIZE+1 ? capacity : LP_HDR_SIZE+1);    if (lp == NULL) return NULL;    lpSetTotalBytes(lp,LP_HDR_SIZE+1);    lpSetNumElements(lp,0);    lp[LP_HDR_SIZE] = LP_EOF;    return lp;&#125;\n\n\nlistpack 中每个节点不再包含前一个节点的长度，避免连锁更新的隐患发生。\n\nlistpack 相对于 ziplist，没有了指向末尾节点地址的偏移量，解决 ziplist 内存长度限制的问题。但一个 listpack 最大内存使用不能超过 1GB。\n\n\n跳表数组：查询快，插入删除慢 链表：查询慢，插入删除快 跳表：跳表是基于链表的一个优化，在链表的插入删除快的特性之上，也增加了它的查询效率。它是将有序链表改造为支持折半查找算法，它的插入、删除、查询都很快\n\n跳表缺陷：需要额外空间来建立索引层，以空间换时间，因此zset一开始是以紧凑列表存储，后续才会转换为跳表\n\n跳表的创建（添加元素时）\n\n\n\n当前zset不存在时，若添加元素时集合长度达到zset_max_listpack_entries，或添加的最后一个元素的大小超过zset_max_listpack_value，则直接创建跳表，跳表头结点创建最大层数（ZSKIPLIST_MAXLEVEL：32）的索引，并插入跳表当前添加的元素\n当前zset存在时，判断若元素长度超过zset_max_listpack_entries，则将紧凑列表转换为跳表，跳表头结点创建最大层数（ZSKIPLIST_MAXLEVEL：32）的索引，然后把其他元素依次插入跳表\n\n\n\n跳表的查询 从起始节点开始，通过多级索引进行折半查找，最终找到需要的数据\n\n跳表的插入 先通过折半查找找到节点对应要插入的链表位置，然后通过随机得到一个要插入的节点的索引层数，然后插入节点，并构建对应的多级索引\n\n跳表的删除 先通过折半查找找到要删除的节点的链表位置，删除节点，并删除对应的多级索引\n\n\n淘汰策略\nnoeviction（默认策略）：不会删除任何数据，拒绝所有写入操作并返回客户端错误消息（error）OOM command not allowed when used memory，此时 Redis 只响应删和读操作；\n\nallkeys-lru：从所有 key 中使用 LRU（Least Recently Used）算法进行淘汰（LRU 算法：最近最少使用算法）；\n\nallkeys-lfu：从所有 key 中使用 LFU（Least Frequently Used）算法进行淘汰（LFU 算法：最不常用算法，根据使用频率计算，4.0 版本新增）；\n\nvolatile-lru：从设置了过期时间的 key 中使用 LRU 算法进行淘汰；\n\nvolatile-lfu：从设置了过期时间的 key 中使用 LFU 算法进行淘汰；\n\nallkeys-random：从所有 key 中随机淘汰数据；\n\nvolatile-random：从设置了过期时间的 key 中随机淘汰数据；\n\nvolatile-ttl：在设置了过期时间的key中，淘汰过期时间剩余最短的\n\n\nRedis的LRU实现由于Redis 主要运行在单个线程中，它采用的是一种近似的 LRU 算法，而不是传统的完全 LRU 算法（没有把所有key组织为链表）。这种实现方式在保证性能的同时，仍然能够有效地识别并淘汰最近最少使用的键。当 Redis 进行内存淘汰时，会使用随机采样的方式来淘汰数据，它是随机取 5 个值（此值可配置），然后淘汰最久没有使用的那个。\nRedis的LFU实现Redis 在访问 key 时，对 logc进行变化：\n\n先按照上次访问距离当前的时长，来对 logc 进行衰减；\n\n再按照一定概率增加 logc 的值\n\n\nredis.conf 提供了两个配置项，用于调整 LFU 算法从而控制 logc 的增长和衰减：\n\nlfu-decay-time 用于调整 logc 的衰减速度，它是一个以分钟为单位的数值，默认值为1，lfu-decay-time 值越大，衰减越慢；\n\nlfu-log-factor 用于调整 logc 的增长速度，lfu-log-factor 值越大，logc 增长越慢\n\n\n删除策略redis的key过期删除策略采用惰性删除+定期删除实现：\n\n惰性删除：不主动删除过期键，每次从数据库访问 key 时，都检测 key 是否过期，如果过期则删除该 key\n\nRedis 的惰性删除策略由 db.c 文件中的 expireIfNeeded 函数实现，代码如下：\nint expireIfNeeded(redisDb *db, robj *key) &#123;    // 判断 key 是否过期    if (!keyIsExpired(db,key)) return 0;    ....    /* 删除过期键 */    ....    // 如果 server.lazyfree_lazy_expire 为 1 表示异步删除，反之同步删除；    return server.lazyfree_lazy_expire ? dbAsyncDelete(db,key) :                                         dbSyncDelete(db,key);&#125;\n\n定期删除：定期删除策略的做法是，每隔一段时间随机从数据库中取出一定数量的 key 进行检查，并删除其中的过期key\n\n在 Redis 中，默认每秒进行 10 次过期检查一次数据库，此配置可通过 Redis 的配置文件 redis.conf 进行配置，配置键为 hz 它的默认值是 hz 10；定期删除的实现在 expire.c 文件下的 activeExpireCycle 函数中，其中随机抽查的数量由 ACTIVE_EXPIRE_CYCLE_LOOKUPS_PER_LOOP 定义的，它是写死在代码中的，数值是 20；也就是说，数据库每轮抽查时，会随机选择 20 个 key 判断是否过期。\n管道Pipelineredis提供pipeline，可以让客户端一次发送一连串的命令给服务器执行，然后再返回执行结果\n\n应用场景：\n\n\n需要多次执行一连串的redis命令，且命令之间没有依赖的场景\n\n• 缺陷：\n\n\n不保证原子性，pipeline拿到命令只管串行执行，不管执行成功与否，也没有回滚机制\n\n\n\n\npipeline在执行过程中无法知道执行结果，只有全部执行结束才会返回全部结果\n\n\n\n\npipeline也不宜一次性发送过多命令，尽管节省了IO，但在redis端也依然会进行执行队列顺序执行\n\n\n使用示例/** * 一次io获取个值 * * @param redisKeyEnum * @param ids * @param clz * @param &lt;T&gt; * @param &lt;E&gt; * @return */public &lt;T, E extends T&gt; List&lt;T&gt; multiGet(RedisKeyEnum redisKeyEnum, List&lt;String&gt; ids, Class&lt;E&gt; clz) &#123;    ShardRedisConnectionFactory factory = getShardRedisConnectionFactory(redisKeyEnum);    ShardedJedis shardedJedis = factory.getConnection();    return execute(factory, shardedJedis, new Supplier&lt;List&lt;T&gt;&gt;() &#123;        @Override        public List&lt;T&gt; get() &#123;            // 1.获取管道            ShardedJedisPipeline pipeline = shardedJedis.pipelined();            List&lt;T&gt; list = new ArrayList&lt;&gt;();            List&lt;Response&lt;String&gt;&gt; respList = new ArrayList&lt;&gt;();            for (String id : ids) &#123;                String key = getKey(redisKeyEnum, id);                // 2.通过管道执行命令                Response&lt;String&gt; resp = pipeline.get(key);                respList.add(resp);            &#125;            // 3.统一提交命令            pipeline.sync();            for (Response&lt;String&gt; resp : respList) &#123;                // 4.遍历获取全部的命令执行返回结果                String result = resp.get();                if (result == null) &#123;                    continue;                &#125;                if (clz.equals(String.class)) &#123;                    list.add((E) result);                &#125; else &#123;                    list.add(JsonUtil.json2Obj(result, clz));                &#125;            &#125;            return list;        &#125;    &#125;);&#125;\n\n事务Redis 事务的本质是一组命令的集合。事务支持一次执行多个命令，一个事务中所有命令都会被序列化。在事务执行过程，会按照顺序串行化执行队列中的命令，其他客户端提交的命令请求不会插入到事务执行命令序列中。\n事务的命令：\n\nMULTI ：开启事务，redis会将后续的命令逐个放入队列中，然后使用EXEC命令来原子化执行这个命令系列。\n\nEXEC：执行事务中的所有操作命令。\n\nDISCARD：取消事务，放弃执行事务块中的所有命令。\n\nWATCH：监视一个或多个key,如果事务在执行前，这个key(或多个key)被其他命令修改，则事务被中断，不会执行事务中的任何命令。\n\nUNWATCH：取消WATCH对所有key的监视。\n\n\nredis事务在编译错误可以回滚，而运行时错误不能回滚，简单说，redis事务不支持回滚\nRedis的持久化redis提供了两种持久化的方式，分别是RDB（Redis DataBase）和AOF（Append Only File）。\n\nRDB，简而言之，就是在不同的时间点，将redis存储的数据生成快照并存储到磁盘等介质上；\n\nAOF，则是换了一个角度来实现持久化，那就是将redis执行过的所有写指令记录下来，在下次redis重新启动时，只要把这些写指令从前到后再重复执行一遍，就可以实现数据恢复了。AOF类似MySQL的binlog\n\n\n其实RDB和AOF两种方式也可以同时使用，在这种情况下，如果redis重启的话，则会优先采用AOF方式来进行数据恢复，这是因为AOF方式的数据恢复完整度更高。\n如果你没有数据持久化的需求，也完全可以关闭RDB和AOF方式，这样的话，redis将变成一个纯内存数据库\n1. AOFAOF日志是一种追加式持久化方式，它记录了每个写操作命令，以追加的方式将命令写入AOF文件。通过重新执行AOF文件中的命令，可以重建出数据在内存中的状态。AOF日志提供了更精确的持久化，适用于需要更高数据安全性和实时性的场景。\n优点：\n\nAOF日志可以实现更精确的数据持久化，每个写操作都会被记录。\n\n在AOF文件中，数据可以更好地恢复，因为它保存了所有的写操作历史。\n\nAOF日志适用于需要实时恢复数据的场景，如秒级数据恢复要求。\n\n\n缺点：\n\nAOF日志相对于RDB快照来说，可能会占用更多的磁盘空间，因为它是记录每个写操作的文本文件。\n\nAOF日志在恢复大数据集时可能会比RDB快照慢，因为需要逐条执行写操作。\n\n\n根据不同的需求，可以选择RDB快照、AOF日志或两者结合使用。你可以根据数据的重要性、恢复速度要求以及磁盘空间限制来选择合适的持久化方式。有时候，也可以通过同时使用两种方式来提供更高的数据保护级别。\n2. RDBRDB快照是一种全量持久化方式，它会周期性地将内存中的数据以二进制格式保存到磁盘上的RDB文件。RDB文件是一个经过压缩的二进制文件，包含了数据库在某个时间点的数据快照。RDB快照有助于实现紧凑的数据存储，适合用于备份和恢复。\n优点：\n\nRDB快照在恢复大数据集时速度较快，因为它是全量的数据快照。\n\n由于RDB文件是压缩的二进制文件，它在磁盘上的存储空间相对较小。\n\n适用于数据备份和灾难恢复。\n\n\n缺点：\n\nRDB快照是周期性的全量持久化，可能导致某个时间点之后的数据丢失。\n\n在保存快照时，Redis服务器会阻塞，可能对系统性能造成影响。\n\n\n发布订阅Redis提供了基于“发布&#x2F;订阅”模式的消息机制。此种模式下，消息发布者和订阅者不进行直接通信，发布者客户端向指定的频道（channel） 发布消息，订阅该频道的每个客户端都可以收到该消息。结构如下：\n\n该消息通信模式可用于模块间的解耦\n# 订阅消息 subscribe channel [channel ...]# 发布消息publish channel &quot;hello&quot;# 按模式订阅频道psubscribe pattern [pattern ...]# 退订频道unsubscribe pattern [pattern ...]# 按模式退订频道punsubscribe pattern [pattern ...]\nRedis发布订阅与消息队列的区别\n消息队列可以支持多种消息协议，但 Redis 没有提供对这些协议的支持；\n\n消息队列可以提供持久化功能，但 Redis无法对消息持久化存储，一旦消息被发送，如果没有订阅者接收，那么消息就会丢失；\n\n消息队列可以提供消息传输保障，当客户端连接超时或事务回滚等情况发生时，消息会被重新发送给客户端，Redis 没有提供消息传输保障。\n\n发布订阅消息量过多过频繁，也会占用redis的内存空间，挤占业务逻辑key的空间（可以通过放到不同redis解决）\n\n\nRedis集群模式redis集群主要有三种模式：主从复制，哨兵模式和Cluster\n主从复制主从复制模式中包含一个主数据库实例（master）与一个或多个从数据库实例（slave）\n\n工作机制\nslave启动后，向master发送SYNC命令，master接收到SYNC命令后通过bgsave保存快照，并使用缓冲区记录保存快照这段时间内执行的写命令\n\nmaster将保存的快照文件发送给slave，并继续记录执行的写命令\n\nslave接收到快照文件后，加载快照文件，载入数据\n\nmaster快照发送完后开始向slave发送缓冲区的写命令，slave接收命令并执行，完成复制初始化\n\nmaster每次执行一个写命令都会同步发送给slave，保持master与slave之间数据的一致性\n\n\n主从复制配置replicaof 127.0.0.1 6379 # master的ip，port masterauth 123456 # master的密码 replica-serve-stale-data no # 如果slave无法与master同步，设置成slave不可读，方便监控脚本发现问题\n\n优缺点优点：\n\nmaster能自动将数据同步到slave，可以进行读写分离，分担master的读压力\n\nmaster、slave之间的同步是以非阻塞的方式进行的，同步期间，客户端仍然可以提交查询或更新请求\n\n\n缺点：\n\n不具备自动容错与恢复功能，master或slave的宕机都可能导致客户端请求失败，需要等待机器重启或手动切换客户端IP才能恢复\n\nmaster宕机，如果宕机前数据没有同步完，则切换IP后会存在数据不一致的问题\n\n难以支持在线扩容，Redis的容量受限于单机配置\n\n\n哨兵模式主从切换技术的方法是：当主服务器宕机后，需要手动把一台从服务器切换为主服务器，这就需要人工干预，费事费力，还会造成一段时间内服务不可用。这不是一种推荐的方式，更多时候，我们优先考虑哨兵模式。\n哨兵模式是一种特殊的模式，首先Redis提供了哨兵的命令，哨兵是一个独立的进程，作为进程，它会独立运行。其原理是哨兵通过发送命令，等待Redis服务器响应，从而监控运行的多个Redis实例。\n\n这里的哨兵有两个作用\n\n通过发送命令，让Redis服务器返回监控其运行状态，包括主服务器和从服务器。\n\n当哨兵监测到master宕机，会自动将slave切换成master，然后通过发布订阅模式通知其他的从服务器，修改配置文件，让它们切换主机。\n\n\n然而一个哨兵进程对Redis服务器进行监控，可能会出现问题，为此，我们可以使用多个哨兵进行监控。各个哨兵之间还会进行监控，这样就形成了多哨兵模式。\n\n哨兵配置1.主从服务器配置\n# 使得Redis服务器可以跨网络访问bind 0.0.0.0# 设置密码requirepass &quot;123456&quot;# 指定主服务器，注意：有关slaveof的配置只是配置从服务器，主服务器不需要配置slaveof 192.168.11.128 6379# 主服务器密码，注意：有关slaveof的配置只是配置从服务器，主服务器不需要配置masterauth 123456\n\n2.配置哨兵 在Redis安装目录下有一个sentinel.conf文件，copy一份进行修改\n# 禁止保护模式protected-mode no# 配置监听的主服务器，这里sentinel monitor代表监控，mymaster代表服务器的名称，可以自定义，192.168.11.128代表监控的主服务器，6379代表端口，2代表只有两个或两个以上的哨兵认为主服务器不可用的时候，才会进行failover操作。sentinel monitor mymaster 192.168.11.128 6379 2# sentinel author-pass定义服务的密码，mymaster是服务名称，123456是Redis服务器密码# sentinel auth-pass &lt;master-name&gt; &lt;password&gt;sentinel auth-pass mymaster 123456\n\n3.启动服务器和哨兵\n# 启动Redis服务器进程./redis-server ../redis.conf# 启动哨兵进程./redis-sentinel ../sentinel.conf\n\nCluster模式哨兵模式解决了主从复制不能自动故障转移，达不到高可用的问题，但还是存在难以在线扩容，Redis容量受限于单机配置的问题。\nCluster模式实现了Redis的分布式存储，即每台节点存储不同的内容，来解决在线扩容的问题\n\nCluster特点\n无中心结构：所有的redis节点彼此互联(PING-PONG机制),内部使用二进制协议优化传输速度和带宽\n\n分布式存储：Redis Cluster将数据分散存储在多个节点上，每个节点负责存储和处理其中的一部分数据。这种分布式存储方式允许集群处理更大的数据集，并提供更高的性能和可扩展性。\n\n数据复制：每个主节点都有一个或多个从节点，从节点会自动复制主节点上的数据。数据复制可以提供数据的冗余备份，并在主节点故障时自动切换到从节点，以保证系统的可用性。\n\n自动分片和故障转移：Redis Cluster会自动将数据分片到不同的节点上，同时提供自动化的故障检测和故障转移机制。当节点发生故障或下线时，集群会自动检测并进行相应的故障转移操作（投票机制：节点的fail是通过集群中超过半数的节点检测失效时才生效），以保持数据的可用性和一致性。\n\n节点间通信：Redis Cluster中的节点之间通过内部通信协议进行交互，共同协作完成数据的分片、复制和故障转移等操作。节点间通信的协议和算法确保了数据的正确性和一致性。\n\n\n工作机制\n在Redis的每个节点上，都有一个插槽（slot），取值范围为0-16383\n\n当我们存取key的时候，Redis会根据CRC16的算法得出一个结果，然后把结果对16384求余数，这样每个key都会对应一个编号在0-16383之间的哈希槽，通过这个值，去找到对应的插槽所对应的节点，然后直接自动跳转到这个对应的节点上进行存取操作\n\n为了保证高可用，Cluster模式也引入主从复制模式，一个主节点对应一个或者多个从节点，当主节点宕机的时候，就会启用从节点\n\n当其它主节点ping一个主节点A时，如果半数以上的主节点与A通信超时，那么认为主节点A宕机了。如果主节点A和它的从节点都宕机了，那么该集群就无法再提供服务了\n\n\nCluster模式集群节点最小配置6个节点(3主3从，因为需要半数以上)，其中主节点提供读写操作，从节点作为备用节点，不提供请求，只作为故障转移使用。\nCluster部署redis.conf配置：\nport 7100 # 本示例6个节点端口分别为7100,7200,7300,7400,7500,7600 daemonize yes # r后台运行 pidfile /var/run/redis_7100.pid # pidfile文件对应7100,7200,7300,7400,7500,7600 cluster-enabled yes # 开启集群模式 masterauth passw0rd # 如果设置了密码，需要指定master密码cluster-config-file nodes_7100.conf # 集群的配置文件，同样对应7100,7200等六个节点cluster-node-timeout 15000 # 请求超时 默认15秒，可自行设置 \n\n启动redis：\n[root@dev-server-1 cluster]# redis-server redis_7100.conf[root@dev-server-1 cluster]# redis-server redis_7200.conf\n\n组成集群：\nredis-cli --cluster create --cluster-replicas 1 127.0.0.1:7100 127.0.0.1:7200 127.0.0.1:7300 127.0.0.1:7400 127.0.0.1:7500 127.0.0.1:7600 -a passw0rd\n\n–cluster-replicas：表示副本数量，也就是从服务器数量，因为我们一共6个服务器，这里设置1个副本，那么Redis会收到消息，一个主服务器有一个副本从服务器，那么会计算得出：三主三从。\nCluster注意点\n数据分片和哈希槽：Redis Cluster 使用数据分片和哈希槽来实现数据的分布式存储。每个节点负责一部分哈希槽，确保数据在集群中均匀分布。在设计应用程序时，需要考虑数据的分片规则和哈希槽的分配，以便正确地将数据路由到相应的节点。\n\n节点的故障和扩展：Redis Cluster 具有高可用性和可伸缩性。当节点发生故障或需要扩展集群时，需要正确处理节点的添加和删除。故障节点会被自动检测和替换，而添加节点需要进行集群重新分片的操作。\n\n客户端的重定向：Redis Cluster 在处理键的读写操作时可能会返回重定向错误（MOVED 或 ASK）。应用程序需要正确处理这些错误，根据重定向信息更新路由表，并将操作重定向到正确的节点上。\n\n数据一致性的保证：由于 Redis Cluster 使用异步复制进行数据同步，所以在节点故障和网络分区恢复期间，可能会发生数据不一致的情况。应用程序需要考虑数据一致性的问题，并根据具体业务需求采取适当的措施。\n\n客户端连接的负载均衡：在连接 Redis Cluster 时，应该使用适当的负载均衡策略，将请求均匀地分布到集群中的各个节点上，以避免单个节点过载或出现热点访问。\n\n事务和原子性操作：Redis Cluster 中的事务操作只能在单个节点上执行，无法跨越多个节点。如果需要执行跨节点的原子性操作，可以使用 Lua 脚本来实现。\n\n集群监控和管理：对 Redis Cluster 进行监控和管理是很重要的。可以使用 Redis 自带的命令行工具或第三方监控工具来监控集群的状态、性能指标和节点健康状况，以及执行管理操作，如节点添加、删除和重新分片等。\n\n\nRedis常见问题当使用redis作为数据库的缓存层时，会经常遇见这几种问题，以下是这些问题的描述以及对应的解决方案\n缓存穿透概念：请求过来之后，访问不存在的数据，redis中查询不到，则穿透到数据库进行查询\n现象：大量穿透访问造成redis命中率下降，数据库压力飙升\n解决方案：\n\n空值缓存：如果一个查询的数据返回空，仍然把这个结果缓存到redis，以缓解数据库的查询压力\n\n布隆过滤器：布隆过滤器由一个很长的二进制数组结合n个hash算法计算出n个数组下标，将这些数据下标置为1。在查找数据时，再次通过n个hash算法计算出数组下标，如果这些下标的值为1，表示该值可能存在(存在hash冲突的原因)，如果为0，则表示该值一定不存在。因此，布隆过滤器中存在，数据不一定存在，但若布隆过滤器中不存在，则数据一定不存在，依靠此特性可以过滤掉一定的空值数据\n\n\n缓存击穿概念：请求访问的key对应的数据存在，但key在redis中已过期，则访问击穿到数据库\n现象：若大批请求中访问的key均过期，那么redis正常运行，但数据库的瞬时并发压力会飙升\n解决方案：\n\n热点数据永不过期：热点数据可以一直在redis中请求到，不会过期，则不会出现缓存击穿现象\n\n使用互斥锁：当访问redis的key过期之后，在请求数据库重新加载数据之前，先获取互斥锁（单进程可以synchronized，分布式使用分布式锁），获取到锁的请求加载数据并放进缓存，没有获取到锁的请求可以进行重试，重试之后便能重新获取到redis中的数据\n\n\n缓存雪崩概念：同一时间大批量key同时过期，造成瞬时对这些key的请求全部击穿到数据库；或redis服务不可用（宕机）\n缓存雪崩与缓存击穿的区别在于：缓存击穿是单个热点数据过期，而缓存雪崩是大批量热点数据过期\n现象：大量热点数据的查询请求会增加数据库瞬时压力\n解决方案：\n\n设置随机过期时间：避免大量key的过期时间过于集中，可以通过随机算法均匀分布key的过期时间点\n\n热点数据永不过期：可以和缓存击穿一样让热点数据不过期\n\n搭建高可用redis服务：针对redis服务不可用，可以对redis进行分布式部署，并实现故障转移（如redis哨兵模式）\n\n控制系统负载：实现熔断限流或服务降级，让系统负载在可控范围内\n\n\n大key问题概念：redis中存在占用内存空间较多的key，其中包含多种情况，如string类型的value值过大，hash类型的所有成员总值过大，zset的成员数量过大等。大key的具体值的界定，要根据实际业务情况判断。\n现象：大key对业务会产生多方面的影响：\n\nredis内存占用过高：大key可能导致内存空间不足，从而触发redis的内存淘汰策略。\n\n阻塞其他操作：对某些大key操作可能导致redis实例阻塞，例如使用Del命令删除key等。\n\n网络拥塞：大key在网络传输中更消耗带宽，可能造成机器内部网络带宽打满。\n\n主从同步延迟：大key在redis进行主从同步时也更容易导致同步延迟，影响数据一致性。\n\n\n原因：\n\n业务设计不合理：在业务设计上，没有考虑大数据量问题，导致一个key存储了大量的数据\n\n未定期清理数据：没有合适的删除机制或过期机制，造成value不断增加\n\n业务逻辑问题：业务逻辑bug导致key的value只增不减\n\n\n排查：\n\nSCAN命令：通过redis的scan命令逐步遍历数据库中的所有key，通过比较大小，站到占用内存较多的大key\n\nbigkeys参数：使用redis-cli命令客户端，连接Redis服务的时候，加上 —bigkeys 参数，可以扫描每种数据类型数量最大的key。\n\n\nredis-cli -h 127.0.0.1 -p 6379 —bigkeys\n\n\nRedis RDB Tools工具：使用开源工具Redis RDB Tools，分析RDB文件，扫描出Redis大key。\n\n例如：输出占用内存大于1kb，排名前3的keys。\nrdb —commond memory —bytes 1024 —largest 3 dump.rbd\n\n\nRedis云商提供的工具：现在基本使用云商提供的redis实例，其本身也提供一定的方法能快速定位大key\n\n解决方案：\n\n大key拆分：可以根据实际业务场景，拆分多个小key，确保value大小在合理范围内\n\n大key清理：redis4.0之后可以使用unlink命令以非阻塞方式安全的删除大key\n\n合理设置过期时间：设置过期时间可以让数据自动失效清理，一定程度避免大key的长时间存在。\n\n合理设置淘汰策略：redis中使用合适的淘汰策略，能在redis内存不足时，淘汰数据，防止大key长时间占用内存\n\n数据压缩：使用string类型，可以对value通过压缩算法进行压缩。可以用gzip，bzip2等常用算法压缩和解压。需要注意的是，这种方法会增加CPU的开销以及处理的响应延迟，同时也增加逻辑代码的复杂性\n\n\n热key问题概念：redis中某个key的访问次数比较多且明显多于其他key，则这个key被定义为热key\n现象：\n\nRedis的CPU占用过高，效率降低，影响其他业务\n\n若热key请求超出redis处理能力，会造成redis宕机，请求击穿到数据库，影响数据库性能\n\n\n原因：某个热点数据访问量暴增，如重大的热搜事件、参与秒杀的商品\n排查：\n\nhotkeys参数：Redis 4.0.3 版本中新增了 hotkeys 参数，该参数能够返回所有 key 的被访问次数（使用前提：redis淘汰策略设置为lfu）\n\nredis-cli -p 6379 --hotkeys\n\n\nMONITOR命令：MONITOR 命令是 Redis 提供的一种实时查看 Redis 的所有操作的方式，可以用于临时监控 Redis 实例的操作情况，包括读写、删除等操作。该命令对 Redis 性能的影响比较大，因此禁止长时间开启 MONITOR（生产环境中建议谨慎使用该命令）\n\n根据业务情况分析：根据实际业务场景分析，可以提前预估可能出现的热key现象，比如秒杀活动的商品数据等\n\n云商redis工具：云服务一般会提供redis的热key分析工具，合理利用，发现热key\n\n\n解决方案：\n\n热key拆分：设计一定的规则，给热key增加后缀，变成多个key，结合Redis Cluster模式，能分散到不同的节点。会带来业务复杂度，以及可能产生数据一致性问题\n\n二级缓存：在应用和redis中间再引入一层缓存层，如本地缓存，来缓解redis压力\n\n热key单独集群部署：针对热key单独做集群部署，和其他业务key进行隔离\n\n\n","categories":["总结笔记"],"tags":["redis"]},{"title":"Elasticsearch总结","url":"/2023_12_28_es/","content":"结构化数据和非结构化数据\n结构化数据：也称作行数据，是由二维表结构来逻辑表达和实现的数据，严格地遵循数据格式与长度规范，主要通过关系型数据库进行存储和管理。指具有固定格式或有限长度的数据，如数据库，元数据等。\n\n非结构化数据：又可称为全文数据，不定长或无固定格式，不适于由数据库二维表来表现，包括所有格式的办公文档、XML、HTML、Word 文档，邮件，各类报表、图片和咅频、视频信息等。\n\n说明：如果要更细致的区分的话，XML、HTML 可划分为半结构化数据。因为它们也具有自己特定的标签格式，所以既可以根据需要按结构化数据来处理，也可抽取出纯文本按非结构化数据来处理。\n\n\n数据的搜索根据两种数据分类，搜索也相应的分为两种：\n\n结构化数据搜索\n非结构化数据搜索\n\n对于结构化数据，因为它们具有特定的结构，所以我们一般都是可以通过关系型数据库（MySQL，Oracle 等）的二维表（Table）的方式存储和搜索，也可以建立索引。\n对于非结构化数据，也即对全文数据的搜索主要有两种方法：\n\n顺序扫描：\n\n通过文字名称也可了解到它的大概搜索方式，即按照顺序扫描的方式查询特定的关键字。\n例如给你一张报纸，让你找到该报纸中“平安”的文字在哪些地方出现过。你肯定需要从头到尾把报纸阅读扫描一遍然后标记出关键字在哪些版块出现过以及它的出现位置。\n这种方式无疑是最耗时的最低效的，如果报纸排版字体小，而且版块较多甚至有多份报纸，等你扫描完你的眼睛也差不多了。\n\n全文搜索：\n\n对非结构化数据顺序扫描很慢，我们是否可以进行优化？把我们的非结构化数据想办法弄得有一定结构不就行了吗？\n将非结构化数据中的一部分信息提取出来，重新组织，使其变得有一定结构，然后对此有一定结构的数据进行搜索，从而达到搜索相对较快的目的。\n这种方式就构成了全文检索的基本思路。这部分从非结构化数据中提取出的然后重新组织的信息，我们称之为索引。\n这种方式的主要工作量在前期索引的创建，但是对于后期搜索却是快速高效的。\nLucene简介通过对生活中数据的类型作了一个简短了解之后，我们知道关系型数据库的 SQL 检索是处理不了这种非结构化数据的。\n这种非结构化数据的处理需要依赖全文搜索，而目前市场上开放源代码的最好全文检索引擎工具包就属于 Apache 的 Lucene了。\n但是 Lucene 只是一个工具包，它不是一个完整的全文检索引擎。Lucene 的目的是为软件开发人员提供一个简单易用的工具包，以方便的在目标系统中实现全文检索的功能，或者是以此为基础建立起完整的全文检索引擎。\n目前以 Lucene 为基础建立的开源可用全文搜索引擎主要是 Solr 和 Elasticsearch。\nSolr 和 Elasticsearch 都是比较成熟的全文搜索引擎，能完成的功能和性能也基本一样。\n但是 ES 本身就具有分布式的特性和易安装使用的特点，而 Solr 的分布式需要借助第三方来实现，例如通过使用 ZooKeeper 来达到分布式协调管理。\n不管是 Solr 还是 Elasticsearch 底层都是依赖于 Lucene，而 Lucene 能实现全文搜索主要是因为它实现了倒排索引的查询结构。\n如何理解倒排索引呢？假如现有三份数据文档，文档的内容如下分别是：\nJava is the best programming language.PHP is the best programming language.Javascript is the best programming language.\n\n为了创建倒排索引，我们通过分词器将每个文档的内容域拆分成单独的词（我们称它为词条或 Term），创建一个包含所有不重复词条的排序列表，然后列出每个词条出现在哪个文档。\n结果如下所示：\nTerm          Doc_1    Doc_2   Doc_3-------------------------------------Java        |   X   |        |is          |   X   |   X    |   Xthe         |   X   |   X    |   Xbest        |   X   |   X    |   Xprogramming |   x   |   X    |   Xlanguage    |   X   |   X    |   XPHP         |       |   X    |Javascript  |       |        |   X-------------------------------------\n\n这种结构由文档中所有不重复词的列表构成，对于其中每个词都有一个文档列表与之关联。\n这种由属性值来确定记录的位置的结构就是倒排索引。带有倒排索引的文件我们称为倒排文件。\n我们将上面的内容转换为图的形式来说明倒排索引的结构信息，如下图所示：\n其中主要有如下几个核心术语需要理解：\n\n词条（Term）：索引里面最小的存储和查询单元，对于英文来说是一个单词，对于中文来说一般指分词后的一个词。\n\n词典（Term Dictionary）：或字典，是词条 Term 的集合。搜索引擎的通常索引单位是单词，单词词典是由文档集合中出现过的所有单词构成的字符串集合，单词词典内每条索引项记载单词本身的一些信息以及指向“倒排列表”的指针。\n\n倒排表（Post list）：一个文档通常由多个词组成，倒排表记录的是某个词在哪些文档里出现过以及出现的位置。每条记录称为一个倒排项（Posting）。倒排表记录的不单是文档编号，还存储了词频等信息。\n\n倒排文件（Inverted File）：所有单词的倒排列表往往顺序地存储在磁盘的某个文件里，这个文件被称之为倒排文件，倒排文件是存储倒排索引的物理文件。\n\n\n从上图我们可以了解到倒排索引主要由两个部分组成：\n\n词典\n倒排文件\n\n词典和倒排表是 Lucene 中很重要的两种数据结构，是实现快速检索的重要基石。词典和倒排文件是分两部分存储的，词典在内存中而倒排文件存储在磁盘上。\nES 核心概念ES 是使用 Java 编写的一种开源搜索引擎，它在内部使用 Lucene 做索引与搜索，通过对 Lucene 的封装，隐藏了 Lucene 的复杂性，取而代之的提供一套简单一致的 RESTful API。\n然而，Elasticsearch 不仅仅是 Lucene，并且也不仅仅只是一个全文搜索引擎。 \n它可以被下面这样准确的形容：\n\n一个分布式的实时文档存储，每个字段可以被索引与搜索。\n\n一个分布式实时分析搜索引擎。\n\n\n能胜任上百个服务节点的扩展，并支持 PB 级别的结构化或者非结构化数据。\n官网对 Elasticsearch 的介绍是 Elasticsearch 是一个分布式、可扩展、近实时的搜索与数据分析引擎。\nES 集群（Cluster）ES 的集群搭建很简单，不需要依赖第三方协调管理组件，自身内部就实现了集群的管理功能。\nES 集群由一个或多个 Elasticsearch 节点组成，每个节点配置相同的 cluster.name 即可加入集群，默认值为 “elasticsearch”。\n确保不同的环境中使用不同的集群名称，否则最终会导致节点加入错误的集群。\n一个 Elasticsearch 服务启动实例就是一个节点（Node）。节点通过 node.name 来设置节点名称，如果不设置则在启动时给节点分配一个随机通用唯一标识符作为名称。\n发现机制那么有一个问题，ES 内部是如何通过一个相同的设置 cluster.name 就能将不同的节点连接到同一个集群的？答案是 Zen Discovery。\nZen Discovery 是 Elasticsearch 的内置默认发现模块（发现模块的职责是发现集群中的节点以及选举 Master 节点）。\n它提供单播和基于文件的发现，并且可以扩展为通过插件支持云环境和其他形式的发现。\nZen Discovery 与其他模块集成，例如，节点之间的所有通信都使用 Transport 模块完成。节点使用发现机制通过 Ping 的方式查找其他节点。\nElasticsearch 默认被配置为使用单播发现，以防止节点无意中加入集群。只有在同一台机器上运行的节点才会自动组成集群。\n如果集群的节点运行在不同的机器上，使用单播，你可以为 Elasticsearch 提供一些它应该去尝试连接的节点列表。\n当一个节点联系到单播列表中的成员时，它就会得到整个集群所有节点的状态，然后它会联系 Master 节点，并加入集群。\n这意味着单播列表不需要包含集群中的所有节点， 它只是需要足够的节点，当一个新节点联系上其中一个并且说上话就可以了。\n如果你使用 Master 候选节点作为单播列表，你只要列出三个就可以了。这个配置在 elasticsearch.yml 文件中：\ndiscovery.zen.ping.unicast.hosts: [&quot;host1&quot;, &quot;host2:port&quot;]\n\n节点启动后先 Ping ，如果 discovery.zen.ping.unicast.hosts 有设置，则 Ping 设置中的 Host ，否则尝试 ping localhost 的几个端口。\nElasticsearch 支持同一个主机启动多个节点，Ping 的 Response 会包含该节点的基本信息以及该节点认为的 Master 节点。\n选举开始，先从各节点认为的 Master 中选，规则很简单，按照 ID 的字典序排序，取第一个。如果各节点都没有认为的 Master ，则从所有节点中选择，规则同上。\n这里有个限制条件就是 discovery.zen.minimum_master_nodes ，如果节点数达不到最小值的限制，则循环上述过程，直到节点数足够可以开始选举。\n最后选举结果是肯定能选举出一个 Master ，如果只有一个 Local 节点那就选出的是自己。\n如果当前节点是 Master ，则开始等待节点数达到 discovery.zen.minimum_master_nodes，然后提供服务。\n如果当前节点不是 Master ，则尝试加入 Master 。Elasticsearch 将以上服务发现以及选主的流程叫做 Zen Discovery 。\n由于它支持任意数目的集群（ 1- N ），所以不能像 Zookeeper 那样限制节点必须是奇数，也就无法用投票的机制来选主，而是通过一个规则。\n只要所有的节点都遵循同样的规则，得到的信息都是对等的，选出来的主节点肯定是一致的。\n但分布式系统的问题就出在信息不对等的情况，这时候很容易出现脑裂（Split-Brain）的问题。\n大多数解决方案就是设置一个 Quorum 值，要求可用节点必须大于 Quorum（一般是超过半数节点），才能对外提供服务。\n而 Elasticsearch 中，这个 Quorum 的配置就是 discovery.zen.minimum_master_nodes 。\n节点的角色每个节点既可以是候选主节点也可以是数据节点，通过在配置文件 ..&#x2F;config&#x2F;elasticsearch.yml 中设置即可，默认都为 true。\nnode.master: true  //是否候选主节点node.data: true    //是否数据节点\n\n数据节点负责数据的存储和相关的操作，例如对数据进行增、删、改、查和聚合等操作，所以数据节点（Data 节点）对机器配置要求比较高，对 CPU、内存和 I&#x2F;O 的消耗很大。\n通常随着集群的扩大，需要增加更多的数据节点来提高性能和可用性。\n候选主节点可以被选举为主节点（Master 节点），集群中只有候选主节点才有选举权和被选举权，其他节点不参与选举的工作。\n主节点负责创建索引、删除索引、跟踪哪些节点是群集的一部分，并决定哪些分片分配给相关的节点、追踪集群中节点的状态等，稳定的主节点对集群的健康是非常重要的。\n\n一个节点既可以是候选主节点也可以是数据节点，但是由于数据节点对 CPU、内存核 I&#x2F;O 消耗都很大。\n所以如果某个节点既是数据节点又是主节点，那么可能会对主节点产生影响从而对整个集群的状态产生影响。\n因此为了提高集群的健康性，我们应该对 Elasticsearch 集群中的节点做好角色上的划分和隔离。可以使用几个配置较低的机器群作为候选主节点群。\n主节点和其他节点之间通过 Ping 的方式互检查，主节点负责 Ping 所有其他节点，判断是否有节点已经挂掉。其他节点也通过 Ping 的方式判断主节点是否处于可用状态。\n虽然对节点做了角色区分，但是用户的请求可以发往任何一个节点，并由该节点负责分发请求、收集结果等操作，而不需要主节点转发。\n这种节点可称之为协调节点，协调节点是不需要指定和配置的，集群中的任何节点都可以充当协调节点的角色。\n脑裂现象同时如果由于网络或其他原因导致集群中选举出多个 Master 节点，使得数据更新时出现不一致，这种现象称之为脑裂，即集群中不同的节点对于 Master 的选择出现了分歧，出现了多个 Master 竞争。\n“脑裂”问题可能有以下几个原因造成：\n\n网络问题：集群间的网络延迟导致一些节点访问不到 Master，认为 Master 挂掉了从而选举出新的 Master，并对 Master 上的分片和副本标红，分配新的主分片。\n\n节点负载：主节点的角色既为 Master 又为 Data，访问量较大时可能会导致 ES 停止响应（假死状态）造成大面积延迟，此时其他节点得不到主节点的响应认为主节点挂掉了，会重新选取主节点。\n\n内存回收：主节点的角色既为 Master 又为 Data，当 Data 节点上的 ES 进程占用的内存较大，引发 JVM 的大规模内存回收，造成 ES 进程失去响应。\n\n\n为了避免脑裂现象的发生，我们可以从原因着手通过以下几个方面来做出优化措施：\n\n适当调大响应时间，减少误判。\n\n通过参数 discovery.zen.ping_timeout 设置节点状态的响应时间，默认为 3s，可以适当调大。如果 Master 在该响应时间的范围内没有做出响应应答，判断该节点已经挂掉了。调大参数（如 6s，discovery.zen.ping_timeout:6），可适当减少误判。\n\n\n选举触发。\n\n我们需要在候选集群中的节点的配置文件中设置参数 discovery.zen.munimum_master_nodes 的值。这个参数表示在选举主节点时需要参与选举的候选主节点的节点数，默认值是 1，官方建议取值(master_eligibel_nodes&#x2F;2)+1，其中 master_eligibel_nodes 为候选主节点的个数。这样做既能防止脑裂现象的发生，也能最大限度地提升集群的高可用性，因为只要不少于 discovery.zen.munimum_master_nodes 个候选节点存活，选举工作就能正常进行。当小于这个值的时候，无法触发选举行为，集群无法使用，不会造成分片混乱的情况。\n\n\n角色分离。\n\n即是上面我们提到的候选主节点和数据节点进行角色分离，这样可以减轻主节点的负担，防止主节点的假死状态发生，减少对主节点“已死”的误判。\n\n\n\n分片（Shards）ES 支持 PB 级全文搜索，当索引上的数据量太大的时候，ES 通过水平拆分的方式将一个索引上的数据拆分出来分配到不同的数据块上，拆分出来的数据库块称之为一个分片。\n这类似于 MySQL 的分库分表，只不过 MySQL 分库分表需要借助第三方组件而 ES 内部自身实现了此功能。\n在一个多分片的索引中写入数据时，通过路由来确定具体写入哪一个分片中，所以在创建索引的时候需要指定分片的数量，并且分片的数量一旦确定就不能修改。\n分片的数量和下面介绍的副本数量都是可以通过创建索引时的 Settings 来配置，ES 默认为一个索引创建 5 个主分片, 并分别为每个分片创建一个副本。\nPUT /myIndex&#123;   &quot;settings&quot; : &#123;      &quot;number_of_shards&quot; : 5,      &quot;number_of_replicas&quot; : 1   &#125;&#125;\n\nES 通过分片的功能使得索引在规模上和性能上都得到提升，每个分片都是 Lucene 中的一个索引文件，每个分片必须有一个主分片和零到多个副本。\n副本（Replicas）副本就是对分片的 Copy，每个主分片都有一个或多个副本分片，当主分片异常时，副本可以提供数据的查询等操作。\n主分片和对应的副本分片是不会在同一个节点上的，所以副本分片数的最大值是 N-1（其中 N 为节点数）。\n对文档的新建、索引和删除请求都是写操作，必须在主分片上面完成之后才能被复制到相关的副本分片。\nES 为了提高写入的能力这个过程是并发写的，同时为了解决并发写的过程中数据冲突的问题，ES 通过乐观锁的方式控制，每个文档都有一个 _version （版本）号，当文档被修改时版本号递增。\n一旦所有的副本分片都报告写成功才会向协调节点报告成功，协调节点向客户端报告成功。\n\n从上图可以看出为了达到高可用，Master 节点会避免将主分片和副本分片放在同一个节点上。\n假设这时节点 Node1 服务宕机了或者网络不可用了，那么主节点上主分片 S0 也就不可用了。\n幸运的是还存在另外两个节点能正常工作，这时 ES 会重新选举新的主节点，而且这两个节点上存在我们所需要的 S0 的所有数据。\n我们会将 S0 的副本分片提升为主分片，这个提升主分片的过程是瞬间发生的。此时集群的状态将会为  Yellow。\n为什么我们集群状态是 Yellow 而不是 Green 呢？虽然我们拥有所有的 2 个主分片，但是同时设置了每个主分片需要对应两份副本分片，而此时只存在一份副本分片。所以集群不能为 Green 的状态。\n如果我们同样关闭了 Node2 ，我们的程序依然可以保持在不丢失任何数据的情况下运行，因为 Node3 为每一个分片都保留着一份副本。\n如果我们重新启动 Node1 ，集群可以将缺失的副本分片再次进行分配，那么集群的状态又将恢复到原来的正常状态。\n如果 Node1 依然拥有着之前的分片，它将尝试去重用它们，只不过这时 Node1 节点上的分片不再是主分片而是副本分片了，如果期间有更改的数据只需要从主分片上复制修改的数据文件即可。\n小结将数据分片是为了提高可处理数据的容量和易于进行水平扩展，为分片做副本是为了提高集群的稳定性和提高并发量。\n副本是乘法，越多消耗越大，但也越保险。分片是除法，分片越多，单分片数据就越少也越分散。\n副本越多，集群的可用性就越高，但是由于每个分片都相当于一个 Lucene 的索引文件，会占用一定的文件句柄、内存及 CPU。\n并且分片间的数据同步也会占用一定的网络带宽，所以索引的分片数和副本数也不是越多越好。\n映射（Mapping）映射是用于定义 ES 对索引中字段的存储类型、分词方式和是否存储等信息，就像数据库中的 Schema ，描述了文档可能具有的字段或属性、每个字段的数据类型。\n只不过关系型数据库建表时必须指定字段类型，而 ES 对于字段类型可以不指定然后动态对字段类型猜测，也可以在创建索引时具体指定字段的类型。\n对字段类型根据数据格式自动识别的映射称之为动态映射（Dynamic Mapping），我们创建索引时具体定义字段类型的映射称之为静态映射或显示映射（Explicit Mapping）。\n在讲解动态映射和静态映射的使用前，我们先来了解下 ES 中的数据有哪些字段类型？之后我们再讲解为什么我们创建索引时需要建立静态映射而不使用动态映射。\nES（v6.8）中字段数据类型主要有以下几类：\n\nText 用于索引全文值的字段，例如电子邮件正文或产品说明。这些字段是被分词的，它们通过分词器传递 ，以在被索引之前将字符串转换为单个术语的列表。\n分析过程允许 Elasticsearch 搜索单个单词中每个完整的文本字段。文本字段不用于排序，很少用于聚合。\nKeyword 用于索引结构化内容的字段，例如电子邮件地址，主机名，状态代码，邮政编码或标签。它们通常用于过滤，排序，和聚合。Keyword 字段只能按其确切值进行搜索。\n通过对字段类型的了解我们知道有些字段需要明确定义的，例如某个字段是 Text 类型还是 Keyword 类型差别是很大的，时间字段也许我们需要指定它的时间格式，还有一些字段我们需要指定特定的分词器等等。\n如果采用动态映射是不能精确做到这些的，自动识别常常会与我们期望的有些差异。\n所以创建索引的时候一个完整的格式应该是指定分片和副本数以及 Mapping 的定义，如下：\nPUT my_index &#123;   &quot;settings&quot; : &#123;      &quot;number_of_shards&quot; : 5,      &quot;number_of_replicas&quot; : 1   &#125;  &quot;mappings&quot;: &#123;    &quot;_doc&quot;: &#123;       &quot;properties&quot;: &#123;         &quot;title&quot;:    &#123; &quot;type&quot;: &quot;text&quot;  &#125;,         &quot;name&quot;:     &#123; &quot;type&quot;: &quot;text&quot;  &#125;,         &quot;age&quot;:      &#123; &quot;type&quot;: &quot;integer&quot; &#125;,          &quot;created&quot;:  &#123;          &quot;type&quot;:   &quot;date&quot;,           &quot;format&quot;: &quot;strict_date_optional_time||epoch_millis&quot;        &#125;      &#125;    &#125;  &#125;&#125;\n\nES 的基本使用在决定使用 Elasticsearch 的时候首先要考虑的是版本问题，Elasticsearch （排除 0.x 和 1.x）目前有如下常用的稳定的主版本：2.x，5.x，6.x，7.x（current）。\n你可能会发现没有 3.x 和 4.x，ES 从 2.4.6 直接跳到了 5.0.0。其实是为了 ELK（ElasticSearch，Logstash，Kibana）技术栈的版本统一，免的给用户带来混乱。\n在 Elasticsearch 是 2.x （2.x 的最后一版 2.4.6 的发布时间是 July 25, 2017） 的情况下，Kibana 已经是 4.x（Kibana 4.6.5 的发布时间是 July 25, 2017）。\n那么在 Kibana 的下一主版本肯定是 5.x 了，所以 Elasticsearch 直接将自己的主版本发布为 5.0.0 了。\n统一之后，我们选版本就不会犹豫困惑了，我们选定 Elasticsearch 的版本后再选择相同版本的 Kibana 就行了，不用担忧版本不兼容的问题。\nElasticsearch 是使用 Java 构建，所以除了注意 ELK 技术的版本统一，我们在选择 Elasticsearch 的版本的时候还需要注意 JDK 的版本。\n因为每个大版本所依赖的 JDK 版本也不同，目前 7.2 版本已经可以支持 JDK11。\n解压安装\n\n下载和解压 Elasticsearch，无需安装解压后即可用，解压后目录如上图：\n\n\nbin：二进制系统指令目录，包含启动命令和安装插件命令等。\n\nconfig：配置文件目录。\n\nlib：依赖包目录。\n\nlogs：日志文件目录。\n\nmodules：模块库，例如 x-pack 的模块。\n\nplugins：插件目录。\n\n\n\n安装目录下运行 bin&#x2F;elasticsearch 来启动 ES。\n\n默认在 9200 端口运行，请求 curl http://localhost:9200/ 或者浏览器输入 http://localhost:9200，得到一个 JSON 对象，其中包含当前节点、集群、版本等信息。\n\n\n&#123;  &quot;name&quot; : &quot;U7fp3O9&quot;,  &quot;cluster_name&quot; : &quot;elasticsearch&quot;,  &quot;cluster_uuid&quot; : &quot;-Rj8jGQvRIelGd9ckicUOA&quot;,  &quot;version&quot; : &#123;    &quot;number&quot; : &quot;6.8.1&quot;,    &quot;build_flavor&quot; : &quot;default&quot;,    &quot;build_type&quot; : &quot;zip&quot;,    &quot;build_hash&quot; : &quot;1fad4e1&quot;,    &quot;build_date&quot; : &quot;2019-06-18T13:16:52.517138Z&quot;,    &quot;build_snapshot&quot; : false,    &quot;lucene_version&quot; : &quot;7.7.0&quot;,    &quot;minimum_wire_compatibility_version&quot; : &quot;5.6.0&quot;,    &quot;minimum_index_compatibility_version&quot; : &quot;5.0.0&quot;  &#125;,  &quot;tagline&quot; : &quot;You Know, for Search&quot;&#125;\ndocker安装# 创建网络、拉镜像，启动，测试验证docker network create es-netdocker network lsdocker pull elasticsearch:7.12.1docker run --network es-net -p 9200:9200 -p 9300:9300 -e &quot;discovery.type=single-node&quot; -e ES_JAVA_OPTS=&quot;-Xms512m -Xmx512m&quot; --name=&quot;es&quot; --cpuset-cpus=&quot;1&quot; -v es-plugins:/usr/share/elasticsearch/plugins --privileged -d elasticsearch:7.12.1http://192.168.134.132:9200/# kibana安装docker run -d --name kibana -e ELASTICSEARCH_HOSTS=http://es:9200 --network=es-net -p 5601:5601 kibana:7.12.1\n\n集群健康状态要检查群集运行状况，我们可以在 Kibana 控制台中运行以下命令 GET &#x2F;_cluster&#x2F;health，得到如下信息：\n&#123;  &quot;cluster_name&quot; : &quot;docker-cluster&quot;,  &quot;status&quot; : &quot;yellow&quot;,  &quot;timed_out&quot; : false,  &quot;number_of_nodes&quot; : 1,  &quot;number_of_data_nodes&quot; : 1,  &quot;active_primary_shards&quot; : 11,  &quot;active_shards&quot; : 11,  &quot;relocating_shards&quot; : 0,  &quot;initializing_shards&quot; : 0,  &quot;unassigned_shards&quot; : 1,  &quot;delayed_unassigned_shards&quot; : 0,  &quot;number_of_pending_tasks&quot; : 0,  &quot;number_of_in_flight_fetch&quot; : 0,  &quot;task_max_waiting_in_queue_millis&quot; : 0,  &quot;active_shards_percent_as_number&quot; : 91.66666666666666&#125;\n\n集群状态通过 绿，黄，红 来标识：\n\n绿色：集群健康完好，一切功能齐全正常，所有分片和副本都可以正常工作。\n\n黄色：预警状态，所有主分片功能正常，但至少有一个副本是不能正常工作的。此时集群是可以正常工作的，但是高可用性在某种程度上会受影响。\n\n红色：集群不可正常使用。某个或某些分片及其副本异常不可用，这时集群的查询操作还能执行，但是返回的结果会不准确。对于分配到这个分片的写入请求将会报错，最终会导致数据的丢失。\n\n\n当集群状态为红色时，它将会继续从可用的分片提供搜索请求服务，但是你需要尽快修复那些未分配的分片。\nES 机制原理ES 的基本概念和基本操作介绍完了之后，我们可能还有很多疑惑：\n\n它们内部是如何运行的？\n\n主分片和副本分片是如何同步的？\n\n创建索引的流程是什么样的？\n\nES 如何将索引数据分配到不同的分片上的？以及这些索引数据是如何存储的？\n\n为什么说 ES 是近实时搜索引擎而文档的 CRUD (创建-读取-更新-删除) 操作是实时的？\n\n以及 Elasticsearch 是怎样保证更新被持久化在断电时也不丢失数据？\n\n还有为什么删除文档不会立刻释放空间？\n\n\n带着这些疑问我们进入接下来的内容。\n写索引原理下图描述了 3 个节点的集群，共拥有 12 个分片，其中有 4 个主分片（S0、S1、S2、S3）和 8 个副本分片（R0、R1、R2、R3），每个主分片对应两个副本分片，节点 1 是主节点（Master 节点）负责整个集群的状态。\n\n写索引是只能写在主分片上，然后同步到副本分片。这里有四个主分片，一条数据 ES 是根据什么规则写到特定分片上的呢？\n这条索引数据为什么被写到 S0 上而不写到 S1 或 S2 上？那条数据为什么又被写到 S3 上而不写到 S0 上了？\n首先这肯定不会是随机的，否则将来要获取文档的时候我们就不知道从何处寻找了。\n实际上，这个过程是根据下面这个公式决定的：\nshard = hash(routing) % number_of_primary_shards\n\nRouting 是一个可变值，默认是文档的 _id ，也可以设置成一个自定义的值。\nRouting 通过 Hash 函数生成一个数字，然后这个数字再除以 number_of_primary_shards （主分片的数量）后得到余数。\n这个在 0 到 number_of_primary_shards-1 之间的余数，就是我们所寻求的文档所在分片的位置。\n这就解释了为什么我们要在创建索引的时候就确定好主分片的数量并且永远不会改变这个数量：因为如果数量变化了，那么所有之前路由的值都会无效，文档也再也找不到了。\n由于在 ES 集群中每个节点通过上面的计算公式都知道集群中的文档的存放位置，所以每个节点都有处理读写请求的能力。\n在一个写请求被发送到某个节点后，该节点即为前面说过的协调节点，协调节点会根据路由公式计算出需要写到哪个分片上，再将请求转发到该分片的主分片节点上。\n\n假如此时数据通过路由计算公式取余后得到的值是 shard&#x3D;hash(routing)%4&#x3D;0。\n则具体流程如下：\n\n客户端向 ES1 节点（协调节点）发送写请求，通过路由计算公式得到值为 0，则当前数据应被写到主分片 S0 上。\n\nES1 节点将请求转发到 S0 主分片所在的节点 ES3，ES3 接受请求并写入到磁盘。\n\n并发将数据复制到两个副本分片 R0 上，其中通过乐观并发控制数据的冲突。一旦所有的副本分片都报告成功，则节点 ES3 将向协调节点报告成功，协调节点向客户端报告成功。\n\n\n存储原理上面介绍了在 ES 内部索引的写处理流程，这个流程是在 ES 的内存中执行的，数据被分配到特定的分片和副本上之后，最终是存储到磁盘上的，这样在断电的时候就不会丢失数据。\n具体的存储路径可在配置文件 ..&#x2F;config&#x2F;elasticsearch.yml 中进行设置，默认存储在安装目录的 Data 文件夹下。\n建议不要使用默认值，因为若 ES 进行了升级，则有可能导致数据全部丢失：\npath.data: /path/to/data  //索引数据path.logs: /path/to/logs  //日志记录\n\n分段存储索引文档以段的形式存储在磁盘上，何为段？索引文件被拆分为多个子文件，则每个子文件叫作段，每一个段本身都是一个倒排索引，并且段具有不变性，一旦索引的数据被写入硬盘，就不可再修改。\n在底层采用了分段的存储模式，使它在读写时几乎完全避免了锁的出现，大大提升了读写性能。\n段被写入到磁盘后会生成一个提交点，提交点是一个用来记录所有提交后段信息的文件。\n一个段一旦拥有了提交点，就说明这个段只有读的权限，失去了写的权限。相反，当段在内存中时，就只有写的权限，而不具备读数据的权限，意味着不能被检索。\n段的概念提出主要是因为：在早期全文检索中为整个文档集合建立了一个很大的倒排索引，并将其写入磁盘中。\n如果索引有更新，就需要重新全量创建一个索引来替换原来的索引。这种方式在数据量很大时效率很低，并且由于创建一次索引的成本很高，所以对数据的更新不能过于频繁，也就不能保证时效性。\n索引文件分段存储并且不可修改，那么新增、更新和删除如何处理呢？\n\n新增，新增很好处理，由于数据是新的，所以只需要对当前文档新增一个段就可以了。\n\n删除，由于不可修改，所以对于删除操作，不会把文档从旧的段中移除而是通过新增一个 .del 文件，文件中会列出这些被删除文档的段信息。这个被标记删除的文档仍然可以被查询匹配到， 但它会在最终结果被返回前从结果集中移除。\n\n更新，不能修改旧的段来进行反映文档的更新，其实更新相当于是删除和新增这两个动作组成。会将旧的文档在 .del 文件中标记删除，然后文档的新版本被索引到一个新的段中。可能两个版本的文档都会被一个查询匹配到，但被删除的那个旧版本文档在结果集返回前就会被移除。\n\n\n段被设定为不可修改具有一定的优势也有一定的缺点，优势主要表现在：\n\n不需要锁。如果你从来不更新索引，你就不需要担心多进程同时修改数据的问题。\n\n一旦索引被读入内核的文件系统缓存，便会留在哪里，由于其不变性。只要文件系统缓存中还有足够的空间，那么大部分读请求会直接请求内存，而不会命中磁盘。这提供了很大的性能提升。\n\n其它缓存(像 Filter 缓存)，在索引的生命周期内始终有效。它们不需要在每次数据改变时被重建，因为数据不会变化。\n\n写入单个大的倒排索引允许数据被压缩，减少磁盘 I&#x2F;O 和需要被缓存到内存的索引的使用量。\n\n\n段的不变性的缺点如下：\n\n当对旧数据进行删除时，旧数据不会马上被删除，而是在 .del 文件中被标记为删除。而旧数据只能等到段更新时才能被移除，这样会造成大量的空间浪费。\n\n若有一条数据频繁的更新，每次更新都是新增新的标记旧的，则会有大量的空间浪费。\n\n每次新增数据时都需要新增一个段来存储数据。当段的数量太多时，对服务器的资源例如文件句柄的消耗会非常大。\n\n在查询的结果中包含所有的结果集，需要排除被标记删除的旧数据，这增加了查询的负担。\n\n\n延迟写策略介绍完了存储的形式，那么索引写入到磁盘的过程是怎样的？是否是直接调 Fsync 物理性地写入磁盘？\n答案是显而易见的，如果是直接写入到磁盘上，磁盘的 I&#x2F;O 消耗上会严重影响性能。\n那么当写数据量大的时候会造成 ES 停顿卡死，查询也无法做到快速响应。如果真是这样 ES 也就不会称之为近实时全文搜索引擎了。\n为了提升写的性能，ES 并没有每新增一条数据就增加一个段到磁盘上，而是采用延迟写的策略。\n每当有新增的数据时，就将其先写入到内存中，在内存和磁盘之间是文件系统缓存。\n当达到默认的时间（1 秒钟）或者内存的数据达到一定量时，会触发一次刷新（Refresh），将内存中的数据生成到一个新的段上并缓存到文件缓存系统 上，稍后再被刷新到磁盘中并生成提交点。\n这里的内存使用的是 ES 的 JVM 内存，而文件缓存系统使用的是操作系统的内存。\n新的数据会继续的被写入内存，但内存中的数据并不是以段的形式存储的，因此不能提供检索功能。\n由内存刷新到文件缓存系统的时候会生成新的段，并将段打开以供搜索使用，而不需要等到被刷新到磁盘。\n在 Elasticsearch 中，写入和打开一个新段的轻量的过程叫做 Refresh （即内存刷新到文件缓存系统）。\n默认情况下每个分片会每秒自动刷新一次。这就是为什么我们说 Elasticsearch 是近实时搜索，因为文档的变化并不是立即对搜索可见，但会在一秒之内变为可见。\n我们也可以手动触发 Refresh，POST &#x2F;_refresh 刷新所有索引，POST &#x2F;nba&#x2F;_refresh 刷新指定的索引。\nTips：尽管刷新是比提交轻量很多的操作，它还是会有性能开销。当写测试的时候， 手动刷新很有用，但是不要在生产&gt;环境下每次索引一个文档都去手动刷新。而且并不是所有的情况都需要每秒刷新。\n可能你正在使用 Elasticsearch 索引大量的日志文件， 你可能想优化索引速度而不是&gt;近实时搜索。\n这时可以在创建索引时在 Settings 中通过调大 refresh_interval &#x3D; “30s” 的值 ， 降低每个索引的刷新频率，设值时需要注意后面带上时间单位，否则默认是毫秒。当 refresh_interval&#x3D;-1 时表示关闭索引的自动刷新。\n虽然通过延时写的策略可以减少数据往磁盘上写的次数提升了整体的写入能力，但是我们知道文件缓存系统也是内存空间，属于操作系统的内存，只要是内存都存在断电或异常情况下丢失数据的危险。\n为了避免丢失数据，Elasticsearch 添加了事务日志（Translog），事务日志记录了所有还没有持久化到磁盘的数据。\n\n添加了事务日志后整个写索引的流程如上图所示：\n\n一个新文档被索引之后，先被写入到内存中，但是为了防止数据的丢失，会追加一份数据到事务日志中。不断有新的文档被写入到内存，同时也都会记录到事务日志中。这时新数据还不能被检索和查询。\n\n当达到默认的刷新时间或内存中的数据达到一定量后，会触发一次  Refresh，将内存中的数据以一个新段形式刷新到文件缓存系统中并清空内存。这时虽然新段未被提交到磁盘，但是可以提供文档的检索功能且不能被修改。\n\n随着新文档索引不断被写入，当日志数据大小超过 512M 或者时间超过 30 分钟时，会触发一次 Flush。内存中的数据被写入到一个新段同时被写入到文件缓存系统，文件系统缓存中数据通过 Fsync 刷新到磁盘中，生成提交点，日志文件被删除，创建一个空的新日志。\n\n\n通过这种方式当断电或需要重启时，ES 不仅要根据提交点去加载已经持久化过的段，还需要工具 Translog 里的记录，把未持久化的数据重新持久化到磁盘上，避免了数据丢失的可能。\n段合并由于自动刷新流程每秒会创建一个新的段 ，这样会导致短时间内的段数量暴增。而段数目太多会带来较大的麻烦。\n每一个段都会消耗文件句柄、内存和 CPU 运行周期。更重要的是，每个搜索请求都必须轮流检查每个段然后合并查询结果，所以段越多，搜索也就越慢。\nElasticsearch 通过在后台定期进行段合并来解决这个问题。小的段被合并到大的段，然后这些大的段再被合并到更大的段。\n段合并的时候会将那些旧的已删除文档从文件系统中清除。被删除的文档不会被拷贝到新的大段中。合并的过程中不会中断索引和搜索。\n\n段合并在进行索引和搜索时会自动进行，合并进程选择一小部分大小相似的段，并且在后台将它们合并到更大的段中，这些段既可以是未提交的也可以是已提交的。\n合并结束后老的段会被删除，新的段被 Flush 到磁盘，同时写入一个包含新段且排除旧的和较小的段的新提交点，新的段被打开可以用来搜索。\n段合并的计算量庞大， 而且还要吃掉大量磁盘 I&#x2F;O，段合并会拖累写入速率，如果任其发展会影响搜索性能。\nElasticsearch 在默认情况下会对合并流程进行资源限制，所以搜索仍然有足够的资源很好地执行。\n性能优化存储设备磁盘在现代服务器上通常都是瓶颈。Elasticsearch 重度使用磁盘，你的磁盘能处理的吞吐量越大，你的节点就越稳定。\n这里有一些优化磁盘 I&#x2F;O 的技巧：\n\n使用 SSD。就像其他地方提过的， 他们比机械磁盘优秀多了。\n\n使用 RAID 0。条带化 RAID 会提高磁盘 I&#x2F;O，代价显然就是当一块硬盘故障时整个就故障了。不要使用镜像或者奇偶校验 RAID 因为副本已经提供了这个功能。\n\n另外，使用多块硬盘，并允许 Elasticsearch 通过多个 path.data 目录配置把数据条带化分配到它们上面。\n\n不要使用远程挂载的存储，比如 NFS 或者 SMB&#x2F;CIFS。这个引入的延迟对性能来说完全是背道而驰的。\n\n如果你用的是 EC2，当心 EBS。即便是基于 SSD 的 EBS，通常也比本地实例的存储要慢。\n\n\n内部索引优化\nElasticsearch 为了能快速找到某个 Term，先将所有的 Term 排个序，然后根据二分法查找 Term，时间复杂度为 logN，就像通过字典查找一样，这就是 Term Dictionary。\n现在再看起来，似乎和传统数据库通过 B-Tree 的方式类似。但是如果 Term 太多，Term Dictionary 也会很大，放内存不现实，于是有了 Term Index。\n就像字典里的索引页一样，A 开头的有哪些 Term，分别在哪页，可以理解 Term Index是一棵树。\n这棵树不会包含所有的 Term，它包含的是 Term 的一些前缀。通过 Term Index 可以快速地定位到 Term Dictionary 的某个 Offset，然后从这个位置再往后顺序查找。\n在内存中用 FST 方式压缩 Term Index，FST 以字节的方式存储所有的 Term，这种压缩方式可以有效的缩减存储空间，使得 Term Index 足以放进内存，但这种方式也会导致查找时需要更多的 CPU 资源。\n对于存储在磁盘上的倒排表同样也采用了压缩技术减少存储所占用的空间。\n调整配置参数调整配置参数建议如下：\n\n给每个文档指定有序的具有压缩良好的序列模式 ID，避免随机的 UUID-4 这样的 ID，这样的 ID 压缩比很低，会明显拖慢 Lucene。\n\n对于那些不需要聚合和排序的索引字段禁用 Doc values。Doc Values 是有序的基于 document&#x3D;&gt;field value 的映射列表。\n\n不需要做模糊检索的字段使用 Keyword 类型代替 Text 类型，这样可以避免在建立索引前对这些文本进行分词。\n\n如果你的搜索结果不需要近实时的准确度，考虑把每个索引的 index.refresh_interval 改到 30s 。如果你是在做大批量导入，导入期间你可以通过设置这个值为 -1 关掉刷新，还可以通过设置 index.number_of_replicas: 0 关闭副本。别忘记在完工的时候重新开启它。\n\n避免深度分页查询建议使用 Scroll 进行分页查询。普通分页查询时，会创建一个 from+size 的空优先队列，每个分片会返回 from+size 条数据，默认只包含文档 ID 和得分 Score 给协调节点。如果有 N 个分片，则协调节点再对（from+size）×n 条数据进行二次排序，然后选择需要被取回的文档。当 from 很大时，排序过程会变得很沉重，占用 CPU 资源严重。\n\n减少映射字段，只提供需要检索，聚合或排序的字段。其他字段可存在其他存储设备上，例如 Hbase，在 ES 中得到结果后再去 Hbase 查询这些字段。\n\n创建索引和查询时指定路由 Routing 值，这样可以精确到具体的分片查询，提升查询效率。路由的选择需要注意数据的分布均衡。\n\n\nJVM 调优JVM 调优建议如下：\n\n确保堆内存最小值（ Xms ）与最大值（ Xmx ）的大小是相同的，防止程序在运行时改变堆内存大小。Elasticsearch 默认安装后设置的堆内存是 1GB。可通过 ..&#x2F;config&#x2F;jvm.option 文件进行配置，但是最好不要超过物理内存的50%和超过 32GB。\n\nGC 默认采用 CMS 的方式，并发但是有 STW 的问题，可以考虑使用 G1 收集器。\n\nES 非常依赖文件系统缓存（Filesystem Cache），快速搜索。一般来说，应该至少确保物理上有一半的可用内存分配到文件系统缓存。\n\n\n在spring中使用复合查询maven依赖项：\n&lt;parent&gt;    &lt;!-- Springboot依赖 --&gt;    &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;    &lt;artifactId&gt;spring-boot-starter-parent&lt;/artifactId&gt;    &lt;version&gt;2.5.6&lt;/version&gt;    &lt;relativePath/&gt;&lt;/parent&gt; &lt;!--Spring Boot Web--&gt;    &lt;dependency&gt;        &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt;        &lt;artifactId&gt;spring-boot-starter-web&lt;/artifactId&gt;    &lt;/dependency&gt;    &lt;dependency&gt;        &lt;groupId&gt;org.elasticsearch.client&lt;/groupId&gt;        &lt;artifactId&gt;elasticsearch-rest-high-level-client&lt;/artifactId&gt;        &lt;version&gt;7.12.1&lt;/version&gt;    &lt;/dependency&gt;\nconfg类：\n@Configurationpublic class Elasticsearch &#123;    @Bean    public RestHighLevelClient esClient()&#123;        return new RestHighLevelClient(                RestClient.builder(                        new HttpHost(&quot;localhost&quot;,9200, &quot;http&quot;)                )        );    &#125;&#125;\n\n在es中创建如下索引：\n/** * 以下创建person索引， * 一个分片，每分片一个副本，总分片数量为2     PUT /person    &#123;    &quot;settings&quot;: &#123;    &quot;number_of_shards&quot;: 1,    &quot;number_of_replicas&quot;: 1    &#125;,    &quot;mappings&quot;: &#123;    &quot;properties&quot;: &#123;    &quot;id&quot;: &#123;    &quot;type&quot;: &quot;long&quot;    &#125;,    &quot;name&quot;: &#123;    &quot;type&quot;: &quot;keyword&quot;    &#125;,    &quot;code&quot;: &#123;    &quot;type&quot;: &quot;keyword&quot;    &#125;,    &quot;phone&quot;: &#123;    &quot;type&quot;: &quot;keyword&quot;    &#125;,    &quot;birth&quot;: &#123;    &quot;type&quot;: &quot;date&quot;    &#125;,    &quot;lastIp&quot;: &#123;    &quot;type&quot;: &quot;keyword&quot;    &#125;,    &quot;url&quot;: &#123;    &quot;type&quot;: &quot;keyword&quot;    &#125;,    &quot;protocol&quot;: &#123;    &quot;type&quot;: &quot;keyword&quot;    &#125;,    &quot;address&quot;: &#123;    &quot;type&quot;: &quot;text&quot;,    &quot;analyzer&quot;: &quot;ik_max_word&quot;    &#125;,    &quot;zeroCode&quot;: &#123;    &quot;type&quot;: &quot;keyword&quot;    &#125;,    &quot;description&quot;: &#123;    &quot;type&quot;: &quot;text&quot;    , &quot;analyzer&quot;: &quot;ik_max_word&quot;    &#125;,    &quot;email&quot;: &#123;    &quot;type&quot;: &quot;keyword&quot;    &#125;    &#125;    &#125;    &#125;    */\n\n创建controller:\n@RestController@RequestMapping(value = &quot;/es&quot;)@Slf4jpublic class EsController &#123;    @Autowired    private RestHighLevelClient esClient;\t/**     复合查询     http://localhost:8080/es/search?pageNo=10000&amp;pageSize=1     http://localhost:8080/es/search?pageNo=1&amp;pageSize=1&amp;includes=id&amp;includes=name&amp;excludes=name     http://localhost:8080/es/search?pageNo=1&amp;pageSize=5&amp;includes=id&amp;includes=name&amp;includes=protocol&amp;includes=address&amp;excludes=name&amp;protocol=http     http://localhost:8080/es/search?pageNo=1&amp;pageSize=2&amp;includes=id&amp;includes=name&amp;includes=protocol&amp;includes=code&amp;protocol=ws&amp;id=10011     http://localhost:8080/es/search?pageNo=1&amp;pageSize=2&amp;includes=id&amp;includes=name&amp;includes=protocol&amp;includes=code&amp;includes=description&amp;protocol=ws&amp;description=宝玉     http://localhost:8080/es/search?pageNo=1&amp;pageSize=2&amp;includes=id&amp;includes=name&amp;includes=protocol&amp;includes=code&amp;includes=description&amp;description=宝玉&amp;idStart=4&amp;idStop=10000     * @param deepRequest     * @return     */    @RequestMapping(value = &quot;search&quot;)    public String searchFromEs(DeepRequest deepRequest)&#123;        log.info(&quot;start--&gt;&quot;);        long l = System.currentTimeMillis();        Page page = txtService.searchFromEs(deepRequest);        log.info(&quot;stop,cost--&gt; &quot;+ (System.currentTimeMillis() - l));        return JSONUtil.toJsonPrettyStr(page);    &#125;&#125;\n\n\n创建service层代码：\n@Service@Slf4jpublic class TxtService &#123;public Page searchFromEs(DeepRequest deepRequest) &#123;        Page page = new Page();        try &#123;            SearchRequest searchRequest = new SearchRequest(&quot;person&quot;);            page.setPageNo(deepRequest.getPageNo());            page.setPageSize(deepRequest.getPageSize());            buildParams(deepRequest, searchRequest);            Integer start = 0;            if(deepRequest.getPageNo() != null &amp;&amp; deepRequest.getPageSize() != null)&#123;                start = (deepRequest.getPageNo() - 1) * deepRequest.getPageSize();            &#125;            searchRequest.source().from(start);            if(deepRequest.getPageSize() != null)&#123;                searchRequest.source().size(deepRequest.getPageSize());            &#125;else &#123;                searchRequest.source().size(10);                page.setPageSize(10);            &#125;            SearchHits hits = null;            TotalHits totalHits = null;            SearchResponse search = esClient.search(searchRequest, RequestOptions.DEFAULT);            hits = search.getHits();            totalHits = hits.getTotalHits();            List&lt;Person&gt; lists = new ArrayList&lt;&gt;();            SearchHit[] hitLists = hits.getHits();            for (SearchHit doc:hitLists)&#123;                String sourceAsString = doc.getSourceAsString();                Person person = JSONUtil.toBean(sourceAsString, Person.class);                Object[] sortValues = doc.getSortValues();                if(sortValues.length != 0)&#123;                    person.setDistance(sortValues[0].toString());                &#125;                lists.add(person);            &#125;            page.setTotal(totalHits.value);            page.setData(lists);        &#125;catch (Exception e)&#123;            e.printStackTrace();        &#125;        return page;    &#125;&#125; private void buildParams(DeepRequest deepRequest, SearchRequest searchRequest) &#123;        SearchSourceBuilder searchSourceBuilder = new SearchSourceBuilder();        if(!CollectionUtils.isEmpty(deepRequest.getIncludes())                || !CollectionUtils.isEmpty(deepRequest.getExcludes()))&#123;            String [] includes = new String[CollectionUtils.isEmpty(deepRequest.getIncludes()) ? 0: deepRequest.getIncludes().size()];            if(!CollectionUtils.isEmpty(deepRequest.getIncludes()))&#123;                for (int i = 0; i &lt; deepRequest.getIncludes().size(); i++) &#123;                    includes[i] = deepRequest.getIncludes().get(i);                &#125;            &#125;            String [] excludes = new String[CollectionUtils.isEmpty(deepRequest.getExcludes())? 0: deepRequest.getExcludes().size()];            if(!CollectionUtils.isEmpty(deepRequest.getExcludes()))&#123;                for (int i = 0; i &lt; deepRequest.getExcludes().size(); i++) &#123;                    excludes[i] = deepRequest.getExcludes().get(i);                &#125;            &#125;            searchSourceBuilder.fetchSource(includes,excludes);        &#125;        if(deepRequest.getDescription() != null)&#123;            MatchQueryBuilder matchQueryBuilder = QueryBuilders.matchQuery(&quot;description&quot;, deepRequest.getDescription());            matchQueryBuilder.operator(Operator.OR);            searchSourceBuilder.query(matchQueryBuilder);        &#125;        if(deepRequest.getId() != null)&#123;            TermQueryBuilder termQueryBuilder = QueryBuilders.termQuery(&quot;id&quot;, deepRequest.getId());            searchSourceBuilder.query(termQueryBuilder);        &#125;        if(deepRequest.getCode() != null)&#123;            TermQueryBuilder termQueryBuilder = QueryBuilders.termQuery(&quot;code&quot;, deepRequest.getCode());            searchSourceBuilder.query(termQueryBuilder);        &#125;        if(deepRequest.getProtocol() != null)&#123;            TermQueryBuilder termQueryBuilder = QueryBuilders.termQuery(&quot;protocol&quot;, deepRequest.getProtocol());            searchSourceBuilder.query(termQueryBuilder);        &#125;        if(deepRequest.getIdStart() != null &amp;&amp; deepRequest.getIdStop() != null)&#123;            RangeQueryBuilder id = QueryBuilders.rangeQuery(&quot;id&quot;);            id.gte(deepRequest.getIdStart());            id.lte(deepRequest.getIdStop());            searchSourceBuilder.query(id);        &#125;                searchRequest.source(searchSourceBuilder);    &#125;\n\n在kibana中使用## 查看所有索引状态GET /_cat/indices?v## 查看节点健康情况GET /_cluster/health## 创建索引PUT /person&#123;  &quot;settings&quot;: &#123;    &quot;number_of_shards&quot;: 1,    &quot;number_of_replicas&quot;: 1  &#125;,  &quot;mappings&quot;: &#123;    &quot;properties&quot;: &#123;      &quot;id&quot;: &#123;        &quot;type&quot;: &quot;long&quot;      &#125;,      &quot;name&quot;: &#123;        &quot;type&quot;: &quot;keyword&quot;      &#125;,      &quot;code&quot;: &#123;        &quot;type&quot;: &quot;keyword&quot;      &#125;,      &quot;phone&quot;: &#123;        &quot;type&quot;: &quot;keyword&quot;      &#125;,      &quot;birth&quot;: &#123;        &quot;type&quot;: &quot;date&quot;      &#125;,      &quot;lastIp&quot;: &#123;        &quot;type&quot;: &quot;keyword&quot;      &#125;,      &quot;url&quot;: &#123;        &quot;type&quot;: &quot;keyword&quot;      &#125;,      &quot;protocol&quot;: &#123;        &quot;type&quot;: &quot;keyword&quot;      &#125;,      &quot;address&quot;: &#123;        &quot;type&quot;: &quot;text&quot;,        &quot;analyzer&quot;: &quot;ik_max_word&quot;      &#125;,      &quot;zeroCode&quot;: &#123;        &quot;type&quot;: &quot;keyword&quot;      &#125;,      &quot;description&quot;: &#123;        &quot;type&quot;: &quot;text&quot;        , &quot;analyzer&quot;: &quot;ik_max_word&quot;      &#125;,      &quot;email&quot;: &#123;        &quot;type&quot;: &quot;keyword&quot;      &#125;    &#125;  &#125;&#125;## 查看索引结构GET /person/_mapping## 删除索引DELETE /person## 排序分叶GET /person/_search&#123;  &quot;track_total_hits&quot;: true,   &quot;from&quot;: 9999  ,&quot;size&quot;: 1  ,&quot;sort&quot;: [    &#123;      &quot;id&quot;: &#123;        &quot;order&quot;: &quot;asc&quot;      &#125;    &#125;  ]&#125;## 滚动查询GET /person/_search?scroll=1m&#123;  &quot;track_total_hits&quot;: true  ,&quot;size&quot;: 3  ,&quot;sort&quot;: [    &#123;      &quot;id&quot;: &#123;        &quot;order&quot;: &quot;asc&quot;      &#125;    &#125;  ]&#125;## 滚动查询GET /_search/scroll&#123;  &quot;scroll_id&quot;: &quot;FGluY2x1ZGVfY29udGV4dF91dWlkDXF1ZXJ5QW5kRmV0Y2gBFmh4WEdITUxaUzNTdXFPRkU3eDhtNncAAAAAAAAArRZoMFlXU29HSFJUT1FDTUU3NlpsQV93&quot;  ,&quot;scroll&quot;: &quot;1m&quot;&#125;## 分词GET /_analyze&#123; &quot;analyzer&quot;:&quot;ik_max_word&quot;, &quot;text&quot;:&quot;我是中国人&quot; &#125;GET /_analyze &#123; &quot;analyzer&quot;:&quot;ik_smart&quot;, &quot;text&quot;:&quot;我是中国人&quot; &#125;## 按照分词搜索GET /person/_search&#123;   &quot;from&quot;: 0,    &quot;size&quot;: 2,    &quot;query&quot;: &#123;   &quot;match&quot;: &#123;     &quot;description&quot;: &quot;宝玉&quot;      &#125;   &#125;  &#125;## in查询  GET /person/_search &#123;   &quot;query&quot;: &#123;     &quot;terms&quot;: &#123;       &quot;id&quot;: [         &quot;1&quot;,&quot;2&quot;,&quot;3&quot;         ]     &#125;   &#125; &#125;## 区间查询  GET /person/_search &#123;   &quot;query&quot;: &#123;     &quot;range&quot;: &#123;       &quot;id&quot;: &#123;         &quot;gte&quot;: 10,         &quot;lte&quot;: 20       &#125;     &#125;   &#125; &#125;## 分组统计  GET /person/_search &#123;   &quot;aggs&quot;: &#123;     &quot;protocol_agg&quot;: &#123;       &quot;terms&quot;: &#123;         &quot;field&quot;: &quot;protocol&quot;,         &quot;size&quot;: 10       &#125;     &#125;,     &quot;name_agg&quot;:&#123;        &quot;terms&quot;: &#123;          &quot;field&quot;: &quot;name&quot;,          &quot;size&quot;: 10        &#125;            &#125;   &#125;,   &quot;size&quot;: 1 &#125;\n\n","categories":["总结笔记"],"tags":["elasticsearch"]},{"title":"Lambda表达式和匿名内部类","url":"/2021_04_16_lambda/","content":"Java Lambda表达式的一个重要用法是简化某些匿名内部类（Anonymous Classes）的写法。实际上Lambda表达式并不仅仅是匿名内部类的语法糖，JVM内部是通过invokedynamic指令来实现Lambda表达式的。Java Lambda表达式虽然可以简化某些匿名内部类，但Lambda表达式并不能取代所有的匿名内部类，只能用来取代函数接口（Functional Interface）的简写。\n\n\n一、Lambda表达式和匿名内部类1.取代某些匿名内部类例子1：无参函数的简写如果需要新建一个线程，一种常见的写法是这样：\n// JDK7 匿名内部类写法new Thread(new Runnable()&#123;// 接口名\t@Override\tpublic void run()&#123;// 方法名\t\tSystem.out.println(&quot;Thread run()&quot;);\t&#125;&#125;).start();\n\n上述代码给Tread类传递了一个匿名的Runnable对象，重载Runnable接口的run()方法来实现相应逻辑。这是JDK7以及之前的常见写法。匿名内部类省去了为类起名字的烦恼，但还是不够简化，在Java 8中可以简化为如下形式：\n// JDK8 Lambda表达式写法new Thread(\t\t() -&gt; System.out.println(&quot;Thread run()&quot;)// 省略接口名和方法名).start();\n\n上述代码跟匿名内部类的作用是一样的，但比匿名内部类更进一步。这里连接口名和函数名都一同省掉了，写起来更加神清气爽。如果函数体有多行，可以用大括号括起来，就像这样：\n// JDK8 Lambda表达式代码块写法new Thread(        () -&gt; &#123;            System.out.print(&quot;Hello&quot;);            System.out.println(&quot; Hoolee&quot;);        &#125;).start();\n\n\n例子2：带参函数的简写如果要给一个字符串列表通过自定义比较器，按照字符串长度进行排序，Java 7的书写形式如下：\n// JDK7 匿名内部类写法List&lt;String&gt; list = Arrays.asList(&quot;I&quot;, &quot;love&quot;, &quot;you&quot;, &quot;too&quot;);Collections.sort(list, new Comparator&lt;String&gt;()&#123;// 接口名    @Override    public int compare(String s1, String s2)&#123;// 方法名        if(s1 == null)            return -1;        if(s2 == null)            return 1;        return s1.length()-s2.length();    &#125;&#125;);\n上述代码通过内部类重载了Comparator接口的compare()方法，实现比较逻辑。采用Lambda表达式可简写如下：\n// JDK8 Lambda表达式写法List&lt;String&gt; list = Arrays.asList(&quot;I&quot;, &quot;love&quot;, &quot;you&quot;, &quot;too&quot;);Collections.sort(list, (s1, s2) -&gt;&#123;// 省略参数表的类型    if(s1 == null)        return -1;    if(s2 == null)        return 1;    return s1.length()-s2.length();&#125;);\n上述代码跟匿名内部类的作用是一样的。除了省略了接口名和方法名，代码中把参数表的类型也省略了。这得益于javac的类型推断机制，编译器能够根据上下文信息推断出参数的类型，当然也有推断失败的时候，这时就需要手动指明参数类型了。注意，Java是强类型语言，每个变量和对象都必需有明确的类型。\n2.什么情况下简写也许你已经想到了，能够使用Lambda的依据是必须有相应的函数接口（函数接口，是指内部只有一个抽象方法的接口）。这一点跟Java是强类型语言吻合，也就是说你并不能在代码的任何地方任性的写Lambda表达式。实际上Lambda的类型就是对应函数接口的类型。Lambda表达式另一个依据是类型推断机制，在上下文信息足够的情况下，编译器可以推断出参数表的类型，而不需要显式指名。Lambda表达更多合法的书写形式如下：\n// Lambda表达式的书写形式Runnable run = () -&gt; System.out.println(&quot;Hello World&quot;);// 1ActionListener listener = event -&gt; System.out.println(&quot;button clicked&quot;);// 2Runnable multiLine = () -&gt; &#123;// 3 代码块    System.out.print(&quot;Hello&quot;);    System.out.println(&quot; Hoolee&quot;);&#125;;BinaryOperator&lt;Long&gt; add = (Long x, Long y) -&gt; x + y;// 4BinaryOperator&lt;Long&gt; addImplicit = (x, y) -&gt; x + y;// 5 类型推断\n\n上述代码中，1展示了无参函数的简写；2处展示了有参函数的简写，以及类型推断机制；3是代码块的写法；4和5再次展示了类型推断机制。\n3.自定义一个函数接口自定义函数接口很容易，只需要编写一个只有一个抽象方法的接口即可。\n// 自定义函数接口@FunctionalInterfacepublic interface ConsumerInterface&lt;T&gt;&#123;\tvoid accept(T t);&#125;\n\n上面代码中的@FunctionalInterface是可选的，但加上该标注编译器会帮你检查接口是否符合函数接口规范。就像加入@Override标注会检查是否重载了函数一样。\n有了上述接口定义，就可以写出类似如下的代码：\nConsumerInterface&lt;String&gt; consumer = str -&gt; System.out.println(str);\n进一步的，还可以这样使用：\nclass MyStream&lt;T&gt;&#123;\tprivate List&lt;T&gt; list;    ...\tpublic void myForEach(ConsumerInterface&lt;T&gt; consumer)&#123;// 1\t\tfor(T t : list)&#123;\t\t\tconsumer.accept(t);\t\t&#125;\t&#125;&#125;MyStream&lt;String&gt; stream = new MyStream&lt;String&gt;();stream.myForEach(str -&gt; System.out.println(str));// 使用自定义函数接口书写Lambda表达式\n\n4. 匿名内部类实现经过第一篇的的介绍，我们看到Lambda表达式似乎只是为了简化匿名内部类书写，这看起来仅仅通过语法糖在编译阶段把所有的Lambda表达式替换成匿名内部类就可以了。但实时并非如此。在JVM层面，Lambda表达式和匿名内部类有着明显的差别。\n匿名内部类仍然是一个类，只是不需要程序员显示指定类名，编译器会自动为该类取名。因此如果有如下形式的代码，编译之后将会产生两个class文件：\npublic class MainAnonymousClass &#123;\tpublic static void main(String[] args) &#123;\t\tnew Thread(new Runnable()&#123;\t\t\t@Override\t\t\tpublic void run()&#123;\t\t\t\tSystem.out.println(&quot;Anonymous Class Thread run()&quot;);\t\t\t&#125;\t\t&#125;).start();;\t&#125;&#125;\n编译之后文件分布如下，两个class文件分别是主类和匿名内部类产生的：\n\n进一步分析主类MainAnonymousClass.class的字节码，可发现其创建了匿名内部类的对象：\n// javap -c MainAnonymousClass.classpublic class MainAnonymousClass &#123;  ...  public static void main(java.lang.String[]);    Code:       0: new           #2                  // class java/lang/Thread       3: dup       4: new           #3                  // class MainAnonymousClass$1 /*创建内部类对象*/       7: dup       8: invokespecial #4                  // Method MainAnonymousClass$1.&quot;&lt;init&gt;&quot;:()V      11: invokespecial #5                  // Method java/lang/Thread.&quot;&lt;init&gt;&quot;:(Ljava/lang/Runnable;)V      14: invokevirtual #6                  // Method java/lang/Thread.start:()V      17: return&#125;\n5.Lambda表达式实现Lambda表达式通过invokedynamic指令实现，书写Lambda表达式不会产生新的类。如果有如下代码，编译之后只有一个class文件：\npublic class MainLambda &#123;\tpublic static void main(String[] args) &#123;\t\tnew Thread(\t\t\t\t() -&gt; System.out.println(&quot;Lambda Thread run()&quot;)\t\t\t).start();;\t&#125;&#125;\n编译之后的结果：\n\n通过javap反编译命名，我们更能看出Lambda表达式内部表示的不同：\n// javap -c -p MainLambda.classpublic class MainLambda &#123;  ...  public static void main(java.lang.String[]);    Code:       0: new           #2                  // class java/lang/Thread       3: dup       4: invokedynamic #3,  0              // InvokeDynamic #0:run:()Ljava/lang/Runnable; /*使用invokedynamic指令调用*/       9: invokespecial #4                  // Method java/lang/Thread.&quot;&lt;init&gt;&quot;:(Ljava/lang/Runnable;)V      12: invokevirtual #5                  // Method java/lang/Thread.start:()V      15: return  private static void lambda$main$0();  /*Lambda表达式被封装成主类的私有方法*/    Code:       0: getstatic     #6                  // Field java/lang/System.out:Ljava/io/PrintStream;       3: ldc           #7                  // String Lambda Thread run()       5: invokevirtual #8                  // Method java/io/PrintStream.println:(Ljava/lang/String;)V       8: return&#125;\n\n反编译之后我们发现Lambda表达式被封装成了主类的一个私有方法，并通过invokedynamic指令进行调用。\n推论，this引用的意义既然Lambda表达式不是内部类的简写，那么Lambda内部的this引用也就跟内部类对象没什么关系了。在Lambda表达式中this的意义跟在表达式外部完全一样。因此下列代码将输出两遍Hello Hoolee，而不是两个引用地址。\npublic class Hello &#123;\tRunnable r1 = () -&gt; &#123; System.out.println(this); &#125;;\tRunnable r2 = () -&gt; &#123; System.out.println(toString()); &#125;;\tpublic static void main(String[] args) &#123;\t\tnew Hello().r1.run();\t\tnew Hello().r2.run();\t&#125;\tpublic String toString() &#123; return &quot;Hello Hoolee&quot;; &#125;&#125;\n\n\n二、集合API我们先从最熟悉的*Java集合框架(Java Collections Framework, JCF)*开始说起。\n为引入Lambda表达式，Java8新增了java.util.function包，里面包含常用的函数接口，这是Lambda表达式的基础，Java集合框架也新增部分接口，以便与Lambda表达式对接。\n首先回顾一下Java集合框架的接口继承结构：\n\n上图中绿色标注的接口类，表示在Java8中加入了新的接口方法，当然由于继承关系，他们相应的子类也都会继承这些新方法。下表详细列举了这些方法。\n\n\n\n接口名\nJava8新加入的方法\n\n\n\nCollection\nremoveIf() spliterator() stream() parallelStream() forEach()\n\n\nList\nreplaceAll() sort()\n\n\nMap\ngetOrDefault() forEach() replaceAll() putIfAbsent() remove() replace() computeIfAbsent() computeIfPresent() compute() merge()\n\n\n这些新加入的方法大部分要用到java.util.function包下的接口，这意味着这些方法大部分都跟Lambda表达式相关。我们将逐一学习这些方法。\n1.Collection中的新方法如上所示，接口Collection和List新加入了一些方法，我们以是List的子类ArrayList为例来说明。了解Java7ArrayList实现原理，将有助于理解下文。\nforEach()该方法的签名为void forEach(Consumer&lt;? super E&gt; action)，作用是对容器中的每个元素执行action指定的动作，其中Consumer是个函数接口，里面只有一个待实现方法void accept(T t)（后面我们会看到，这个方法叫什么根本不重要，你甚至不需要记忆它的名字）。\n需求：假设有一个字符串列表，需要打印出其中所有长度大于3的字符串.\nJava7及以前我们可以用增强的for循环实现：\n// 使用曾强for循环迭代ArrayList&lt;String&gt; list = new ArrayList&lt;&gt;(Arrays.asList(&quot;I&quot;, &quot;love&quot;, &quot;you&quot;, &quot;too&quot;));for(String str : list)&#123;    if(str.length()&gt;3)        System.out.println(str);&#125;\n\n现在使用forEach()方法结合匿名内部类，可以这样实现：\n// 使用forEach()结合匿名内部类迭代ArrayList&lt;String&gt; list = new ArrayList&lt;&gt;(Arrays.asList(&quot;I&quot;, &quot;love&quot;, &quot;you&quot;, &quot;too&quot;));list.forEach(new Consumer&lt;String&gt;()&#123;    @Override    public void accept(String str)&#123;        if(str.length()&gt;3)            System.out.println(str);    &#125;&#125;);\n上述代码调用forEach()方法，并使用匿名内部类实现Comsumer接口。到目前为止我们没看到这种设计有什么好处，但是不要忘记Lambda表达式，使用Lambda表达式实现如下：\n// 使用forEach()结合Lambda表达式迭代ArrayList&lt;String&gt; list = new ArrayList&lt;&gt;(Arrays.asList(&quot;I&quot;, &quot;love&quot;, &quot;you&quot;, &quot;too&quot;));list.forEach( str -&gt; &#123;        if(str.length()&gt;3)            System.out.println(str);    &#125;);\n上述代码给forEach()方法传入一个Lambda表达式，我们不需要知道accept()方法，也不需要知道Consumer接口，类型推导帮我们做了一切。\nremoveIf()该方法签名为boolean removeIf(Predicate&lt;? super E&gt; filter)，作用是删除容器中所有满足filter指定条件的元素，其中Predicate是一个函数接口，里面只有一个待实现方法boolean test(T t)，同样的这个方法的名字根本不重要，因为用的时候不需要书写这个名字。\n需求：假设有一个字符串列表，需要删除其中所有长度大于3的字符串。\n我们知道如果需要在迭代过程冲对容器进行删除操作必须使用迭代器，否则会抛出ConcurrentModificationException，所以上述任务传统的写法是：\n// 使用迭代器删除列表元素ArrayList&lt;String&gt; list = new ArrayList&lt;&gt;(Arrays.asList(&quot;I&quot;, &quot;love&quot;, &quot;you&quot;, &quot;too&quot;));Iterator&lt;String&gt; it = list.iterator();while(it.hasNext())&#123;    if(it.next().length()&gt;3) // 删除长度大于3的元素        it.remove();&#125;\n\n现在使用removeIf()方法结合匿名内部类，我们可是这样实现：\n// 使用removeIf()结合匿名名内部类实现ArrayList&lt;String&gt; list = new ArrayList&lt;&gt;(Arrays.asList(&quot;I&quot;, &quot;love&quot;, &quot;you&quot;, &quot;too&quot;));list.removeIf(new Predicate&lt;String&gt;()&#123; // 删除长度大于3的元素    @Override    public boolean test(String str)&#123;        return str.length()&gt;3;    &#125;&#125;);\n上述代码使用removeIf()方法，并使用匿名内部类实现Precicate接口。相信你已经想到用Lambda表达式该怎么写了：\n// 使用removeIf()结合Lambda表达式实现ArrayList&lt;String&gt; list = new ArrayList&lt;&gt;(Arrays.asList(&quot;I&quot;, &quot;love&quot;, &quot;you&quot;, &quot;too&quot;));list.removeIf(str -&gt; str.length()&gt;3); // 删除长度大于3的元素\n使用Lambda表达式不需要记忆Predicate接口名，也不需要记忆test()方法名，只需要知道此处需要一个返回布尔类型的Lambda表达式就行了。\nreplaceAll()该方法签名为void replaceAll(UnaryOperator&lt;E&gt; operator)，作用是对每个元素执行operator指定的操作，并用操作结果来替换原来的元素。其中UnaryOperator是一个函数接口，里面只有一个待实现函数T apply(T t)。\n需求：假设有一个字符串列表，将其中所有长度大于3的元素转换成大写，其余元素不变。\nJava7及之前似乎没有优雅的办法：\n// 使用下标实现元素替换ArrayList&lt;String&gt; list = new ArrayList&lt;&gt;(Arrays.asList(&quot;I&quot;, &quot;love&quot;, &quot;you&quot;, &quot;too&quot;));for(int i=0; i&lt;list.size(); i++)&#123;    String str = list.get(i);    if(str.length()&gt;3)        list.set(i, str.toUpperCase());&#125;\n\n使用replaceAll()方法结合匿名内部类可以实现如下：\n// 使用匿名内部类实现ArrayList&lt;String&gt; list = new ArrayList&lt;&gt;(Arrays.asList(&quot;I&quot;, &quot;love&quot;, &quot;you&quot;, &quot;too&quot;));list.replaceAll(new UnaryOperator&lt;String&gt;()&#123;    @Override    public String apply(String str)&#123;        if(str.length()&gt;3)            return str.toUpperCase();        return str;    &#125;&#125;);\n上述代码调用replaceAll()方法，并使用匿名内部类实现UnaryOperator接口。我们知道可以用更为简洁的Lambda表达式实现：\n// 使用Lambda表达式实现ArrayList&lt;String&gt; list = new ArrayList&lt;&gt;(Arrays.asList(&quot;I&quot;, &quot;love&quot;, &quot;you&quot;, &quot;too&quot;));list.replaceAll(str -&gt; &#123;    if(str.length()&gt;3)        return str.toUpperCase();    return str;&#125;);\nsort()该方法定义在List接口中，方法签名为void sort(Comparator&lt;? super E&gt; c)，该方法根据c指定的比较规则对容器元素进行排序。Comparator接口我们并不陌生，其中有一个方法int compare(T o1, T o2)需要实现，显然该接口是个函数接口。\n需求：假设有一个字符串列表，按照字符串长度增序对元素排序。\n由于Java7以及之前sort()方法在Collections工具类中，所以代码要这样写：\n// Collections.sort()方法ArrayList&lt;String&gt; list = new ArrayList&lt;&gt;(Arrays.asList(&quot;I&quot;, &quot;love&quot;, &quot;you&quot;, &quot;too&quot;));Collections.sort(list, new Comparator&lt;String&gt;()&#123;    @Override    public int compare(String str1, String str2)&#123;        return str1.length()-str2.length();    &#125;&#125;);\n\n现在可以直接使用List.sort()方法，结合Lambda表达式，可以这样写：\n// List.sort()方法结合Lambda表达式ArrayList&lt;String&gt; list = new ArrayList&lt;&gt;(Arrays.asList(&quot;I&quot;, &quot;love&quot;, &quot;you&quot;, &quot;too&quot;));list.sort((str1, str2) -&gt; str1.length()-str2.length());\n\nspliterator()方法签名为Spliterator&lt;E&gt; spliterator()，该方法返回容器的可拆分迭代器。从名字来看该方法跟iterator()方法有点像，我们知道Iterator是用来迭代容器的，Spliterator也有类似作用，但二者有如下不同：\n\nSpliterator既可以像Iterator那样逐个迭代，也可以批量迭代。批量迭代可以降低迭代的开销。\nSpliterator是可拆分的，一个Spliterator可以通过调用Spliterator&lt;T&gt; trySplit()方法来尝试分成两个。一个是this，另一个是新返回的那个，这两个迭代器代表的元素没有重叠。\n\n可通过（多次）调用Spliterator.trySplit()方法来分解负载，以便多线程处理。\nstream()和parallelStream()stream()和parallelStream()分别返回该容器的Stream视图表示，不同之处在于parallelStream()返回并行的Stream。**Stream是Java函数式编程的核心类**，我们会在后面章节中学习。\n2.Map中的新方法相比Collection，Map中加入了更多的方法，我们以HashMap为例来逐一探秘。了解Java7HashMap实现原理，将有助于理解下文。\nforEach()该方法签名为void forEach(BiConsumer&lt;? super K,? super V&gt; action)，作用是对Map中的每个映射执行action指定的操作，其中BiConsumer是一个函数接口，里面有一个待实现方法void accept(T t, U u)。BinConsumer接口名字和accept()方法名字都不重要，请不要记忆他们。\n需求：假设有一个数字到对应英文单词的Map，请输出Map中的所有映射关系．\nJava7以及之前经典的代码如下：\n// Java7以及之前迭代MapHashMap&lt;Integer, String&gt; map = new HashMap&lt;&gt;();map.put(1, &quot;one&quot;);map.put(2, &quot;two&quot;);map.put(3, &quot;three&quot;);for(Map.Entry&lt;Integer, String&gt; entry : map.entrySet())&#123;    System.out.println(entry.getKey() + &quot;=&quot; + entry.getValue());&#125;\n\n使用Map.forEach()方法，结合匿名内部类，代码如下：\n// 使用forEach()结合匿名内部类迭代MapHashMap&lt;Integer, String&gt; map = new HashMap&lt;&gt;();map.put(1, &quot;one&quot;);map.put(2, &quot;two&quot;);map.put(3, &quot;three&quot;);map.forEach(new BiConsumer&lt;Integer, String&gt;()&#123;    @Override    public void accept(Integer k, String v)&#123;        System.out.println(k + &quot;=&quot; + v);    &#125;&#125;);\n上述代码调用forEach()方法，并使用匿名内部类实现BiConsumer接口。当然，实际场景中没人使用匿名内部类写法，因为有Lambda表达式：\n// 使用forEach()结合Lambda表达式迭代MapHashMap&lt;Integer, String&gt; map = new HashMap&lt;&gt;();map.put(1, &quot;one&quot;);map.put(2, &quot;two&quot;);map.put(3, &quot;three&quot;);map.forEach((k, v) -&gt; System.out.println(k + &quot;=&quot; + v));&#125;\n\ngetOrDefault()该方法跟Lambda表达式没关系，但是很有用。方法签名为V getOrDefault(Object key, V defaultValue)，作用是**按照给定的key查询Map中对应的value，如果没有找到则返回defaultValue**。使用该方法程序员可以省去查询指定键值是否存在的麻烦．\n需求；假设有一个数字到对应英文单词的Map，输出4对应的英文单词，如果不存在则输出NoValue\n// 查询Map中指定的值，不存在时使用默认值HashMap&lt;Integer, String&gt; map = new HashMap&lt;&gt;();map.put(1, &quot;one&quot;);map.put(2, &quot;two&quot;);map.put(3, &quot;three&quot;);// Java7以及之前做法if(map.containsKey(4))&#123; // 1    System.out.println(map.get(4));&#125;else&#123;    System.out.println(&quot;NoValue&quot;);&#125;// Java8使用Map.getOrDefault()System.out.println(map.getOrDefault(4, &quot;NoValue&quot;)); // 2\nputIfAbsent()该方法跟Lambda表达式没关系，但是很有用。方法签名为V putIfAbsent(K key, V value)，作用是只有在不存在key值的映射或映射值为null时，才将value指定的值放入到Map中，否则不对Map做更改．该方法将条件判断和赋值合二为一，使用起来更加方便．\nremove()我们都知道Map中有一个remove(Object key)方法，来根据指定key值删除Map中的映射关系；Java8新增了remove(Object key, Object value)方法，只有在当前Map中**key正好映射到value时**才删除该映射，否则什么也不做．\nreplace()在Java7及以前，要想替换Map中的映射关系可通过put(K key, V value)方法实现，该方法总是会用新值替换原来的值．为了更精确的控制替换行为，Java8在Map中加入了两个replace()方法，分别如下：\n\nreplace(K key, V value)，只有在当前Map中**key的映射存在时**才用value去替换原来的值，否则什么也不做．\nreplace(K key, V oldValue, V newValue)，只有在当前Map中**key的映射存在且等于oldValue时**才用newValue去替换原来的值，否则什么也不做．\n\nreplaceAll()该方法签名为replaceAll(BiFunction&lt;? super K,? super V,? extends V&gt; function)，作用是对Map中的每个映射执行function指定的操作，并用function的执行结果替换原来的value，其中BiFunction是一个函数接口，里面有一个待实现方法R apply(T t, U u)．不要被如此多的函数接口吓到，因为使用的时候根本不需要知道他们的名字．\n需求：假设有一个数字到对应英文单词的Map，请将原来映射关系中的单词都转换成大写．\nJava7以及之前经典的代码如下：\n// Java7以及之前替换所有Map中所有映射关系HashMap&lt;Integer, String&gt; map = new HashMap&lt;&gt;();map.put(1, &quot;one&quot;);map.put(2, &quot;two&quot;);map.put(3, &quot;three&quot;);for(Map.Entry&lt;Integer, String&gt; entry : map.entrySet())&#123;    entry.setValue(entry.getValue().toUpperCase());&#125;\n\n使用replaceAll()方法结合匿名内部类，实现如下：\n// 使用replaceAll()结合匿名内部类实现HashMap&lt;Integer, String&gt; map = new HashMap&lt;&gt;();map.put(1, &quot;one&quot;);map.put(2, &quot;two&quot;);map.put(3, &quot;three&quot;);map.replaceAll(new BiFunction&lt;Integer, String, String&gt;()&#123;    @Override    public String apply(Integer k, String v)&#123;        return v.toUpperCase();    &#125;&#125;);\n上述代码调用replaceAll()方法，并使用匿名内部类实现BiFunction接口。更进一步的，使用Lambda表达式实现如下：\n// 使用replaceAll()结合Lambda表达式实现HashMap&lt;Integer, String&gt; map = new HashMap&lt;&gt;();map.put(1, &quot;one&quot;);map.put(2, &quot;two&quot;);map.put(3, &quot;three&quot;);map.replaceAll((k, v) -&gt; v.toUpperCase());\n\n简洁到让人难以置信．\nmerge()该方法签名为merge(K key, V value, BiFunction&lt;? super V,? super V,? extends V&gt; remappingFunction)，作用是：\n\n如果Map中key对应的映射不存在或者为null，则将value（不能是null）关联到key上；\n否则执行remappingFunction，如果执行结果非null则用该结果跟key关联，否则在Map中删除key的映射．\n\n参数中BiFunction函数接口前面已经介绍过，里面有一个待实现方法R apply(T t, U u)．\nmerge()方法虽然语义有些复杂，但该方法的用方式很明确，一个比较常见的场景是将新的错误信息拼接到原来的信息上，比如：\nmap.merge(key, newMsg, (v1, v2) -&gt; v1+v2);\n\ncompute()该方法签名为compute(K key, BiFunction&lt;? super K,? super V,? extends V&gt; remappingFunction)，作用是把remappingFunction的计算结果关联到key上，如果计算结果为null，则在Map中删除key的映射．\n要实现上述merge()方法中错误信息拼接的例子，使用compute()代码如下：\nmap.compute(key, (k,v) -&gt; v==null ? newMsg : v.concat(newMsg));\n\ncomputeIfAbsent()该方法签名为V computeIfAbsent(K key, Function&lt;? super K,? extends V&gt; mappingFunction)，作用是：只有在当前Map中不存在key值的映射或映射值为null时，才调用mappingFunction，并在mappingFunction执行结果非null时，将结果跟key关联．\nFunction是一个函数接口，里面有一个待实现方法R apply(T t)．\ncomputeIfAbsent()常用来对Map的某个key值建立初始化映射．比如我们要实现一个多值映射，Map的定义可能是Map&lt;K,Set&lt;V&gt;&gt;，要向Map中放入新值，可通过如下代码实现：\nMap&lt;Integer, Set&lt;String&gt;&gt; map = new HashMap&lt;&gt;();// Java7及以前的实现方式if(map.containsKey(1))&#123;    map.get(1).add(&quot;one&quot;);&#125;else&#123;    Set&lt;String&gt; valueSet = new HashSet&lt;String&gt;();    valueSet.add(&quot;one&quot;);    map.put(1, valueSet);&#125;// Java8的实现方式map.computeIfAbsent(1, v -&gt; new HashSet&lt;String&gt;()).add(&quot;yi&quot;);\n\n使用computeIfAbsent()将条件判断和添加操作合二为一，使代码更加简洁．\ncomputeIfPresent()该方法签名为V computeIfPresent(K key, BiFunction&lt;? super K,? super V,? extends V&gt; remappingFunction)，作用跟computeIfAbsent()相反，即，只有在当前Map中存在key值的映射且非null时，才调用remappingFunction，如果remappingFunction执行结果为null，则删除key的映射，否则使用该结果替换key原来的映射．\n这个函数的功能跟如下代码是等效的：\n// Java7及以前跟computeIfPresent()等效的代码if (map.get(key) != null) &#123;    V oldValue = map.get(key);    V newValue = remappingFunction.apply(key, oldValue);    if (newValue != null)        map.put(key, newValue);    else        map.remove(key);    return newValue;&#125;return null;\n\n\n3.总结\nJava8为容器新增一些有用的方法，这些方法有些是为完善原有功能，有些是为引入函数式编程，学习和使用这些方法有助于我们写出更加简洁有效的代码．\n函数接口虽然很多，但绝大多数时候我们根本不需要知道它们的名字，书写Lambda表达式时类型推断帮我们做了一切．\n\n三、Streams API你可能没意识到Java对函数式编程的重视程度，看看Java 8加入函数式编程扩充多少功能就清楚了。Java 8之所以费这么大功夫引入函数式编程，原因有二：\n\n代码简洁函数式编程写出的代码简洁且意图明确，使用stream接口让你从此告别for循环。\n多核友好，Java函数式编程使得编写并行程序从未如此简单，你需要的全部就是调用一下parallel()方法。\n\n这一节我们学习stream，也就是Java函数式编程的主角。对于Java 7来说stream完全是个陌生东西，stream并不是某种数据结构，它只是数据源的一种视图。这里的数据源可以是一个数组，Java容器或I&#x2F;O channel等。正因如此要得到一个stream通常不会手动创建，而是调用对应的工具方法，比如：\n\n调用Collection.stream()或者Collection.parallelStream()方法\n调用Arrays.stream(T[] array)方法\n\n常见的stream接口继承关系如图：\n\n\n图中4种stream接口继承自BaseStream，其中IntStream, LongStream, DoubleStream对应三种基本类型（int, long, double，注意不是包装类型），Stream对应所有剩余类型的stream视图。为不同数据类型设置不同stream接口，可以1.提高性能，2.增加特定接口函数。\n\n\n\n\n你可能会奇怪为什么不把IntStream等设计成Stream的子接口？毕竟这接口中的方法名大部分是一样的。答案是这些方法的名字虽然相同，但是返回类型不同，如果设计成父子接口关系，这些方法将不能共存，因为Java不允许只有返回类型不同的方法重载。\n虽然大部分情况下stream是容器调用Collection.stream()方法得到的，但stream和collections有以下不同：\n\n无存储。stream不是一种数据结构，它只是某种数据源的一个视图，数据源可以是一个数组，Java容器或I&#x2F;O channel等。\n为函数式编程而生。对stream的任何修改都不会修改背后的数据源，比如对stream执行过滤操作并不会删除被过滤的元素，而是会产生一个不包含被过滤元素的新stream。\n惰式执行。stream上的操作并不会立即执行，只有等到用户真正需要结果的时候才会执行。\n可消费性。stream只能被“消费”一次，一旦遍历过就会失效，就像容器的迭代器那样，想要再次遍历必须重新生成。\n\n对stream的操作分为为两类，**中间操作(intermediate operations)和结束操作(terminal operations)**，二者特点是：\n\n__中间操作总是会惰式执行__，调用中间操作只会生成一个标记了该操作的新stream，仅此而已。\n__结束操作会触发实际计算__，计算发生时会把所有中间操作积攒的操作以pipeline的方式执行，这样可以减少迭代次数。计算完成之后stream就会失效。\n\n如果你熟悉Apache Spark RDD，对stream的这个特点应该不陌生。\n下表汇总了Stream接口的部分常见方法：\n\n\n\n操作类型\n接口方法\n\n\n\n中间操作\nconcat() distinct() filter() flatMap() limit() map() peek()  skip() sorted() parallel() sequential() unordered()\n\n\n结束操作\nallMatch() anyMatch() collect() count() findAny() findFirst()  forEach() forEachOrdered() max() min() noneMatch() reduce() toArray()\n\n\n区分中间操作和结束操作最简单的方法，就是看方法的返回值，返回值为stream的大都是中间操作，否则是结束操作。\n1.stream方法使用stream跟函数接口关系非常紧密，没有函数接口stream就无法工作。回顾一下：__函数接口是指内部只有一个抽象方法的接口__。通常函数接口出现的地方都可以使用Lambda表达式，所以不必记忆函数接口的名字。\nforEach()我们对forEach()方法并不陌生，在Collection中我们已经见过。方法签名为void forEach(Consumer&lt;? super E&gt; action)，作用是对容器中的每个元素执行action指定的动作，也就是对元素进行遍历。\n// 使用Stream.forEach()迭代Stream&lt;String&gt; stream = Stream.of(&quot;I&quot;, &quot;love&quot;, &quot;you&quot;, &quot;too&quot;);stream.forEach(str -&gt; System.out.println(str));\n由于forEach()是结束方法，上述代码会立即执行，输出所有字符串。\nfilter()\n\n函数原型为Stream&lt;T&gt; filter(Predicate&lt;? super T&gt; predicate)，作用是返回一个只包含满足predicate条件元素的Stream。\n// 保留长度等于3的字符串Stream&lt;String&gt; stream= Stream.of(&quot;I&quot;, &quot;love&quot;, &quot;you&quot;, &quot;too&quot;);stream.filter(str -&gt; str.length()==3)    .forEach(str -&gt; System.out.println(str));\n\n上述代码将输出为长度等于3的字符串you和too。注意，由于filter()是个中间操作，如果只调用filter()不会有实际计算，因此也不会输出任何信息。\ndistinct()\n\n函数原型为Stream&lt;T&gt; distinct()，作用是返回一个去除重复元素之后的Stream。\nStream&lt;String&gt; stream= Stream.of(&quot;I&quot;, &quot;love&quot;, &quot;you&quot;, &quot;too&quot;, &quot;too&quot;);stream.distinct()    .forEach(str -&gt; System.out.println(str));\n\n上述代码会输出去掉一个too之后的其余字符串。\n\nsorted()排序函数有两个，一个是用自然顺序排序，一个是使用自定义比较器排序，函数原型分别为Stream&lt;T&gt;　sorted()和Stream&lt;T&gt;　sorted(Comparator&lt;? super T&gt; comparator)。\nStream&lt;String&gt; stream= Stream.of(&quot;I&quot;, &quot;love&quot;, &quot;you&quot;, &quot;too&quot;);stream.sorted((str1, str2) -&gt; str1.length()-str2.length())    .forEach(str -&gt; System.out.println(str));\n\n上述代码将输出按照长度升序排序后的字符串，结果完全在预料之中。\nmap()\n\n函数原型为&lt;R&gt; Stream&lt;R&gt; map(Function&lt;? super T,? extends R&gt; mapper)，作用是返回一个对当前所有元素执行执行mapper之后的结果组成的Stream。直观的说，就是对每个元素按照某种操作进行转换，转换前后Stream中元素的个数不会改变，但元素的类型取决于转换之后的类型。\nStream&lt;String&gt; stream　= Stream.of(&quot;I&quot;, &quot;love&quot;, &quot;you&quot;, &quot;too&quot;);stream.map(str -&gt; str.toUpperCase())    .forEach(str -&gt; System.out.println(str));\n上述代码将输出原字符串的大写形式。\nflatMap()\n\n函数原型为&lt;R&gt; Stream&lt;R&gt; flatMap(Function&lt;? super T,? extends Stream&lt;? extends R&gt;&gt; mapper)，作用是对每个元素执行mapper指定的操作，并用所有mapper返回的Stream中的元素组成一个新的Stream作为最终返回结果。说起来太拗口，通俗的讲flatMap()的作用就相当于把原stream中的所有元素都”摊平”之后组成的Stream，转换前后元素的个数和类型都可能会改变。\nStream&lt;List&lt;Integer&gt;&gt; stream = Stream.of(Arrays.asList(1,2), Arrays.asList(3, 4, 5));stream.flatMap(list -&gt; list.stream())    .forEach(i -&gt; System.out.println(i));\n\n上述代码中，原来的stream中有两个元素，分别是两个List&lt;Integer&gt;，执行flatMap()之后，将每个List都“摊平”成了一个个的数字，所以会新产生一个由5个数字组成的Stream。所以最终将输出1~5这5个数字。\n2.多面手reduce()接下来我们将仍然以Stream为例，介绍流的规约操作。\n规约操作（reduction operation）又被称作折叠操作（fold），是通过某个连接动作将所有元素汇总成一个汇总结果的过程。元素求和、求最大值或最小值、求出元素总个数、将所有元素转换成一个列表或集合，都属于规约操作。Stream类库有两个通用的规约操作reduce()和collect()，也有一些为简化书写而设计的专用规约操作，比如sum()、max()、min()、count()等。\n最大或最小值这类规约操作很好理解（至少方法语义上是这样），我们着重介绍reduce()和collect()，这是比较有魔法的地方。\nreduce操作可以实现从一组元素中生成一个值，sum()、max()、min()、count()等都是reduce操作，将他们单独设为函数只是因为常用。reduce()的方法定义有三种重写形式：\n\nOptional&lt;T&gt; reduce(BinaryOperator&lt;T&gt; accumulator)\nT reduce(T identity, BinaryOperator&lt;T&gt; accumulator)\n&lt;U&gt; U reduce(U identity, BiFunction&lt;U,? super T,U&gt; accumulator, BinaryOperator&lt;U&gt; combiner)\n\n虽然函数定义越来越长，但语义不曾改变，多的参数只是为了指明初始值（参数identity），或者是指定并行执行时多个部分结果的合并方式（参数combiner）。reduce()最常用的场景就是从一堆值中生成一个值。用这么复杂的函数去求一个最大或最小值，你是不是觉得设计者有病。其实不然，因为“大”和“小”或者“求和”有时会有不同的语义。\n需求：从一组单词中找出最长的单词。这里“大”的含义就是“长”。\n// 找出最长的单词Stream&lt;String&gt; stream = Stream.of(&quot;I&quot;, &quot;love&quot;, &quot;you&quot;, &quot;too&quot;);Optional&lt;String&gt; longest = stream.reduce((s1, s2) -&gt; s1.length()&gt;=s2.length() ? s1 : s2);//Optional&lt;String&gt; longest = stream.max((s1, s2) -&gt; s1.length()-s2.length());System.out.println(longest.get());\n上述代码会选出最长的单词love，其中Optional是（一个）值的容器，使用它可以避免null值的麻烦。当然可以使用Stream.max(Comparator&lt;? super T&gt; comparator)方法来达到同等效果，但reduce()自有其存在的理由。\n\n\n需求：求出一组单词的长度之和。这是个“求和”操作，操作对象输入类型是String，而结果类型是Integer。\n// 求单词长度之和Stream&lt;String&gt; stream = Stream.of(&quot;I&quot;, &quot;love&quot;, &quot;you&quot;, &quot;too&quot;);Integer lengthSum = stream.reduce(0,　// 初始值　// (1)        (sum, str) -&gt; sum+str.length(), // 累加器 // (2)        (a, b) -&gt; a+b);　// 部分和拼接器，并行执行时才会用到 // (3)// int lengthSum = stream.mapToInt(str -&gt; str.length()).sum();System.out.println(lengthSum);\n上述代码标号(2)处将i. 字符串映射成长度，ii. 并和当前累加和相加。这显然是两步操作，使用reduce()函数将这两步合二为一，更有助于提升性能。如果想要使用map()和sum()组合来达到上述目的，也是可以的。\nreduce()擅长的是生成一个值，如果想要从Stream生成一个集合或者Map等复杂的对象该怎么办呢？终极武器collect()横空出世！\n3. 终极武器collect()不夸张的讲，如果你发现某个功能在Stream接口中没找到，十有八九可以通过collect()方法实现。collect()是Stream接口方法中最灵活的一个，学会它才算真正入门Java函数式编程。先看几个热身的小例子：\n// 将Stream转换成容器或MapStream&lt;String&gt; stream = Stream.of(&quot;I&quot;, &quot;love&quot;, &quot;you&quot;, &quot;too&quot;);List&lt;String&gt; list = stream.collect(Collectors.toList()); // (1)// Set&lt;String&gt; set = stream.collect(Collectors.toSet()); // (2)// Map&lt;String, Integer&gt; map = stream.collect(Collectors.toMap(Function.identity(), String::length)); // (3)\n上述代码分别列举了如何将Stream转换成List、Set和Map。虽然代码语义很明确，可是我们仍然会有几个疑问：\n\nFunction.identity()是干什么的？\nString::length是什么意思？\nCollectors是个什么东西？\n\n4.接口的静态方法和默认方法Function是一个接口，那么Function.identity()是什么意思呢？这要从两方面解释：\n\nJava 8允许在接口中加入具体方法。接口中的具体方法有两种，default方法和static方法，identity()就是Function接口的一个静态方法。\nFunction.identity()返回一个输出跟输入一样的Lambda表达式对象，等价于形如t -&gt; t形式的Lambda表达式。\n\n上面的解释是不是让你疑问更多？不要问我为什么接口中可以有具体方法，也不要告诉我你觉得t -&gt; t比identity()方法更直观。我会告诉你接口中的default方法是一个无奈之举，在Java 7及之前要想在定义好的接口中加入新的抽象方法是很困难甚至不可能的，因为所有实现了该接口的类都要重新实现。试想在Collection接口中加入一个stream()抽象方法会怎样？default方法就是用来解决这个尴尬问题的，直接在接口中实现新加入的方法。既然已经引入了default方法，为何不再加入static方法来避免专门的工具类呢！\n5.方法引用诸如String::length的语法形式叫做方法引用（method references），这种语法用来替代某些特定形式Lambda表达式。如果Lambda表达式的全部内容就是调用一个已有的方法，那么可以用方法引用来替代Lambda表达式。方法引用可以细分为四类：\n\n\n\n方法引用类别\n举例\n\n\n\n引用静态方法\nInteger::sum\n\n\n引用某个对象的方法\nlist::add\n\n\n引用某个类的方法\nString::length\n\n\n引用构造方法\nHashMap::new\n\n\n我们会在后面的例子中使用方法引用。\n6.收集器相信前面繁琐的内容已彻底打消了你学习Java函数式编程的热情，不过很遗憾，下面的内容更繁琐。但这不能怪Stream类库，因为要实现的功能本身很复杂。\n\n\n收集器（Collector）是为Stream.collect()方法量身打造的工具接口（类）。考虑一下将一个Stream转换成一个容器（或者Map）需要做哪些工作？我们至少需要两样东西：\n\n目标容器是什么？是ArrayList还是HashSet，或者是个TreeMap。\n新元素如何添加到容器中？是List.add()还是Map.put()。\n\n如果并行的进行规约，还需要告诉collect() 3. 多个部分结果如何合并成一个。\n结合以上分析，collect()方法定义为&lt;R&gt; R collect(Supplier&lt;R&gt; supplier, BiConsumer&lt;R,? super T&gt; accumulator, BiConsumer&lt;R,R&gt; combiner)，三个参数依次对应上述三条分析。不过每次调用collect()都要传入这三个参数太麻烦，收集器Collector就是对这三个参数的简单封装,所以collect()的另一定义为&lt;R,A&gt; R collect(Collector&lt;? super T,A,R&gt; collector)。Collectors工具类可通过静态方法生成各种常用的Collector。举例来说，如果要将Stream规约成List可以通过如下两种方式实现：\n//　将Stream规约成ListStream&lt;String&gt; stream = Stream.of(&quot;I&quot;, &quot;love&quot;, &quot;you&quot;, &quot;too&quot;);List&lt;String&gt; list = stream.collect(ArrayList::new, ArrayList::add, ArrayList::addAll);// 方式１//List&lt;String&gt; list = stream.collect(Collectors.toList());// 方式2System.out.println(list);\n通常情况下我们不需要手动指定collect()的三个参数，而是调用collect(Collector&lt;? super T,A,R&gt; collector)方法，并且参数中的Collector对象大都是直接通过Collectors工具类获得。实际上传入的收集器的行为决定了collect()的行为。\n7.使用collect()生成Collection前面已经提到通过collect()方法将Stream转换成容器的方法，这里再汇总一下。将Stream转换成List或Set是比较常见的操作，所以Collectors工具已经为我们提供了对应的收集器，通过如下代码即可完成：\n// 将Stream转换成List或SetStream&lt;String&gt; stream = Stream.of(&quot;I&quot;, &quot;love&quot;, &quot;you&quot;, &quot;too&quot;);List&lt;String&gt; list = stream.collect(Collectors.toList()); // (1)Set&lt;String&gt; set = stream.collect(Collectors.toSet()); // (2)\n上述代码能够满足大部分需求，但由于返回结果是接口类型，我们并不知道类库实际选择的容器类型是什么，有时候我们可能会想要人为指定容器的实际类型，这个需求可通过Collectors.toCollection(Supplier&lt;C&gt; collectionFactory)方法完成。\n// 使用toCollection()指定规约容器的类型ArrayList&lt;String&gt; arrayList = stream.collect(Collectors.toCollection(ArrayList::new));// (3)HashSet&lt;String&gt; hashSet = stream.collect(Collectors.toCollection(HashSet::new));// (4)\n上述代码(3)处指定规约结果是ArrayList，而(4)处指定规约结果为HashSet。一切如你所愿。\n8.使用collect()生成Map前面已经说过Stream背后依赖于某种数据源，数据源可以是数组、容器等，但不能是Map。反过来从Stream生成Map是可以的，但我们要想清楚Map的key和value分别代表什么，根本原因是我们要想清楚要干什么。通常在三种情况下collect()的结果会是Map：\n\n使用Collectors.toMap()生成的收集器，用户需要指定如何生成Map的key和value。\n使用Collectors.partitioningBy()生成的收集器，对元素进行二分区操作时用到。\n使用Collectors.groupingBy()生成的收集器，对元素做group操作时用到。\n\n情况1：使用toMap()生成的收集器，这种情况是最直接的，前面例子中已提到，这是和Collectors.toCollection()并列的方法。如下代码展示将学生列表转换成由&lt;学生，GPA&gt;组成的Map。非常直观，无需多言。\n// 使用toMap()统计学生GPAMap&lt;Student, Double&gt; studentToGPA =     students.stream().collect(Collectors.toMap(Function.identity(),// 如何生成key                                     student -&gt; computeGPA(student)));// 如何生成value\n情况2：使用partitioningBy()生成的收集器，这种情况适用于将Stream中的元素依据某个二值逻辑（满足条件，或不满足）分成互补相交的两部分，比如男女性别、成绩及格与否等。下列代码展示将学生分成成绩及格或不及格的两部分。\n// Partition students into passing and failingMap&lt;Boolean, List&lt;Student&gt;&gt; passingFailing = students.stream()         .collect(Collectors.partitioningBy(s -&gt; s.getGrade() &gt;= PASS_THRESHOLD));\n情况3：使用groupingBy()生成的收集器，这是比较灵活的一种情况。跟SQL中的group by语句类似，这里的groupingBy()也是按照某个属性对数据进行分组，属性相同的元素会被对应到Map的同一个key上。下列代码展示将员工按照部门进行分组：\n// Group employees by departmentMap&lt;Department, List&lt;Employee&gt;&gt; byDept = employees.stream()            .collect(Collectors.groupingBy(Employee::getDepartment));\n以上只是分组的最基本用法，有些时候仅仅分组是不够的。在SQL中使用group by是为了协助其他查询，比如1. 先将员工按照部门分组，2. 然后统计每个部门员工的人数。Java类库设计者也考虑到了这种情况，增强版的groupingBy()能够满足这种需求。增强版的groupingBy()允许我们对元素分组之后再执行某种运算，比如求和、计数、平均值、类型转换等。这种先将元素分组的收集器叫做上游收集器，之后执行其他运算的收集器叫做下游收集器(downstream Collector)。\n// 使用下游收集器统计每个部门的人数Map&lt;Department, Integer&gt; totalByDept = employees.stream()                    .collect(Collectors.groupingBy(Employee::getDepartment,                                                   Collectors.counting()));// 下游收集器\n上面代码的逻辑是不是越看越像SQL？高度非结构化。还有更狠的，下游收集器还可以包含更下游的收集器，这绝不是为了炫技而增加的把戏，而是实际场景需要。考虑将员工按照部门分组的场景，如果我们想得到每个员工的名字（字符串），而不是一个个Employee对象，可通过如下方式做到：\n// 按照部门对员工分布组，并只保留员工的名字Map&lt;Department, List&lt;String&gt;&gt; byDept = employees.stream()                .collect(Collectors.groupingBy(Employee::getDepartment,                        Collectors.mapping(Employee::getName,// 下游收集器                                Collectors.toList())));// 更下游的收集器\n如果看到这里你还没有对Java函数式编程失去信心，恭喜你，你已经顺利成为Java函数式编程大师了。\n9.使用collect()做字符串join这个肯定是大家喜闻乐见的功能，字符串拼接时使用Collectors.joining()生成的收集器，从此告别for循环。Collectors.joining()方法有三种重写形式，分别对应三种不同的拼接方式。无需多言，代码过目难忘。\n// 使用Collectors.joining()拼接字符串Stream&lt;String&gt; stream = Stream.of(&quot;I&quot;, &quot;love&quot;, &quot;you&quot;);//String joined = stream.collect(Collectors.joining());// &quot;Iloveyou&quot;//String joined = stream.collect(Collectors.joining(&quot;,&quot;));// &quot;I,love,you&quot;String joined = stream.collect(Collectors.joining(&quot;,&quot;, &quot;&#123;&quot;, &quot;&#125;&quot;));// &quot;&#123;I,love,you&#125;&quot;\n10.collect()还可以做更多除了可以使用Collectors工具类已经封装好的收集器，我们还可以自定义收集器，或者直接调用collect(Supplier&lt;R&gt; supplier, BiConsumer&lt;R,? super T&gt; accumulator, BiConsumer&lt;R,R&gt; combiner)方法，收集任何形式你想要的信息。不过Collectors工具类应该能满足我们的绝大部分需求，手动实现之间请先看看文档。\n四、Stream Pipelines前面我们已经学会如何使用Stream API，用起来真的很爽，但简洁的方法下面似乎隐藏着无尽的秘密，如此强大的API是如何实现的呢？比如Pipeline是怎么执行的，每次方法调用都会导致一次迭代吗？自动并行又是怎么做到的，线程个数是多少？接下来我们学习Stream流水线的原理，这是Stream实现的关键所在。\n首先回顾一下容器执行Lambda表达式的方式，以ArrayList.forEach()方法为例，具体代码如下：\n// ArrayList.forEach()public void forEach(Consumer&lt;? super E&gt; action) &#123;    ...    for (int i=0; modCount == expectedModCount &amp;&amp; i &lt; size; i++) &#123;        action.accept(elementData[i]);// 回调方法    &#125;    ...&#125;\n\n我们看到ArrayList.forEach()方法的主要逻辑就是一个for循环，在该for循环里不断调用action.accept()回调方法完成对元素的遍历。这完全没有什么新奇之处，回调方法在Java GUI的监听器中广泛使用。Lambda表达式的作用就是相当于一个回调方法，这很好理解。\nStream API中大量使用Lambda表达式作为回调方法，但这并不是关键。理解Stream我们更关心的是另外两个问题：流水线和自动并行。使用Stream或许很容易写入如下形式的代码：\nint longestStringLengthStartingWithA        = strings.stream()              .filter(s -&gt; s.startsWith(&quot;A&quot;))              .mapToInt(String::length)              .max();\n\n上述代码求出以字母A开头的字符串的最大长度，一种直白的方式是为每一次函数调用都执一次迭代，这样做能够实现功能，但效率上肯定是无法接受的。类库的实现着使用流水线（Pipeline）的方式巧妙的避免了多次迭代，其基本思想是在一次迭代中尽可能多的执行用户指定的操作。为讲解方便我们汇总了Stream的所有操作。\nStream操作分类中间操作(Intermediate operations)无状态(Stateless)unordered() filter() map() mapToInt() mapToLong() mapToDouble() flatMap() flatMapToInt() flatMapToLong() flatMapToDouble() peek()有状态(Stateful)distinct() sorted() sorted() limit() skip() 结束操作(Terminal operations)非短路操作forEach() forEachOrdered() toArray() reduce() collect() max() min() count()短路操作(short-circuiting)anyMatch() allMatch() noneMatch() findFirst() findAny()\n\nStream上的所有操作分为两类：中间操作和结束操作，中间操作只是一种标记，只有结束操作才会触发实际计算。中间操作又可以分为无状态的(Stateless)和有状态的(Stateful)，无状态中间操作是指元素的处理不受前面元素的影响，而有状态的中间操作必须等到所有元素处理之后才知道最终结果，比如排序是有状态操作，在读取所有元素之前并不能确定排序结果；结束操作又可以分为短路操作和非短路操作，短路操作是指不用处理全部元素就可以返回结果，比如找到第一个满足条件的元素。之所以要进行如此精细的划分，是因为底层对每一种情况的处理方式不同。为了更好的理解流的中间操作和终端操作，可以通过下面的两段代码来看他们的执行过程。\nIntStream.range(1, 10)   .peek(x -&gt; System.out.print(&quot;\\nA&quot; + x))   .limit(3)   .peek(x -&gt; System.out.print(&quot;B&quot; + x))   .forEach(x -&gt; System.out.print(&quot;C&quot; + x));\n输出为：A1B1C1A2B2C2A3B3C3中间操作是懒惰的，也就是中间操作不会对数据做任何操作，直到遇到了最终操作。而最终操作，都是比较热情的。他们会往前回溯所有的中间操作。也就是当执行到最后的forEach操作的时候，它会回溯到它的上一步中间操作，上一步中间操作，又会回溯到上上一步的中间操作，…，直到最初的第一步。第一次forEach执行的时候，会回溯peek 操作，然后peek会回溯更上一步的limit操作，然后limit会回溯更上一步的peek操作，顶层没有操作了，开始自上向下开始执行，输出：A1B1C1第二次forEach执行的时候，然后会回溯peek 操作，然后peek会回溯更上一步的limit操作，然后limit会回溯更上一步的peek操作，顶层没有操作了，开始自上向下开始执行，输出：A2B2C2\n…当第四次forEach执行的时候，然后会回溯peek 操作，然后peek会回溯更上一步的limit操作，到limit的时候，发现limit(3)这个job已经完成，这里就相当于循环里面的break操作，跳出来终止循环。\n再来看第二段代码：\nIntStream.range(1, 10)   .peek(x -&gt; System.out.print(&quot;\\nA&quot; + x))   .skip(6)   .peek(x -&gt; System.out.print(&quot;B&quot; + x))   .forEach(x -&gt; System.out.print(&quot;C&quot; + x));\n输出为：A1A2A3A4A5A6A7B7C7A8B8C8A9B9C9第一次forEach执行的时候，会回溯peek操作，然后peek会回溯更上一步的skip操作，skip回溯到上一步的peek操作，顶层没有操作了，开始自上向下开始执行，执行到skip的时候，因为执行到skip，这个操作的意思就是跳过，下面的都不要执行了，也就是就相当于循环里面的continue，结束本次循环。输出：A1\n第二次forEach执行的时候，会回溯peek操作，然后peek会回溯更上一步的skip操作，skip回溯到上一步的peek操作，顶层没有操作了，开始自上向下开始执行，执行到skip的时候，发现这是第二次skip，结束本次循环。输出：A2\n…\n第七次forEach执行的时候，会回溯peek操作，然后peek会回溯更上一步的skip操作，skip回溯到上一步的peek操作，顶层没有操作了，开始自上向下开始执行，执行到skip的时候，发现这是第七次skip，已经大于6了，它已经执行完了skip(6)的job了。这次skip就直接跳过，继续执行下面的操作。输出：A7B7C7\n…直到循环结束。\n1.一种直白的实现方式\n\n仍然考虑上述求最长字符串的程序，一种直白的流水线实现方式是为每一次函数调用都执一次迭代，并将处理中间结果放到某种数据结构中（比如数组，容器等）。具体说来，就是调用filter()方法后立即执行，选出所有以A开头的字符串并放到一个列表list1中，之后让list1传递给mapToInt()方法并立即执行，生成的结果放到list2中，最后遍历list2找出最大的数字作为最终结果。程序的执行流程如如所示：\n这样做实现起来非常简单直观，但有两个明显的弊端：\n\n迭代次数多。迭代次数跟函数调用的次数相等。\n频繁产生中间结果。每次函数调用都产生一次中间结果，存储开销无法接受。\n\n这些弊端使得效率底下，根本无法接受。如果不使用Stream API我们都知道上述代码该如何在一次迭代中完成，大致是如下形式：\nint longest = 0;for(String str : strings)&#123;    if(str.startsWith(&quot;A&quot;))&#123;// 1. filter(), 保留以A开头的字符串        int len = str.length();// 2. mapToInt(), 转换成长度        longest = Math.max(len, longest);// 3. max(), 保留最长的长度    &#125;&#125;\n\n采用这种方式我们不但减少了迭代次数，也避免了存储中间结果，显然这就是流水线，因为我们把三个操作放在了一次迭代当中。只要我们事先知道用户意图，总是能够采用上述方式实现跟Stream API等价的功能，但问题是Stream类库的设计者并不知道用户的意图是什么。如何在无法假设用户行为的前提下实现流水线，是类库的设计者要考虑的问题。\n2.Stream流水线解决方案我们大致能够想到，应该采用某种方式记录用户每一步的操作，当用户调用结束操作时将之前记录的操作叠加到一起在一次迭代中全部执行掉。沿着这个思路，有几个问题需要解决：\n\n用户的操作如何记录？\n操作如何叠加？\n叠加之后的操作如何执行？\n执行后的结果（如果有）在哪里？\n\n操作如何记录\n\n注意这里使用的是“操作(operation)”一词，指的是“Stream中间操作”的操作，很多Stream操作会需要一个回调函数（Lambda表达式），因此一个完整的操作是&lt;*数据来源，操作，回调函数*&gt;构成的三元组。Stream中使用Stage的概念来描述一个完整的操作，并用某种实例化后的PipelineHelper来代表Stage，将具有先后顺序的各个Stage连到一起，就构成了整个流水线。跟Stream相关类和接口的继承关系图示。\n还有IntPipeline, LongPipeline, DoublePipeline没在图中画出，这三个类专门为三种基本类型（不是包装类型）而定制的，跟ReferencePipeline是并列关系。图中Head用于表示第一个Stage，即调用调用诸如Collection.stream()方法产生的Stage，很显然这个Stage里不包含任何操作；StatelessOp和StatefulOp分别表示无状态和有状态的Stage，对应于无状态和有状态的中间操作。\nStream流水线组织结构示意图如下：\n\n\n图中通过Collection.stream()方法得到Head也就是stage0，紧接着调用一系列的中间操作，不断产生新的Stream。这些Stream对象以双向链表的形式组织在一起，构成整个流水线，由于每个Stage都记录了前一个Stage和本次的操作以及回调函数，依靠这种结构就能建立起对数据源的所有操作。这就是Stream记录操作的方式。\n操作如何叠加以上只是解决了操作记录的问题，要想让流水线起到应有的作用我们需要一种将所有操作叠加到一起的方案。你可能会觉得这很简单，只需要从流水线的head开始依次执行每一步的操作（包括回调函数）就行了。这听起来似乎是可行的，但是你忽略了前面的Stage并不知道后面Stage到底执行了哪种操作，以及回调函数是哪种形式。换句话说，只有当前Stage本身才知道该如何执行自己包含的动作。这就需要有某种协议来协调相邻Stage之间的调用关系。\n这种协议由Sink接口完成，Sink接口包含的方法如下表所示：\n方法名作用void begin(long size)开始遍历元素之前调用该方法，通知Sink做好准备。void end()所有元素遍历完成之后调用，通知Sink没有更多的元素了。boolean cancellationRequested()是否可以结束操作，可以让短路操作尽早结束。void accept(T t)遍历元素时调用，接受一个待处理元素，并对元素进行处理。Stage把自己包含的操作和回调方法封装到该方法里，前一个Stage只需要调用当前Stage.accept(T t)方法就行了。\n\n有了上面的协议，相邻Stage之间调用就很方便了，每个Stage都会将自己的操作封装到一个Sink里，前一个Stage只需调用后一个Stage的accept()方法即可，并不需要知道其内部是如何处理的。当然对于有状态的操作，Sink的begin()和end()方法也是必须实现的。比如Stream.sorted()是一个有状态的中间操作，其对应的Sink.begin()方法可能创建一个盛放结果的容器，而accept()方法负责将元素添加到该容器，最后end()负责对容器进行排序。对于短路操作，Sink.cancellationRequested()也是必须实现的，比如Stream.findFirst()是短路操作，只要找到一个元素，cancellationRequested()就应该返回true，以便调用者尽快结束查找。Sink的四个接口方法常常相互协作，共同完成计算任务。实际上Stream API内部实现的的本质，就是如何重写Sink的这四个接口方法。\n有了Sink对操作的包装，Stage之间的调用问题就解决了，执行时只需要从流水线的head开始对数据源依次调用每个Stage对应的Sink.{begin(), accept(), cancellationRequested(), end()}方法就可以了。一种可能的Sink.accept()方法流程是这样的：\nvoid accept(U u)&#123;    1. 使用当前Sink包装的回调函数处理u    2. 将处理结果传递给流水线下游的Sink&#125;\n\nSink接口的其他几个方法也是按照这种[处理-&gt;转发]的模型实现。下面我们结合具体例子看看Stream的中间操作是如何将自身的操作包装成Sink以及Sink是如何将处理结果转发给下一个Sink的。先看Stream.map()方法：\n// Stream.map()，调用该方法将产生一个新的Streampublic final &lt;R&gt; Stream&lt;R&gt; map(Function&lt;? super P_OUT, ? extends R&gt; mapper) &#123;    ...    return new StatelessOp&lt;P_OUT, R&gt;(this, StreamShape.REFERENCE,                                 StreamOpFlag.NOT_SORTED | StreamOpFlag.NOT_DISTINCT) &#123;        @Override /*opWripSink()方法返回由回调函数包装而成Sink*/        Sink&lt;P_OUT&gt; opWrapSink(int flags, Sink&lt;R&gt; downstream) &#123;            return new Sink.ChainedReference&lt;P_OUT, R&gt;(downstream) &#123;                @Override                public void accept(P_OUT u) &#123;                    R r = mapper.apply(u);// 1. 使用当前Sink包装的回调函数mapper处理u                    downstream.accept(r);// 2. 将处理结果传递给流水线下游的Sink                &#125;            &#125;;        &#125;    &#125;;&#125;\n\n上述代码看似复杂，其实逻辑很简单，就是将回调函数mapper包装到一个Sink当中。由于Stream.map()是一个无状态的中间操作，所以map()方法返回了一个StatelessOp内部类对象（一个新的Stream），调用这个新Stream的opWripSink()方法将得到一个包装了当前回调函数的Sink。\n再来看一个复杂一点的例子。Stream.sorted()方法将对Stream中的元素进行排序，显然这是一个有状态的中间操作，因为读取所有元素之前是没法得到最终顺序的。抛开模板代码直接进入问题本质，sorted()方法是如何将操作封装成Sink的呢？sorted()一种可能封装的Sink代码如下：\n// Stream.sort()方法用到的Sink实现class RefSortingSink&lt;T&gt; extends AbstractRefSortingSink&lt;T&gt; &#123;    private ArrayList&lt;T&gt; list;// 存放用于排序的元素    RefSortingSink(Sink&lt;? super T&gt; downstream, Comparator&lt;? super T&gt; comparator) &#123;        super(downstream, comparator);    &#125;    @Override    public void begin(long size) &#123;        ...        // 创建一个存放排序元素的列表        list = (size &gt;= 0) ? new ArrayList&lt;T&gt;((int) size) : new ArrayList&lt;T&gt;();    &#125;    @Override    public void end() &#123;        list.sort(comparator);// 只有元素全部接收之后才能开始排序        downstream.begin(list.size());        if (!cancellationWasRequested) &#123;// 下游Sink不包含短路操作            list.forEach(downstream::accept);// 2. 将处理结果传递给流水线下游的Sink        &#125;        else &#123;// 下游Sink包含短路操作            for (T t : list) &#123;// 每次都调用cancellationRequested()询问是否可以结束处理。                if (downstream.cancellationRequested()) break;                downstream.accept(t);// 2. 将处理结果传递给流水线下游的Sink            &#125;        &#125;        downstream.end();        list = null;    &#125;    @Override    public void accept(T t) &#123;        list.add(t);// 1. 使用当前Sink包装动作处理t，只是简单的将元素添加到中间列表当中    &#125;&#125;\n\n上述代码完美的展现了Sink的四个接口方法是如何协同工作的：\n\n首先begin()方法告诉Sink参与排序的元素个数，方便确定中间结果容器的的大小；\n之后通过accept()方法将元素添加到中间结果当中，最终执行时调用者会不断调用该方法，直到遍历所有元素；\n最后end()方法告诉Sink所有元素遍历完毕，启动排序步骤，排序完成后将结果传递给下游的Sink；\n如果下游的Sink是短路操作，将结果传递给下游时不断询问下游cancellationRequested()是否可以结束处理。\n\n叠加之后的操作如何执行\n\nSink完美封装了Stream每一步操作，并给出了[处理-&gt;转发]的模式来叠加操作。这一连串的齿轮已经咬合，就差最后一步拨动齿轮启动执行。是什么启动这一连串的操作呢？也许你已经想到了启动的原始动力就是结束操作(Terminal Operation)，一旦调用某个结束操作，就会触发整个流水线的执行。\n结束操作之后不能再有别的操作，所以结束操作不会创建新的流水线阶段(Stage)，直观的说就是流水线的链表不会在往后延伸了。结束操作会创建一个包装了自己操作的Sink，这也是流水线中最后一个Sink，这个Sink只需要处理数据而不需要将结果传递给下游的Sink（因为没有下游）。对于Sink的[处理-&gt;转发]模型，结束操作的Sink就是调用链的出口。\n我们再来考察一下上游的Sink是如何找到下游Sink的。一种可选的方案是在PipelineHelper中设置一个Sink字段，在流水线中找到下游Stage并访问Sink字段即可。但Stream类库的设计者没有这么做，而是设置了一个Sink AbstractPipeline.opWrapSink(int flags, Sink downstream)方法来得到Sink，该方法的作用是返回一个新的包含了当前Stage代表的操作以及能够将结果传递给downstream的Sink对象。为什么要产生一个新对象而不是返回一个Sink字段？这是因为使用opWrapSink()可以将当前操作与下游Sink（上文中的downstream参数）结合成新Sink。试想只要从流水线的最后一个Stage开始，不断调用上一个Stage的opWrapSink()方法直到最开始（不包括stage0，因为stage0代表数据源，不包含操作），就可以得到一个代表了流水线上所有操作的Sink，用代码表示就是这样：\n// AbstractPipeline.wrapSink()// 从下游向上游不断包装Sink。如果最初传入的sink代表结束操作，// 函数返回时就可以得到一个代表了流水线上所有操作的Sink。final &lt;P_IN&gt; Sink&lt;P_IN&gt; wrapSink(Sink&lt;E_OUT&gt; sink) &#123;    ...    for (AbstractPipeline p=AbstractPipeline.this; p.depth &gt; 0; p=p.previousStage) &#123;        sink = p.opWrapSink(p.previousStage.combinedFlags, sink);    &#125;    return (Sink&lt;P_IN&gt;) sink;&#125;\n\n现在流水线上从开始到结束的所有的操作都被包装到了一个Sink里，执行这个Sink就相当于执行整个流水线，执行Sink的代码如下：\n// AbstractPipeline.copyInto(), 对spliterator代表的数据执行wrappedSink代表的操作。final &lt;P_IN&gt; void copyInto(Sink&lt;P_IN&gt; wrappedSink, Spliterator&lt;P_IN&gt; spliterator) &#123;    ...    if (!StreamOpFlag.SHORT_CIRCUIT.isKnown(getStreamAndOpFlags())) &#123;        wrappedSink.begin(spliterator.getExactSizeIfKnown());// 通知开始遍历        spliterator.forEachRemaining(wrappedSink);// 迭代        wrappedSink.end();// 通知遍历结束    &#125;    ...&#125;\n\n上述代码首先调用wrappedSink.begin()方法告诉Sink数据即将到来，然后调用spliterator.forEachRemaining()方法对数据进行迭代（Spliterator是容器的一种迭代器，参阅），最后调用wrappedSink.end()方法通知Sink数据处理结束。逻辑如此清晰。\n执行后的结果在哪里最后一个问题是流水线上所有操作都执行后，用户所需要的结果（如果有）在哪里？首先要说明的是不是所有的Stream结束操作都需要返回结果，有些操作只是为了使用其副作用(Side-effects)，比如使用Stream.forEach()方法将结果打印出来就是常见的使用副作用的场景（事实上，除了打印之外其他场景都应避免使用副作用），对于真正需要返回结果的结束操作结果存在哪里呢？\n\n特别说明：副作用不应该被滥用，也许你会觉得在Stream.forEach()里进行元素收集是个不错的选择，就像下面代码中那样，但遗憾的是这样使用的正确性和效率都无法保证，因为Stream可能会并行执行。大多数使用副作用的地方都可以使用归约操作更安全和有效的完成。\n\n// 错误的收集方式ArrayList&lt;String&gt; results = new ArrayList&lt;&gt;();stream.filter(s -&gt; pattern.matcher(s).matches())      .forEach(s -&gt; results.add(s));  // Unnecessary use of side-effects!// 正确的收集方式List&lt;String&gt;results =     stream.filter(s -&gt; pattern.matcher(s).matches())             .collect(Collectors.toList());  // No side-effects!\n\n回到流水线执行结果的问题上来，需要返回结果的流水线结果存在哪里呢？这要分不同的情况讨论，下表给出了各种有返回结果的Stream结束操作。\n返回类型对应的结束操作booleananyMatch() allMatch() noneMatch()OptionalfindFirst() findAny()归约结果reduce() collect()数组toArray()\n\n\n对于表中返回boolean或者Optional的操作（Optional是存放 一个 值的容器）的操作，由于值返回一个值，只需要在对应的Sink中记录这个值，等到执行结束时返回就可以了。\n对于归约操作，最终结果放在用户调用时指定的容器中（容器类型通过收集器指定）。collect(), reduce(), max(), min()都是归约操作，虽然max()和min()也是返回一个Optional，但事实上底层是通过调用reduce()方法实现的。\n对于返回是数组的情况，毫无疑问的结果会放在数组当中。这么说当然是对的，但在最终返回数组之前，结果其实是存储在一种叫做Node的数据结构中的。Node是一种多叉树结构，元素存储在树的叶子当中，并且一个叶子节点可以存放多个元素。这样做是为了并行执行方便。关于Node的具体结构，我们会在下一节探究Stream如何并行执行时给出详细说明。\n\n3.结语本节详细介绍了Stream流水线的组织方式和执行过程，学习本文将有助于理解原理并写出正确的Stream代码，同时打消你对Stream API效率方面的顾虑。如你所见，Stream API实现如此巧妙，即使我们使用外部迭代手动编写等价代码，也未必更加高效。\n注：留下本文所用的JDK版本，以便有考究癖的人考证：\n$ java -versionjava version &quot;1.8.0_101&quot;Java(TM) SE Runtime Environment (build 1.8.0_101-b13)Java HotSpot(TM) Server VM (build 25.101-b13, mixed mode)\n\n\n\n\n\n五、parallelStream 介绍我们已经对Stream有过很多的了解，对其原理及常见使用方法已经也有了一定的认识。流在处理数据进行一些迭代操作的时候确认很方便，但是在执行一些耗时或是占用资源很高的任务时候，串行化的流无法带来速度&#x2F;性能上的提升，并不能满足我们的需要，通常我们会使用多线程来并行或是分片分解执行任务，而在Stream中也提供了这样的并行方法，那就是使用parallelStream()方法或者是使用stream().parallel()来转化为并行流。开箱即用的并行流的使用看起来如此简单，然后我们就可能会忍不住思考，并行流的实现原理是怎样的？它的使用会给我们带来多大的性能提升？我们可以在什么场景下使用以及使用时应该注意些什么？\n首先我们看一下Java 的并行 API 演变历程基本如下：\n\n1.0-1.4 中的 java.lang.Thread\n5.0 中的 java.util.concurrent\n6.0 中的 Phasers 等\n7.0 中的 Fork&#x2F;Join 框架\n8.0 中的 Lambda\n\n1.parallelStream是什么？先看一下Collection接口提供的并行流方法\n/** * Returns a possibly parallel &#123;@code Stream&#125; with this collection as its * source.  It is allowable for this method to return a sequential stream. * * &lt;p&gt;This method should be overridden when the &#123;@link #spliterator()&#125; * method cannot return a spliterator that is &#123;@code IMMUTABLE&#125;, * &#123;@code CONCURRENT&#125;, or &lt;em&gt;late-binding&lt;/em&gt;. (See &#123;@link #spliterator()&#125; * for details.) * * @implSpec * The default implementation creates a parallel &#123;@code Stream&#125; from the * collection&#x27;s &#123;@code Spliterator&#125;. * * @return a possibly parallel &#123;@code Stream&#125; over the elements in this * collection * @since 1.8 */default Stream&lt;E&gt; parallelStream() &#123;    return StreamSupport.stream(spliterator(), true);&#125;\n注意其中的代码注释的返回值 @return a possibly parallel 一句说明调用了这个方法，只是可能会返回一个并行的流，流是否能并行执行还受到其他一些条件的约束。parallelStream其实就是一个并行执行的流，它通过默认的ForkJoinPool，可能提高你的多线程任务的速度。引用Custom thread pool in Java 8 parallel stream上面的两段话：\n\nThe parallel streams use the default ForkJoinPool.commonPool which by default has one less threads as you have processors, as returned by Runtime.getRuntime().availableProcessors() (This means that parallel streams use all your processors because they also use the main thread)。\n\n做个实验来证明上面这句话的真实性：\npublic static void main(String[] args) &#123;    IntStream list = IntStream.range(0, 10);    Set&lt;Thread&gt; threadSet = new HashSet&lt;&gt;();    //开始并行执行    list.parallel().forEach(i -&gt; &#123;        Thread thread = Thread.currentThread();        System.err.println(&quot;integer：&quot; + i + &quot;，&quot; + &quot;currentThread:&quot; + thread.getName());        threadSet.add(thread);    &#125;);    System.out.println(&quot;all threads：&quot; + Joiner.on(&quot;，&quot;).join(threadSet.stream().map(Thread::getName).collect(Collectors.toList())));&#125;\n\n\n从运行结果里面我们可以很清楚的看到parallelStream同时使用了主线程和ForkJoinPool.commonPool创建的线程。值得说明的是这个运行结果并不是唯一的，实际运行的时候可能会得到多个结果，比如：\n\n\n甚至你的运行结果里面只有主线程。\n来源于java 8 实战的书籍的一段话：\n\n并行流内部使用了默认的ForkJoinPool（7.2节会进一步讲到分支&#x2F;合并框架），它默认的线程数量就是你的处理器数量，这个值是由Runtime.getRuntime().available- Processors()得到的。 但是你可以通过系统属性java.util.concurrent.ForkJoinPool.common. parallelism来改变线程池大小，如下所示： System.setProperty(&quot;java.util.concurrent.ForkJoinPool.common.parallelism&quot;,&quot;12&quot;); 这是一个全局设置，因此它将影响代码中所有的并行流。反过来说，目前还无法专为某个 并行流指定这个值。一般而言，让ForkJoinPool的大小等于处理器数量是个不错的默认值， 除非你有很好的理由，否则我们强烈建议你不要修改它。\n\n// 设置全局并行流并发线程数System.setProperty(&quot;java.util.concurrent.ForkJoinPool.common.parallelism&quot;, &quot;12&quot;);System.out.println(ForkJoinPool.getCommonPoolParallelism());// 输出 12System.setProperty(&quot;java.util.concurrent.ForkJoinPool.common.parallelism&quot;, &quot;20&quot;);System.out.println(ForkJoinPool.getCommonPoolParallelism());// 输出 12\n为什么两次的运行结果是一样的呢？上面刚刚说过了这是一个全局设置，java.util.concurrent.ForkJoinPool.common.parallelism是final类型的，整个JVM中只允许设置一次。既然默认的并发线程数不能反复修改，那怎么进行不同线程数量的并发测试呢？答案是：引入ForkJoinPool\nIntStream range = IntStream.range(1, 100000);// 传入parallelismnew ForkJoinPool(parallelism).submit(() -&gt; range.parallel().forEach(System.out::println)).get();\n因此，使用parallelStream时需要注意的一点是，多个parallelStream之间默认使用的是同一个线程池，所以IO操作尽量不要放进parallelStream中，否则会阻塞其他parallelStream。\n\nUsing a ForkJoinPool and submit for a parallel stream does not reliably use all threads. If you look at this ( Parallel stream from a HashSet doesn’t run in parallel ) and this ( Why does the parallel stream not use all the threads of the ForkJoinPool? ), you’ll see the reasoning.\n\n// 获取当前机器CPU处理器的数量System.out.println(Runtime.getRuntime().availableProcessors());// 输出 4// parallelStream默认的并发线程数System.out.println(ForkJoinPool.getCommonPoolParallelism());// 输出 3\n为什么parallelStream默认的并发线程数要比CPU处理器的数量少1个？文章的开始已经提过了。因为最优的策略是每个CPU处理器分配一个线程，然而主线程也算一个线程，所以要占一个名额。这一点可以从源码中看出来：\nstatic final int MAX_CAP      = 0x7fff;        // max #workers - 1// 无参构造函数public ForkJoinPool() &#123;        this(Math.min(MAX_CAP, Runtime.getRuntime().availableProcessors()),             defaultForkJoinWorkerThreadFactory, null, false);&#125;bs-channel\n\n2.从parallelStream认识Fork&#x2F;Join 框架Fork&#x2F;Join 框架的核心是采用分治法的思想，将一个大任务拆分为若干互不依赖的子任务，把这些子任务分别放到不同的队列里，并为每个队列创建一个单独的线程来执行队列里的任务。同时，为了最大限度地提高并行处理能力，采用了工作窃取算法来运行任务，也就是说当某个线程处理完自己工作队列中的任务后，尝试当其他线程的工作队列中窃取一个任务来执行，直到所有任务处理完毕。所以为了减少线程之间的竞争，通常会使用双端队列，被窃取任务线程永远从双端队列的头部拿任务执行，而窃取任务的线程永远从双端队列的尾部拿任务执行。\n\nFork&#x2F;Join 的运行流程图\n\n简单地说就是大任务拆分成小任务，分别用不同线程去完成，然后把结果合并后返回。所以第一步是拆分，第二步是分开运算，第三步是合并。这三个步骤分别对应的就是Collector的supplier,accumulator和combiner。\n\n工作窃取算法Fork&#x2F;Join最核心的地方就是利用了现代硬件设备多核,在一个操作时候会有空闲的CPU,那么如何利用好这个空闲的cpu就成了提高性能的关键,而这里我们要提到的工作窃取（work-stealing）算法就是整个Fork&#x2F;Join框架的核心理念,工作窃取（work-stealing）算法是指某个线程从其他队列里窃取任务来执行。  \n\n3.使用parallelStream的利弊使用parallelStream的几个好处：\n\n代码优雅，可以使用lambda表达式，原本几句代码现在一句可以搞定；\n运用多核特性(forkAndJoin)并行处理，大幅提高效率。关于并行流和多线程的性能测试可以看一下下面的几篇博客：并行流适用场景-CPU密集型提交订单性能优化系列之006-普通的Thread多线程改为Java8的parallelStream并发流\n\n然而，任何事物都不是完美的，并行流也不例外，其中最明显的就是使用(parallel)Stream极其不便于代码的跟踪调试，此外并行流带来的不确定性也使得我们对它的使用变得格外谨慎。我们得去了解更多的并行流的相关知识来保证自己能够正确的使用这把双刃剑。\nparallelStream使用时需要注意的点：\n\nparallelStream是线程不安全的；List&lt;Integer&gt; values = new ArrayList&lt;&gt;();IntStream.range(1, 10000).parallel().forEach(values::add);System.out.println(values.size());\nvalues集合大小可能不是10000。集合里面可能会存在null元素或者抛出下标越界的异常信息。原因：List不是线程安全的集合，add方法在多线程环境下会存在并发问题。当执行add方法时，会先将此容器的大小增加。。即size++，然后将传进的元素赋值给新增的elementData[size++]，即新的内存空间。但是此时如果在size++后直接来取这个List,而没有让add完成赋值操作，则会导致此List的长度加一，，但是最后一个元素是空（null），所以在获取它进行计算的时候报了空指针异常。而下标越界还不能仅仅依靠这个来解释，如果你观察发生越界时的数组下标，分别为10、15、22、33、49和73。结合前面讲的数组自动机制，数组初始长度为10，第一次扩容为15&#x3D;10+10&#x2F;2，第二次扩容22&#x3D;15+15&#x2F;2，第三次扩容33&#x3D;22+22&#x2F;2…以此类推，我们不难发现，越界异常都发生在数组扩容之时。grow()方法解释了基于数组的ArrayList是如何扩容的。数组进行扩容时，会将老数组中的元素重新拷贝一份到新的数组中，通过oldCapacity + (oldCapacity &gt;&gt; 1)运算，每次数组容量的增长大约是其原容量的1.5倍。 /** * Increases the capacity to ensure that it can hold at least the * number of elements specified by the minimum capacity argument. * * @param minCapacity the desired minimum capacity */private void grow(int minCapacity) &#123;    // overflow-conscious code    int oldCapacity = elementData.length;    int newCapacity = oldCapacity + (oldCapacity &gt;&gt; 1);// 1.5倍扩容    if (newCapacity - minCapacity &lt; 0)        newCapacity = minCapacity;    if (newCapacity - MAX_ARRAY_SIZE &gt; 0)        newCapacity = hugeCapacity(minCapacity);    // minCapacity is usually close to size, so this is a win:    elementData = Arrays.copyOf(elementData, newCapacity);// 拷贝旧的数组到新的数组中&#125;/** * Appends the specified element to the end of this list. * * @param e element to be appended to this list * @return &lt;tt&gt;true&lt;/tt&gt; (as specified by &#123;@link Collection#add&#125;) */public boolean add(E e) &#123;    ensureCapacityInternal(size + 1);  // Increments modCount!! 检查array容量    elementData[size++] = e;// 赋值，增大Size的值    return true;&#125;\n解决方法：加锁、使用线程安全的集合或者采用collect()或者reduce()操作就是满足线程安全的了。List&lt;Integer&gt; values = new ArrayList&lt;&gt;();for (int i = 0; i &lt; 10000; i++) &#123;    values.add(i);&#125;List&lt;Integer&gt; collect = values.stream().parallel().collect(Collectors.toList());System.out.println(collect.size());\nparallelStream 适用的场景是CPU密集型的，只是做到别浪费CPU，假如本身电脑CPU的负载很大，那还到处用并行流，那并不能起到作用；\n\n\nI&#x2F;O密集型 磁盘I&#x2F;O、网络I&#x2F;O都属于I&#x2F;O操作，这部分操作是较少消耗CPU资源，一般并行流中不适用于I&#x2F;O密集型的操作，就比如使用并流行进行大批量的消息推送，涉及到了大量I&#x2F;O，使用并行流反而慢了很多\nCPU密集型 计算类型就属于CPU密集型了，这种操作并行流就能提高运行效率。\n\n\n不要在多线程中使用parallelStream，原因同上类似，大家都抢着CPU是没有提升效果，反而还会加大线程切换开销；\n会带来不确定性，请确保每条处理无状态且没有关联；\n考虑NQ模型：N可用的数据量，Q针对每个数据元素执行的计算量，乘积 N * Q 越大，就越有可能获得并行提速。N * Q&gt;10000（大概是集合大小超过1000） 就会获得有效提升；\nparallelStream是创建一个并行的Stream,而且它的并行操作是不具备线程传播性的,所以是无法获取ThreadLocal创建的线程变量的值；\n在使用并行流的时候是无法保证元素的顺序的，也就是即使你用了同步集合也只能保证元素都正确但无法保证其中的顺序；\nlambda的执行并不是瞬间完成的，所有使用parallel stream的程序都有可能成为阻塞程序的源头，并且在执行过程中程序中的其他部分将无法访问这些workers，这意味着任何依赖parallel streams的程序在什么别的东西占用着common ForkJoinPool时将会变得不可预知并且暗藏危机。\n\n六、Stream性能测验我们已经对Stream API的用法聊了这么多了，用起简洁直观，但性能到底怎么样呢？会不会有很高的性能损失？本节我们对Stream API的性能一探究竟。\n为保证测试结果真实可信，我们将JVM运行在-server模式下，测试数据在GB量级，测试机器采用常见的商用服务器，配置如下：\nOSCentOS 6.7 x86_64CPUIntel Xeon X5675, 12M Cache 3.06 GHz, 6 Cores 12 Threads内存96GBJDKjava version 1.8.0_91, Java HotSpot(TM) 64-Bit Server VM\n\n测试所用代码在这里，测试结果汇总.\n1.测试方法和测试数据性能测试并不是容易的事，Java性能测试更费劲，因为虚拟机对性能的影响很大，JVM对性能的影响有两方面：\n\nGC的影响。GC的行为是Java中很不好控制的一块，为增加确定性，我们手动指定使用CMS收集器，并使用10GB固定大小的堆内存。具体到JVM参数就是-XX:+UseConcMarkSweepGC -Xms10G -Xmx10G\nJIT(Just-In-Time)即时编译技术。即时编译技术会将热点代码在JVM运行的过程中编译成本地代码，测试时我们会先对程序预热，触发对测试函数的即时编译。相关的JVM参数是-XX:CompileThreshold=10000。\n\nStream并行执行时用到ForkJoinPool.commonPool()得到的线程池，为控制并行度我们使用Linux的taskset命令指定JVM可用的核数。\n测试数据由程序随机生成。为防止一次测试带来的抖动，测试4次求出平均时间作为运行时间。\n2.实验一 基本类型迭代测试内容：找出整型数组中的最小值。对比for循环外部迭代和Stream API内部迭代性能。\n测试程序IntTest，测试结果如下图：\n\n\n图中展示的是for循环外部迭代耗时为基准的时间比值。分析如下：\n\n对于基本类型Stream串行迭代的性能开销明显高于外部迭代开销（两倍）；\nStream并行迭代的性能比串行迭代和外部迭代都好。\n\n并行迭代性能跟可利用的核数有关，上图中的并行迭代使用了全部12个核，为考察使用核数对性能的影响，我们专门测试了不同核数下的Stream并行迭代效果：\n\n\n分析，对于基本类型：\n\n使用Stream并行API在单核情况下性能很差，比Stream串行API的性能还差；\n随着使用核数的增加，Stream并行效果逐渐变好，比使用for循环外部迭代的性能还好。\n\n以上两个测试说明，对于基本类型的简单迭代，Stream串行迭代性能更差，但多核情况下Stream迭代时性能较好。\n3.实验二 对象迭代再来看对象的迭代效果。\n测试内容：找出字符串列表中最小的元素（自然顺序），对比for循环外部迭代和Stream API内部迭代性能。\n测试程序StringTest，测试结果如下图：\n\n\n结果分析如下：\n\n对于对象类型Stream串行迭代的性能开销仍然高于外部迭代开销（1.5倍），但差距没有基本类型那么大。\nStream并行迭代的性能比串行迭代和外部迭代都好。\n\n再来单独考察Stream并行迭代效果：\n\n\n分析，对于对象类型：\n\n使用Stream并行API在单核情况下性能比for循环外部迭代差；\n随着使用核数的增加，Stream并行效果逐渐变好，多核带来的效果明显。\n\n以上两个测试说明，对于对象类型的简单迭代，Stream串行迭代性能更差，但多核情况下Stream迭代时性能较好。\n4.实验三 复杂对象归约从实验一、二的结果来看，Stream串行执行的效果都比外部迭代差（很多），是不是说明Stream真的不行了？先别下结论，我们再来考察一下更复杂的操作。\n测试内容：给定订单列表，统计每个用户的总交易额。对比使用外部迭代手动实现和Stream API之间的性能。\n我们将订单简化为&lt;userName, price, timeStamp&gt;构成的元组，并用Order对象来表示。测试程序ReductionTest，测试结果如下图：\n\n\n分析，对于复杂的归约操作：\n\nStream API的性能普遍好于外部手动迭代，并行Stream效果更佳；\n\n再来考察并行度对并行效果的影响，测试结果如下：\n\n\n分析，对于复杂的归约操作：\n\n使用Stream并行归约在单核情况下性能比串行归约以及手动归约都要差，简单说就是最差的；\n随着使用核数的增加，Stream并行效果逐渐变好，多核带来的效果明显。\n\n以上两个实验说明，对于复杂的归约操作，Stream串行归约效果好于手动归约，在多核情况下，并行归约效果更佳。我们有理由相信，对于其他复杂的操作，Stream API也能表现出相似的性能表现。\n5.结论上述三个实验的结果可以总结如下：\n\n对于简单操作，比如最简单的遍历，Stream串行API性能明显差于显示迭代，但并行的Stream API能够发挥多核特性。\n对于复杂操作，Stream串行API性能可以和手动实现的效果匹敌，在并行执行时Stream API效果远超手动实现。\n\n所以，如果出于性能考虑，1. 对于简单操作推荐使用外部迭代手动实现，2. 对于复杂操作，推荐使用Stream API， 3. 在多核情况下，推荐使用并行Stream API来发挥多核优势，4.单核情况下不建议使用并行Stream API。\n如果出于代码简洁性考虑，使用Stream API能够写出更短的代码。即使是从性能方面说，尽可能的使用Stream API也另外一个优势，那就是只要Java Stream类库做了升级优化，代码不用做任何修改就能享受到升级带来的好处。\n七、参考文献\nThe Java® Language Specification\nhttp://viralpatel.net/blogs/lambda-expressions-java-tutorial/\n《Java 8函数式编程 [英]沃伯顿》\nhttp://cr.openjdk.java.net/~briangoetz/lambda/lambda-state-final.html\nhttps://docs.oracle.com/javase/8/docs/api/java/util/stream/package-summary.html#package.description\nhttps://docs.oracle.com/javase/tutorial/java/javaOO/methodreferences.html\nhttps://docs.oracle.com/javase/8/docs/api/java/util/stream/Collector.html\nhttps://docs.oracle.com/javase/8/docs/api/java/util/stream/Stream.html\nhttps://docs.oracle.com/javase/8/docs/api/java/util/stream/Collectors.html\n\n","categories":["总结笔记"],"tags":["java","java8","lambda"]},{"title":"DeepSeek+dify 工作流","url":"/2025_02_10_deepseek_dify/","content":"Deepseek是杭州深度求索公司开发的大语言模型，包含基础模型和代码模型。基础模型有7B和67B参数版本，7B版本在主流英文和中文基准测试中表现出色；67B版本则在多个基准测试中优于GPT-3.5 Turbo。代码模型有1.3B、7B和67B三种参数规模，在多个代码评测基准上有卓越表现，如7B代码模型在HumanEval基准测试中以65%的通过率超越GPT-3.5 Turbo，67B代码模型以82.7%的通过率超越GPT-4。\nDify 是一款开源的大语言模型(LLM) 应用开发平台。它融合了后端即服务（Backend as Service）和 LLMOps 的理念，使开发者可以快速搭建生产级的生成式 AI 应用。即使你是非技术人员，也能参与到 AI 应用的定义和数据运营过程中。\nDify最牛的地方就是像搭积木一样简单——不用懂代码，普通人稍微学学就能用它的「智能机器人」和「流程图」功能，自己拼出一套自动化工具。比如自动回复客户、处理文件这些事，你拖拖拽拽就能搭出想要的功能，特别适合折腾些好玩的小发明。我们可以通过智能体或工作流，自定义工具完成很多我们好玩的功能。报文篇幅较长，简单说下内容：从0开始创建一个票务识别智能体；介绍搭建过程中的各个细节；教会你每一步为什么，而不是只是完成；通过实践这个流程，我相信你可以学会自己搭建自己需要的智能体。\ndeepseek私有化部署\n首先下载并安装Ollama参考官网即可https://ollama.com/\n\n安装deepseek打开Ollama官网首页，在搜索框里输入deepseek-r1，你会发现这个模型排在第一个位置，热度相当高。模型有多个版本，比如1.5b、7b、8b等，数字越大，模型越强大，但对硬件要求也越高。\n\n\n根据你的电脑配置选择合适的模型版本。一般来说，8b是一个比较平衡的选择，既能保证性能，又不会对硬件要求太高。如果你有高性能显卡，当然可以选择更大的模型。笔者显卡是4070super。由于硬盘容量不足，部署的7B版本。\nollama run deepseek-r1:7b\n这个过程类似于Docker拉取镜像，耐心等待即可。安装完成就自动启动了。\ndify安装部署笔者是windows系统，已经安装了docker desktop。\n然后直接访问它的GitHub仓库（https://github.com/langgenius/dify），获取最新版本。解压缩后，进入项目根目录，找到docker文件夹。\n拉取镜像并启动\ndocker compose up -d\n安装完之后，查看：\n初始化：在浏览器地址栏输入以下地址：\nhttp://127.0.0.1/install\n按照提示完成安装并登录账号，进入Dify的主页。\n\n关联本地大模型与Dify\n\n界面界面如下：\n\n新建智能体（工作流）\n\n智能体配置\n\n开始,可以配置入参，文件，图片等 \n节点类型  \n比如python代码节点\n配置python代码\n\n\n提供对外接口\n\napi调用\n\n\n可以实现什么功能\n自定义工作流，解决重复流程问题\n二次开发更复杂能力的节点，实现个性化方案\n对外提供服务，监控服务\n创建智能体应用\n\n最重要的点\n要会用提示词\n\n附录：根据提供的智能体学习智能体首页提供了很多智能体，可以添加到工作空间，学习每个节点组件怎么使用\n附录：dify结构梳理\napi服务\n\nworker服务\n\nweb服务\n\ndb-postgresql服务\n\nredis服务\n\nsandbox服务\n\nplugin_daemon服务\n\nssrf_proxy服务\n\ncertbot服务\n\n网络\n\n\n","categories":["应用笔记"],"tags":["deepseek","ai","dify"]},{"title":"Java 8 新特性二","url":"/2024_01_02_java/","content":"学习必须往深处挖，挖的越深，基础越扎实！\nJava 现在发布的版本很快，每年两个，但是真正会被大规模使用的是 3 年一个的 LTS 版本。\n每 3 年发布一个 LTS（Long-Term Support），长期维护版本。意味着只有Java 8 ，Java 11， Java 17，Java 21 才可能被大规模使用。\n每年发布两个正式版本，分别是 3 月份和 9 月份。\n在 Java 版本中，一个特性的发布都会经历孵化阶段、预览阶段和正式版本。其中孵化和预览可能会跨越多个 Java 版本。所以在介绍 Java 新特性时采用如下这种策略：\n\n每个版本的新特性，都会做一个简单的概述。\n单独出文介绍跟编码相关的新特性，一些如 JVM、性能优化的新特性不单独出文介绍。\n孵化阶段的新特性不出文介绍。\n首次引入为预览特性、新特性增强、首次引入的正式特性，单独出文做详细介绍。\n影响比较大的新特性如果在现阶段没有转正的新特性不单独出文介绍，单独出文的重大特性一般都在 Java 21 版本之前已转为正式特性，例如：\n虚拟线程，Java 19 引入的，在 Java 21 转正，所以在 Java 19 单独出文做详细介绍\n作用域值，Java 20 引入的，但是在 Java 21 还处于预览阶段，所以不做介绍，等将来转正后会详细介绍\n\n\n\nJEP 120：重复注解@Repeatable官方文档:https://openjdk.org/jeps/120\nJava 8 之前如何使用重复注在 Java 8 之前我们是无法在一个类型重复使用多次同一个注解，比如我们常用的 @PropertySource，如果我们在 Java 8 版本以下这样使用：\n@PropertySource(&quot;classpath:config.properties&quot;)@PropertySource(&quot;classpath:application.properties&quot;)public class PropertyTest &#123;&#125;\n\n编译会报错，错误信息是：Duplicate annotation。\n那怎么解决这个问题呢？在 Java 8 之前想到一个方案来解决 Duplicate annotation 错误：新增一个注解 @PropertySources，该注解包裹 @PropertySource，如下：\npublic @interface PropertySources &#123;   PropertySource[] value();&#125;\n\n然后就可以利用 @PropertySources 来完成了：\n@PropertySources(&#123;  @PropertySource(&quot;classpath:config.properties&quot;),  @PropertySource(&quot;classpath:application.properties&quot;)     &#125;)public class PropertyTest &#123;&#125;\n\n利用这种嵌套的方式来规避重复注解的问题，怎么获取呢？\n    @Test    public void  test() &#123;        PropertySources propertySources = PropertyTest.class.getAnnotation(PropertySources.class);        for (PropertySource propertySource : propertySources.value()) &#123;            System.out.println(propertySource.value()[0]);        &#125;    &#125;// 结果......classpath:config.propertiesclasspath:application.properties\n\nJava 8 重复注解 @Repeatable通过上述那种方式确实是可以解决重复注解的问题，但是使用有点儿啰嗦，所以 Java 8 为了解决这个问题引入了注解 @Repeatable 来解决这个问题。\n@Repeatable 注解允许在同一个类型上多次使用相同的注解，它提供了更灵活的注解使用方式。\n下面我们来看看如何使用重复注解。\n\n重复注解声明在使用重复注解之前，需要在自定义注解类型上使用@Repeatable注解，以指定该注解可重复使用的容器注解类型。容器注解类型本身也是一个注解，通常具有一个value属性，其值是一个数组，用于存储重复使用的注解。\n\n@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.TYPE)@Repeatable(MyAnnotations.class)        // 声明重复注解public @interface MyAnnotation &#123;    String name() default &quot;&quot;;&#125; /** * 重复注解容器 */@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.TYPE)public @interface MyAnnotations &#123;    MyAnnotation[] value();&#125; \n\n\n使用重复注解定义了重复注解，我们就可以在一个类型上面使用多个相同的注解，如下：\n\n@MyAnnotation(name = &quot;死磕 Java 并发&quot;)@MyAnnotation(name = &quot;死磕 Netty&quot;)@MyAnnotation(name = &quot;死磕 Redis&quot;)@MyAnnotation(name = &quot;死磕 Java 基础&quot;)@MyAnnotation(name = &quot;死磕 Redis&quot;)public class MyAnnotationTest &#123;&#125;\n\n\n获取重复注解的值使用放射获取元素上面的重复注解，由于我们这里有多个所以需要根据 getAnnotationsByType() 来获取所有重复注解的数组：\n\n@Testpublic void test() &#123;    MyAnnotation[] myAnnotations = MyAnnotationTest.class.getAnnotationsByType(MyAnnotation.class);    for (MyAnnotation myAnnotation : myAnnotations) &#123;        System.out.println(myAnnotation.name());    &#125;&#125;\n\n我们还可以直接获取它的容器注解：\n@Testpublic void test() &#123;    MyAnnotations myAnnotations = MyAnnotationTest.class.getAnnotation(MyAnnotations.class);    for (MyAnnotation myAnnotation : myAnnotations.value()) &#123;        System.out.println(myAnnotation.name());    &#125;&#125;\n\n重复注解很容易就理解了，知道如何自定义注解，然后变换下思路就行了。\nJEP 122：移除Permgen\n官方文档https://openjdk.org/jeps/122\n\n摘要从 Hotspot JVM 中移除永久代，从而不再需要调整永久代的大小。\n\n非目标将类数据共享扩展到应用程序类。减少类元数据所需的内存。启用类元数据的异步收集。\n\n成功指标类元数据、已驻留字符串和类静态变量将从永久代移动到 Java 堆或本地内存中。\n\n\n将从 Hotspot JVM 中移除永久代的代码。\n根据尚未选定的基准测试集测量，应用程序的启动时间和占用空间不会增加超过 1%。\n\n动机这是 JRockit 和 Hotspot 融合工作的一部分。JRockit 客户无需配置永久代（因为 JRockit 没有永久代），并且习惯于不配置永久代。\n\n描述将 Hotspot 中永久代的部分内容移动到 Java 堆，其余部分移动到本地内存。\n\n\nHotspot 对 Java 类（此处称为类元数据）的表示当前存储在 Java 堆的一部分中，该部分称为永久代。此外，已驻留字符串和类静态变量也存储在永久代中。永久代由 Hotspot 管理，并且必须为 Java 应用程序使用的所有类元数据、已驻留字符串和类静态变量提供足够的空间。当加载类时，类元数据和静态变量在永久代中分配，并在卸载类时从永久代中垃圾回收。当永久代进行垃圾回收时，已驻留字符串也会被垃圾回收。\n提议的实现将在本地内存中分配类元数据，并将已驻留字符串和类静态变量移动到 Java 堆。Hotspot 将显式地为类元数据分配和释放本地内存。新类元数据的分配将受可用本地内存量的限制，而不是由-XX:MaxPermSize 的值（无论是默认值还是命令行上指定的值）固定。\n类元数据的本地内存分配将以足够大的块进行，以便能够容纳多个类元数据片段。每个块将与一个类加载器相关联，并且由该类加载器加载的所有类元数据都将由 Hotspot 从该类加载器的块中分配。如有需要，将为类加载器分配额外的块。块的大小将根据应用程序的行为而变化。将选择这些大小以限制内部和外部碎片。当类加载器死亡时，将释放与该类加载器相关联的所有块，从而释放类元数据的空间。在类的生命周期内，类元数据不会被移动。\nJEP 126：Lambda 表达式&amp;函数式接口\n官方文档https://openjdk.org/jeps/126\n\n摘要添加lambda表达式 (闭包) 和支持功能，包括 方法引用、增强的类型推断和虚拟扩展 方法，以Java编程语言和平台。\n\n目标lambda表达式和虚拟扩展的主要功能 方法，以及它们的一组辅助支持功能， 更多平台目标:\n\n\n简化了更抽象的创造和消费， 更高性能的库通过迁移兼容性支持更平滑的库演进除了为Java编程语言添加一个现在常见的功能之外， lambda表达式为改进多核支持提供了可能性 通过启用内部迭代成语。\n围绕lambda的支持语言功能包括虚拟扩展 方法,这将允许接口在源和二进制文件中进化 兼容时尚。\n除了语言更改，协调库和JVM更改 也会发生。\n请注意，活动的和正在进行的 项目LambdaOpenJDK项目 早于JEP进程，相应的JSR也是如此， JSR 335,这是有针对性的 对于Java SE 8 (JSR 336)。\n\n非目标函数类型和一般控制的语言特征 抽象是不将lambda表达式添加到Java的目标。 然而，目的是不排除增加这样的 未来的特点。\n\n动机许多其他常用的面向对象编程语言， 包括托管在JVM上的那些 (例如Groovy、Ruby和 Scala) 和托管在其他虚拟机 (CLR上的C #) 上，包括 支持闭包。因此，Java程序员越来越 熟悉语言功能和编程模型 启用。\n\n\n特别感兴趣的是启用以下成语内部迭代。 数组和集合当前支持外部迭代,在哪里 迭代的控制逻辑位于数据结构之外 被穿越。例如，afor-数组上的每个循环或 集合是外部迭代的一个例子。的语义 forJava中的循环要求严格的串行迭代，这意味着 程序员对标准进行迭代的唯一手段 集合将不允许使用所有可用的内核。\n通过内部迭代，数据结构被传递一段代码 来执行，作为lambda表达式，数据结构为 负责划分计算并报告 结果。由于数据结构熟悉其自身的内部 详细信息，它可能会选择一个更好的调度 通过调整选项进行计算，例如\n备用执行顺序使用线程池的并发执行使用分区和工作窃取的并行执行。的 fork&#x2F;join框架, 在Java SE 7中添加的是一个这样的候选并行执行框架，并且 在广泛的核心范围内提供性能稳定的工作分区 计数。内部迭代样式的一个典型示例是序列 filter-map-reduce操作，例如:\nint maxFooWeight =    collection.filter( /* isFoo Predicate as a lambda */)              .map(    /* Map a Foo to its weight with a lambda */)              .max();  /* Reduction step */\nlambda表达式是具有简洁语法的表达式，用于 表示所需的操作。此样式代替一个或多个 显式for循环，这将不必要的约束迭代 订购收藏品。此外，设计良好的算法可以 不仅可以并行执行这些操作集，而且还可以 将这三个操作聚合到一个并行过程中。\n项目Lambda还包括虚拟扩展方法，这将 解决长期存在的无法添加方法的限制 到广泛使用的接口，因为源兼容性问题。\n通过向现有的集合接口添加扩展方法， 如java.util.Collection和java.util.List,现有 这些类型的实现可以参与新的编程 成语。JDK (和其他地方) 中这些类型的实现可以 重写扩展方法的默认实现 超级接口，以提供更高性能或以其他方式专用 实现。\n\n使用Lambda表达式和匿名内部类\n\nJEP 126：接口的默认方法Java 8 用默认方法与静态方法这两个新概念来扩展接口的声明。默认方法使接口有点像 Traits（Scala 中特征（trait）类似于 Java 中的 Interface，但它可以包含实现代码，也就是目前 Java 8 新增的功能），但与传统的接口又有些不一样，它允许在已有的接口中添加新方法，而同时又保持了与旧版本代码的兼容性。\n默认方法与抽象方法不同之处在于抽象方法必须要求实现，但是默认方法则没有这个要求。相反，每个接口都必须提供一个所谓的默认实现，这样所有的接口实现者将会默认继承它（如果有必要的话，可以覆盖这个默认实现）。让我们看看下面的例子：\npackage h.xd.java8;public class Main &#123;    public static void main(String[] args) &#123;        System.out.println(new DefaultableImpl().notRequired());        System.out.println(new OverridableImpl().notRequired());    &#125;&#125;interface Defaulable &#123;    // Interfaces now allow default methods, the implementer may or    // may not implement (override) them.    default String notRequired() &#123;        return &quot;Default implementation&quot;;    &#125;&#125;class DefaultableImpl implements Defaulable &#123;&#125;class OverridableImpl implements Defaulable &#123;    @Override    public String notRequired() &#123;        return &quot;Overridden implementation&quot;;    &#125;&#125;\n\nDefaulable 接口用关键字 default 声明了一个默认方法 notRequired()，Defaulable 接口的实现者之一 DefaultableImpl 实现了这个接口，并且让默认方法保持原样。Defaulable 接口的另一个实现者 OverridableImpl 用自己的方法覆盖了默认方法。\nJava 8 带来的另一个有趣的特性是接口可以声明（并且可以提供实现）静态方法。例如：\ninterface DefaulableFactory &#123;    // Interfaces now allow static methods    static Defaulable create( Supplier&lt; Defaulable &gt; supplier ) &#123;        return supplier.get();    &#125;&#125;\n\n下面的一小段代码片段把上面的默认方法与静态方法黏合到一起。\npackage h.xd.java8;import java.util.function.Supplier;public class Main &#123;    public static void main( String[] args ) &#123;        Defaulable defaulable = DefaulableFactory.create( DefaultableImpl::new );        System.out.println( defaulable.notRequired() );        defaulable = DefaulableFactory.create( OverridableImpl::new );        System.out.println( defaulable.notRequired() );    &#125;&#125;interface Defaulable &#123;    // Interfaces now allow default methods, the implementer may or    // may not implement (override) them.    default String notRequired() &#123;        return &quot;Default implementation&quot;;    &#125;&#125;class DefaultableImpl implements Defaulable &#123;&#125;class OverridableImpl implements Defaulable &#123;    @Override    public String notRequired() &#123;        return &quot;Overridden implementation&quot;;    &#125;&#125;interface DefaulableFactory &#123;    // Interfaces now allow static methods    static Defaulable create( Supplier&lt; Defaulable &gt; supplier ) &#123;        return supplier.get();    &#125;&#125;\n\n这个程序的控制台输出如下：\nConnected to the target VM, address: &#x27;127.0.0.1:57010&#x27;, transport: &#x27;socket&#x27;Default implementationOverridden implementation\n\n在JVM中，默认方法的实现是非常高效的，并且通过字节码指令为方法调用提供了支持。默认方法允许继续使用现有的Java接口，而同时能够保障正常的编译过程。这方面好的例子是大量的方法被添加到 java.util.Collection 接口中去：stream()，parallelStream()，forEach()，removeIf()，……\n尽管默认方法非常强大，但是在使用默认方法时我们需要小心注意一个地方：在声明一个默认方法前，请仔细思考是不是真的有必要使用默认方法，因为默认方法会带给程序歧义，并且在复杂的继承体系中容易产生编译错误。更多详情请参考官方文档。\nJEP 150：新的日期时间 API &amp;&amp; 日期时间格式化JDK8之前，Java的日期和时间API存在以下问题：\n设计很差：在java.util和java.sql中都有日期类，java.util.Date同时包含日期和时间，而java.sql.Date仅包含日期。此外，用于格式化和解析的类定义在java.text包中。非线程安全：java.util.Date是非线程安全的，所有的日期类都是可变的，这是Java日期类的最大问题之一。时区处理麻烦：日期类并不提供国际化，没有时区支持，虽然后来引入了java.util.Calendar和java.util.TimeZone类，但他们也存在上述所有问题。\n为了解决这些问题，JDK8借鉴了Joda Time的设计，引入了一套全新的日期和时间API，这些API设计合理，线程安全，都位于java.time包中，下面是一些关键类。\nLocalDate：表示日期，包含年月日，格式为 2024-01-02LocalTime：表示时间，包含时分秒，格式为 23:01:28.123456789LocalDateTime：表示日期时间，包含年月日 时分秒，格式为 2024-01-02T23:01:28.123456789DateTimeFormatter：日期时间的格式化类Instant：时间戳，表示时间线上的一个特定瞬间Period：用于计算2个日期（LocalDate）的距离Duration：用于计算2个时间（LocalTime）的距离ZonedDateTime：包含时区的时间\n\nJava使用的日历系统是ISO 8601，它是世界民用历法，也就是通常讲的公历。平年有365天，闰年有366天。\nLocalDate、LocalTime、LocalDateTime类的实例是不可变对象，分别表示ISO 8601日历系统的日期、时间、日期和时间。他们提供了简单的日期或时间，并不包含当前的时间信息，也不包含与时区相关的信息。\nLocalDate日期，格式为：2024-01-02。\n// 获取当前日期LocalDate nowDay = LocalDate.now();// 创建指定日期LocalDate theDay = LocalDate.of(2024, 1, 2);// 获取LocalDateTime对象的日期LocalDate dayFromDateTime = LocalDate.from(LocalDateTime.now());// 解析字符串获取日期LocalDate parsedDay = LocalDate.parse(&quot;2024-01-02&quot;);// 获取LocalDate对象调整后的日期LocalDate nextMonday = theDay.with(TemporalAdjusters.next(DayOfWeek.MONDAY));// 获取日期信息System.out.println(&quot;年：&quot; + nowDay.getYear());System.out.println(&quot;月：&quot; + nowDay.getMonthValue());System.out.println(&quot;日：&quot; + nowDay.getDayOfMonth());System.out.println(&quot;星期：&quot; + nowDay.getDayOfWeek().getValue());// 是否为闰年nowDay.isLeapYear();// 修改日期nowDay.plusYears(1);nowDay.plusMonths(1);nowDay.plusWeeks(1);nowDay.plusDays(2);nowDay.minusYears(1);nowDay.minusMonths(1);nowDay.minusWeeks(1);nowDay.minusDays(2);// 日期比较nowDay.isBefore(tomorrow); // truenowDay.isEqual(tomorrow);  // falsenowDay.isAfter(tomorrow);  // false\n\n\nLocalTime时间，格式为：23:01:28.123456789。\n// 获取当前时间LocalTime now = LocalTime.now();// 创建指定时间LocalTime theTime = LocalTime.of(18, 20, 16);// 获取LocalDateTime对象的时间LocalTime TimeFromDateTime = LocalTime.from(LocalDateTime.now());// 解析字符串获取时间LocalTime parsedTime = LocalTime.parse(&quot;22:25:05&quot;);// 获取LocalTime对象调整后的时间theTime.withHour(20);theTime.withMinute(30);theTime.withMinute(15);// 获取时间信息System.out.println(&quot;时：&quot; + now.getHour());System.out.println(&quot;分：&quot; + now.getMinute());System.out.println(&quot;秒：&quot; + now.getSecond());System.out.println(&quot;纳秒：&quot; + now.getNano());// 修改时间now.plusHours(1);now.plusMinutes(1);now.plusSeconds(10);now.plusNanos(1000);now.minusHours(1);now.minusMinutes(1);now.minusSeconds(10);now.minusNanos(1000);// 时间比较now.isBefore(nextHour); // truenow.isAfter(nextHour);  // falsenow.equals(nextHour);   // false\n\nLocalDateTime日期和时间，格式为：2024-01-02T22:24:05.123456789。\n// 获取当前日期时间LocalDateTime now = LocalDateTime.now();// 创建指定日期时间LocalDateTime theDateTime = LocalDateTime.of(2024, 1, 2, 22, 5, 18);// 根据另一个LocalDateTime对象创建日期时间LocalDateTime dateTimeFromAnother = LocalDateTime.from(LocalDateTime.now());// 解析字符串获取日期时间LocalDateTime parsedDateTime = LocalDateTime.parse(&quot;2024-01-02T22:29:23.787&quot;);LocalDateTime parsedDateTime = LocalDateTime.parse(&quot;2024-01-02 22:29:23&quot;,     DateTimeFormatter.ofPattern(&quot;yyyy-MM-dd HH:mm:ss&quot;));// 获取LocalDateTime对象调整后的日期时间LocalDateTime nextMonth = now.with(TemporalAdjusters.firstDayOfNextMonth());// 获取时间信息System.out.println(&quot;年：&quot; + now.getYear());System.out.println(&quot;月：&quot; + now.getMonthValue());System.out.println(&quot;日：&quot; + now.getDayOfMonth());System.out.println(&quot;星期：&quot; + now.getDayOfWeek().getValue());System.out.println(&quot;时：&quot; + now.getHour());System.out.println(&quot;分：&quot; + now.getMinute());System.out.println(&quot;秒：&quot; + now.getSecond());System.out.println(&quot;纳秒：&quot; + now.getNano());// 格式化LocalDateTimenow.format(DateTimeFormatter.ofPattern(&quot;yyyy-MM-dd HH:mm:ss&quot;));// 桉指定时间单位保留精度now = now.truncatedTo(ChronoUnit.HOURS);// 修改LocalDateTimenow.plusYears(1);now.plusMonths(1);now.plusWeeks(1);now.plusDays(2);now.plusHours(1);now.plusMinutes(1);now.plusSeconds(10);now.plusNanos(1000);now.minusYears(1);now.minusMonths(1);now.minusWeeks(1);now.minusDays(2);now.minusHours(1);now.minusMinutes(1);now.minusSeconds(10);now.minusNanos(1000);// LocalDateTime比较now.isBefore(nextHour); // truenow.isEqual(nextHour);  // falsenow.isAfter(nextHour);  // false// LocalDateTime转为ZonedDateTimeZonedDateTime zonedDateTime = now.atZone(ZoneId.of(&quot;+08&quot;));// LocalDateTime转为OffsetDateTimeOffsetDateTime offsetDateTime = now.atOffset(ZoneOffset.ofHours(8));\n\n时间格式化与解析java.time.format.DateTimeFormatter类专门负责日期时间的格式化与解析，这个类是final的，线程安全。\nDateTimeFormatter fmt = DateTimeFormatter.ofPattern(&quot;yyyy-MM-dd HH:mm:ss&quot;);// 格式化LocalDateTime为字符串System.out.println(LocalDateTime.now().format(fmt));// 解析时间字符串LocalDateTime parsedDateTime = LocalDateTime.parse(&quot;2024-01-02 10:20:30&quot;, fmt);\n\n时间戳Instant类，时间戳，表示时间线上的一个特定瞬间。内部保存了从1970年1月1日 00:00:00以来的秒和纳秒。\n一般不是给用户使用的，而是方便程序做一些统计。\nInstant instant = Instant.now();// 当前时间戳：2024-01-02T15:51:12.068ZSystem.out.println(&quot;当前时间戳：&quot; + instant);// 1970-01-01 00:00:00 到现在的秒：1716565872System.out.println(&quot;1970-01-01 00:00:00 到现在的秒：&quot; + instant.getEpochSecond());// 1970-01-01 00:00:00 到现在的毫秒：1716565872068System.out.println(&quot;1970-01-01 00:00:00 到现在的毫秒：&quot; + instant.toEpochMilli());// 秒后面的纳秒：68000000System.out.println(&quot;秒后面的纳秒：&quot; + instant.getNano());Instant second = Instant.ofEpochSecond(1716565310);System.out.println(second); // 2024-01-02T15:41:50Z\n\n计算日期、时间差Duration：用于计算2个时间的距离；\nPeriod：用于计算2个日期的距离。\nLocalDateTime now = LocalDateTime.now();LocalDateTime nextMonday = now.with(TemporalAdjusters.next(DayOfWeek.MONDAY));// Duration 计算时间的距离Duration duration = Duration.between(now, nextMonday);System.out.println(&quot;相差的天：&quot; + duration.toDays());System.out.println(&quot;相差的小时：&quot; + duration.toHours());System.out.println(&quot;相差的分钟：&quot; + duration.toMinutes());System.out.println(&quot;相差的秒：&quot; + duration.getSeconds());System.out.println(&quot;相差的毫秒：&quot; + duration.toMillis());LocalDate today = LocalDate.now();LocalDate nextTuesday = today.with(TemporalAdjusters.next(DayOfWeek.TUESDAY));// Period 计算日期的距离Period period = Period.between(today, nextTuesday);System.out.println(&quot;相差的年：&quot; + period.getYears());System.out.println(&quot;相差的月：&quot; + period.getMonths());System.out.println(&quot;相差的天：&quot; + period.getDays());\n\n时间调整器有时候我们可能需要根据一个LocalDateTime获取它调整后的时间，比如“下个月的第2天”，“下个周三”，等等。类似的时间可以通过时间调整器获取。\nTemporalAdjuster：时间调整器接口。\nTemporalAdjusters：该类通过静态方法提供了很多常用的TemporalAdjuster的实现。\n// 将 LocalDateTime 调整到下个月第2天TemporalAdjuster secondDayOfNextMonth = temporal -&gt; &#123;\tLocalDateTime dateTime = (LocalDateTime) temporal;\treturn dateTime.plusMonths(1).withDayOfMonth(2);&#125;;LocalDateTime now = LocalDateTime.now();System.out.println(now); // 2024-01-02T12:17:01.524System.out.println(now.with(secondDayOfNextMonth)); // 2024-02-02T12:17:01.524\n\nJDK自带的时间调整器实现：\n// 当月第1天TemporalAdjusters.firstDayOfMonth()// 当月最后1天TemporalAdjusters.lastDayOfMonth()// 下月第1天TemporalAdjusters.firstDayOfNextMonth()// 当年第1天TemporalAdjusters.firstDayOfYear()// 当年最后1天TemporalAdjusters.lastDayOfYear()// 下年第1天TemporalAdjusters.firstDayOfNextYear()// 当月第1个周几TemporalAdjusters.firstInMonth(DayOfWeek dayOfWeek)// 当月最后1个周几TemporalAdjusters.lastInMonth(DayOfWeek dayOfWeek)// 当前时间之后的下1个周几TemporalAdjusters.next(DayOfWeek dayOfWeek)// 当前时间或者之后的下1个周几TemporalAdjusters.nextOrSame(DayOfWeek dayOfWeek)// 当前时间之前的上1个周几TemporalAdjusters.previous(DayOfWeek dayOfWeek)// 当前时间或者之前的上1个周几TemporalAdjusters.previousOrSame(DayOfWeek dayOfWeek)\n\n时区JDK8加入了对时区的支持，LocalDate、LocalTime、LocalDateTime是不带时区的，带时区的日期时间类分别为：ZonedDateTime。\nZoneId类包含了时区信息，每个时区对应一个ID，格式为“区域&#x2F;城市”，比如：Asia&#x2F;Shanghai。\n东八区代表的ZoneId可通过以下多种方式创建：\nZoneId.of(&quot;Asia/Shanghai&quot;);ZoneId.of(&quot;+8&quot;);ZoneId.of(&quot;+08&quot;);ZoneId.of(&quot;+08:00&quot;);ZoneId.of(&quot;UTC+8&quot;);ZoneId.of(&quot;UTC+08&quot;);ZoneId.of(&quot;UTC+08:00&quot;);\n\nZonedDateTime的使用：\n// 打印所有可用的时区ZoneId.getAvailableZoneIds().forEach(System.out::println);LocalDateTime now = LocalDateTime.now(); // 不带时区的当前时间：2024-01-02T12:42:44.550System.out.println(now);ZonedDateTime zonedNow = ZonedDateTime.now();// 当前时区的当前时间：2024-01-02T12:42:44.550+08:00[Asia/Shanghai]System.out.println(zonedNow);// UTC的当前时间：2024-01-02T04:44:39.496ZZonedDateTime utcNow = ZonedDateTime.now(Clock.systemUTC());System.out.println(utcNow);// UTC时间转为东八区时间：2024-01-02T12:44:39.496+08:00ZonedDateTime utc8Now = utcNow.withZoneSameInstant(ZoneId.of(&quot;+08&quot;));System.out.println(utc8Now);\n\nJEP 174：Nashorn JavaScript 引擎官方文档：https://openjdk.org/jeps/174\njava重执行js脚本的引擎\n摘要设计和实现一个新的轻量级，高性能的实现 的JavaScript，并将其集成到JDK中。新引擎将 通过现有的可用于Java应用程序javax.scriptAPI, 也更一般地通过一个新的命令行工具。\n目标Nashorn将基于ECMAScript-262版5.1语言 规范，并且必须通过ECMAScript-262符合性测试。\nNashorn将支持javax.script(JSR 223) API。\n将提供从JavaScript调用Java代码的支持 为Java调用JavaScript代码。这包括直接映射到 JavaBeans。\nNashorn将定义一个新的命令行工具，jjs,用于评估 “shebang” 脚本中的JavaScript代码，此处文档和编辑 字符串。\nNashorn应用程序的性能和内存使用情况应该是 比犀牛好得多。\nNashorn不会暴露任何额外的安全风险。\n提供的库应在本地化下正确运行。\n错误消息和文档将被国际化。\nJEP 179：方法引用和构造器引用参考lambda表达式介绍\n利用 Optional 解决NullPointerException全新的、标准的 Base64 API","categories":["总结笔记"],"tags":["java8","java8-@Repeatable"]},{"title":"Clickhouse总结","url":"/2024_01_10_clickhouse/","content":"OLTP与OLAP什么是 OLTP\n全称: Online Transaction Processing（联机事务处理系统）\n特点:\n专注于事务处理\n进行数据的增删改查操作\n典型代表: MySQL、Oracle\n用于网站和系统应用的后端数据库\n存储业务数据（如下单、支付、注册信息）\n支持事务操作，响应时间要求高\n数据量相对较少\n\n\n\nOLTP 的问题\n当数据量达到 TB 或 PB 级别时，传统的 MySQL 性能不足\n数据分析场景需要全盘扫描统计，不适合 OLTP\n\n什么是 OLAP\n全称: Online Analytical Processing（联机分析处理）\n特点:\n专注于复杂分析操作\n更侧重于决策支持\n典型代表: ClickHouse、Doris、StarRocks\n数据量通常为 TB 级别\n不擅长修改操作，数据一致性要求低\n\n\n\nClickHouse 数据库介绍\n简介:\n是俄罗斯的Yandex于2016年开源的列式存储数据库\n使用c++语言编写\n\n\n类型: OLAP 数据库\n特点:\n实时数据仓库，在线分析处理查询\n支持标准 SQL 语句（如 INSERT、SELECT），实时生成分析数据报告\n提供高级查询功能（复合聚合函数、窗口函数、跨表查询）\n支持列式存储、数据分区和线程并行\n\n\n\n列式存储 vs 行式存储\n行式存储:\n将整行数据存储为一个整体\n\n\n列式存储:\n将每一列作为一个数据块存储\n优势: 更高效的统计查询、数据压缩效果好\n\n\n\n数据分区和线程并行\n数据分区:\n按业务逻辑将数据分类，便于查询管理\n示例: 按日期分区\n\n\n线程并行:\n多个 CPU 核心并行查询不同分区\n优势: 降低查询延迟，利用 CPU 资源\n\n\n\n支持丰富的表引擎\n表引擎:\n决定数据存储方式和位置\n支持查询类型和并发访问\n\n\n常用表引擎:\nMergeTree（支持分区和 TTL）\n日志引擎（快速写入小数据）\n集成引擎（实时接入其他数据源）\n\n\n\nClickHouse 的缺点\n由于单条查询使用多核cpu，不支持高并发请求\n对 UPDATE 和 DELETE 操作支持较差\n单个插入性能较低，建议批量插入\n吃硬件，一般物理机安装，单独安装不和业务混部\n\nClickHouse 的适用场景\n适用于数据量大且需要分析的场景\n适合存储已经处理过的大宽表，进行分析，读取大量列中的少量列\n不适合高并发请求、频繁更新和删除的场景\n\nClickHouse安装安装准备\n防火墙设置：关闭防火墙以避免影响安装\n实际工作中需根据需要配置IP和端口限制\nLinux系统文件限制，取消文件限制：否则影响ch性能\n\n\n使用ulimit -a命令查看系统限制\n关注的主要参数：\n打开的文件数\n用户最大进程数\n\n\n修改配置文件&#x2F;etc&#x2F;security&#x2F;limits.conf没修改前：修改:修改后：\n解释配置项：\n用户与用户组的设置\n软限制（soft）与硬限制（hard）的区别\n打开文件数（number of open files）与进程数（number of processes）\n\n\n\n\n安装依赖  linux安装由运维协助，这里不展开记录，下面直接通过docker安装（生产环境不要使用docker，这里只为了学习原理和使用，并非安装所以简化安装）  参考  https://clickhouse.com/docs/install#from-docker-image\n\n安装 linux安装由运维协助，这里不展开记录，下面直接通过docker安装（生产环境不要使用docker，这里只为了学习原理和使用，并非安装所以简化安装）\ndocker pull yandex/clickhouse-server:21.7.3.14docker run --rm -d --name=clickhouse-server \\--ulimit nofile=262144:262144 -p 8123:8123  \\-p 9009:9009 -p 9090:9000 yandex/clickhouse-server:21.7.3.14docker cp clickhouse-server:/etc/clickhouse-server/config.xml ./conf/config.xmldocker cp clickhouse-server:/etc/clickhouse-server/users.xml ./conf/users.xmldocker run -d --name=clickhouse-server \\-p 8123:8123 -p 9009:9009 -p 9090:9000 \\--ulimit nofile=262144:262144 \\-v ./data/:/var/lib/clickhouse/data \\-v ./conf/:/etc/clickhouse-server \\-v ./log/:/var/log \\yandex/clickhouse-server:21.7.3.14设置密码后客户端链接clickhouse-client --user default --password 1234# refer to https://blog.csdn.net/huas_xq/article/details/123574461\n核心目录：\n\n数据在 data目录\n表结构信息在 metadata目录\n\nClickhouse数据类型整形数值\nInt8, Int16, Int32, Int64:\nInt8: 8位（-128 到 127）\nInt16: 16位（-32768 到 32767）\nInt32: 32位（-2147483648 到 2147483647）\nInt64: 64位（-9223372036854775808 到 9223372036854775807）\n\n\n与Java类型类比\nint8对应byte\nint16对应short\nint32对应int\nint64对应long\n\n\n无符号整型:\nUInt8: 0 到 255\nUInt16: 0 到 65535\nUInt32: 0 到 4294967295\nUInt64: 0 到 18446744073709551615\n\n\n\n浮点型\n浮点型的分类\n\nFloat32: 32位（4字节）\nFloat64: 64位（8字节）\n\n\n精度问题\n\n浮点型存储可能会导致精度丢失，尤其在处理货币时应避免使用。\n\n\n布尔类型\n\n使用uint8表示布尔值\n0表示false\n1表示true\n\n\nDecimal类型\n\nDecimal(p, s):\np: 总位数\ns: 小数位数\n\n\n类型示例\ndecimal(32, 5)  整数小数一共9位， 小数后5位 \ndecimal(64, 5), 整数小数一共18位， 小数后5位 \ndecimal(128, 5) 整数小数一共38位， 小数后5位\n\n\n\n\n字符串类型\n\nString: 可变长度字符串。\nFixedString(n): 固定长度字符串，不足部分用空字节填充。\n\n\n枚举类型\n\n定义和使用枚举， Enum8,Enum16\n插入和查询枚举值的示例 CREATE TABLE t_enum (    x Enum8(&#x27;hello&#x27; = 1, &#x27;world&#x27; = 2)) ENGINE = TinyLog;INSERT INTO t_enum VALUES (&#x27;hello&#x27;),(&#x27;world&#x27;),(&#x27;hello&#x27;);SELECT * FROM t_enum;# 转换查询SELECT CAST(x, &#x27;Int8&#x27;) FROM t_enum;\n插入未知类型会报错\n\n\n时间类型\n\nDate: 仅包含年月日。\nDateTime: 包含年月日时分秒。\nDateTime64: 包含亚秒（毫秒）。\n\n\n数组类型\n\nmergeTree表中不能存储多维数组\n使用 Array 表示，支持单层数组。\n例子：Array(UInt8) 表示存储无符号8位整型的数组。SELECT array(1,2) AS x, toTypeName(x);2d9991e86323 :) SELECT array(1,2) AS x, toTypeName(x);SELECT    [1, 2] AS x,    toTypeName(x)┌─x─────┬─toTypeName([1, 2])─┐│ [1,2] │ Array(UInt8)       │└───────┴────────────────────┘1 rows in set. Elapsed: 0.013 sec.\n\n\n其他数据类型介绍其他常用数据类型可查阅ClickHouse官网获取更多信息https://clickhouse.com/docs/zh/sql-reference/data-types\n\n空值处理ClickHouse中的空值存储方式建议使用无意义的数字或字符表示空值, 避免使用NULL比如可以用-1，无意义得值，字符串可以搞空串或者null等\n\n\nClickhouse表引擎概述\n表引擎的定义：类似于MySQL中的InnoDB和MyISAM，不同引擎具有不同的功能和作用。\n表引擎的作用：\n决定数据的存储方式和位置。\n指定如何读写数据。\n\n\n\nClickHouse表引擎的特点\n存储方式和位置\n数据一般存储在本地磁盘上。\nClickHouse不依赖于Hadoop或HDFS计算资源。\n默认配置路径：&#x2F;var&#x2F;lib&#x2F;clickhouse&#x2F;data。\n\n\n支持的查询及语法\n不同引擎支持不同的查询语法。\n例如，某些引擎不支持多维数组存储。\n\n\n并发数据访问\nClickHouse支持多线程并发查询，但并非所有引擎都支持。\n\n\n索引支持\n不同引擎对索引的支持程度不同，索引的目的是提高查询效率。\n\n\n数据复制\n某些引擎支持数据复制，而其他引擎则不支持。\n\n\n引擎名称是大小写敏感的，采用大驼峰命名法。\n\n表引擎TinyLog\n属于日志家族，适合小数据量（小于100万行）。\n特点：\n存储在磁盘上，不支持索引。\n无并发控制，适合简单测试。\n\n\n做一些简单测试，生产环境肯定不会用CREATE table t_tinylog(id String,name String) engine=TinyLog;\n\n表引擎Memory\n基于内存，性能（超过10G&#x2F;s）快但数据易丢失。\n特点：\n不支持索引，适合小数据量测试。\n\n\n做一些简单测试，生产环境肯定不会用\n\nMergeTree系列\nClickHouse的核心引擎，适合生产环境。相当于innodb之于mysql\n包含多个变种，如ReplacingMergeTree和SummingMergeTree，具有不同功能。参考官方文档：https://clickhouse.com/docs/zh/engines/table-engines/mergetree-family\n\n集成引擎集成外部系统，如MySQL和Kafka。外部集成的意义：无需将数据导入ClickHouse，直接查询外部数据源。\nhttps://clickhouse.com/docs/zh/engines/table-engines/integrations\nMergeTree引擎mergeTree家族的第一个引擎——MergeTree。MergeTree引擎本身是一个表引擎。介绍建表语句和嵌表语句。\n参考：https://clickhouse.com/docs/zh/engines/table-engines/mergetree-family/mergetree\n建表语句CREATE TABLE t_order_mt(    id UInt32,    sku_id String,    total_amount Decimal(16,2),    create_time DateTime) engine=MergeTree partition by toYYYYMM(create_time) primary key (id)order by (id,sku_id);\nprimary key是可以重复的，这和mysql不一样order by 是必须的， primary key 不是必须的\n测试数据（多执行几次，多插入一些）：\nINSERT INTO t_order_mt (id, sku_id, total_amount, create_time) VALUES (1, &#x27;SKU001&#x27;, 100.50, &#x27;2023-01-01 10:00:00&#x27;), (1, &#x27;SKU002&#x27;, 200.75, &#x27;2023-01-01 15:30:00&#x27;), (3, &#x27;SKU003&#x27;, 50.20, &#x27;2023-03-20 09:15:00&#x27;), (4, &#x27;SKU004&#x27;, 150.00, &#x27;2023-05-10 14:45:00&#x27;), (5, &#x27;SKU005&#x27;, 300.99, &#x27;2023-05-10 11:20:00&#x27;), (6, &#x27;SKU006&#x27;, 75.55, &#x27;2023-06-30 16:50:00&#x27;);\n查询结果，分区了O(∩_∩)O\n连接配置连接ClickHouse的步骤：使用主机名和端口（默认端口为8123）。默认用户名为default，密码为空。如果驱动未自动下载，需要手动添加驱动文件。\npartition by 分区（可选）不是必须的建表语句\n\n分区的作用主要目的是避免全表扫描。查询语句中加入分区字段的取值，可以优化查询速度。分区的实现方式类似于物理分隔，像是将数据分放在不同的房间。\n\nClickHouse的分区机制ClickHouse的分区是基于本地磁盘，而hive的分区是在HDFS上建立分区目录。如果不写分区，默认会用一个分区，名为“all”，所有数据都在里面。\n\n并行查询单个查询可以多线程同时执行。每个线程处理一个分区的数据。通常按天分区，因为这样可以避免不必要的麻烦。\n\n分区的存储数据存储\n\n稀疏索引索引文件采用的稀疏索引，和kafka的partition一样\n\n分区目录原理\n\n分区目录如上图所示， 第一位分区键，第二位最小区块编号，第三位，最大区块编号，第四位合并层级\n日期类型分区键，最好存日期类型，不像其他数仓，hive存字符串，这样效率高\n其他类型分区目录，如string，float类型分区建，目录名字取hash\n\n\n数据写入与分区合并\n\n每次数据写入会产生一个临时分区，之后需要执行合并操作。\n合并是定期执行的，可以手动触发。\n手动触发指令 OPTIMIZE TABLE t_order_mt FINAL\n可以加一个分区参数 OPTIMIZE TABLE t_order_mt PARTITION &#x27;20230101&#x27; FINAL\n 合并之后，分区目录目录会变，产生合并后的目录，过期数据会自动清理，删除。\n\n\n\nprimary key主键（可选）\n提供了数据的一级索引，但不是唯一约束\n\n主键的设定主要依据查询中的where条件。（一般加载where条件中）\n\nindex granularity直接翻译就是索引粒度，指在稀疏索引中两个相邻索引对应的间隔，Clickhouse中mergeTree默认是 8192，官方不建议修改这个值，除非该列存在大量重复值，比如在一个分区中几万行才有一个不同数据。\n\n稀疏索引索引文件采用的稀疏索引，和kafka的partition一样要求主键必须有序所以必须要有order bt\n\n\n优点：数据量小，查询效率高\norder by(必选)\n分区内排序\n必填 甚至比主键还重要， 因为要建立稀疏索引，且后面去重和汇总也要用到order by\n可以多个字段，但必须是左前坠， 比如(id,sku_id) 要么是id ,要么是id,sku_id，不能是sku_id,id 和sku_id\n\n二级索引\n版本问题20.1.2.4之前，官方标注是实验性的，之后是默认开启的（20年1月份版本）之前版本可以用以下命令设置，之后版本会报错\nset allow_expreimental_data_skipping_indices=1\n二级索引使用在大量重复数据的场景下二级索引不必有序二级索引也叫跳数索引\n\n如何使用\nCREATE TABLE t_order_mt2(    id UInt32,    sku_id String,    total_amount Decimal(16,2),    create_time DateTime,    INDEX a total_amount TYPE minmax GRANULARITY 5) engine=MergeTreepartition by toYYYYMM(create_time)primary key (id)order by (id,sku_id);\n二级索引的类型是 minmax\n\n\nminmax 是最小最大值索引，粒度是5， 也就是5个数据算一个区间， 然后取最小值和最大值。\nINSERT INTO t_order_mt2 (id, sku_id, total_amount, create_time) VALUES (1, &#x27;SKU001&#x27;, 100.50, &#x27;2023-01-01 10:00:00&#x27;), (1, &#x27;SKU002&#x27;, 200.75, &#x27;2023-01-01 15:30:00&#x27;), (3, &#x27;SKU003&#x27;, 50.20, &#x27;2023-03-20 09:15:00&#x27;), (4, &#x27;SKU004&#x27;, 150.00, &#x27;2023-05-10 14:45:00&#x27;), (5, &#x27;SKU005&#x27;, 300.99, &#x27;2023-05-10 11:20:00&#x27;), (6, &#x27;SKU006&#x27;, 75.55, &#x27;2023-06-30 16:50:00&#x27;);\n\nclickhouse-client --send_logs_level=trace &lt;&lt;&lt; &#x27;select * from t_order_mt2 where total_amount &gt; toDecimal32(900,2)&#x27;\n\n使用了二级索引\n数据TTLtime to alive 数据存活时间\n\n对某一列设置ttl， 数据过期之后，会自动删除CREATE TABLE t_order_mt3(    id UInt32,    sku_id String,    total_amount Decimal(16,2) TTL create_time + INTERVAL 10 SECOND,    create_time DateTime,    INDEX a total_amount TYPE minmax GRANULARITY 5) engine=MergeTreepartition by toYYYYMM(create_time)primary key (id)order by (id,sku_id);\n\n时间到期，这一列就会被删除（相当于时间到了，会启动一个合并任务，处理删除）\n\n表级TTLALTER table t_order_mt3 MODIFY TTL create_time + INTERVAL 10 SECOND;\n会根据每行的create_time 字段进行过期删除\n\nTTL不能使用到主键字段\n参考：https://clickhouse.com/docs/zh/engines/table-engines/mergetree-family/mergetree\nReplacingMergeTree 引擎替换合并树他的存储特性完全继承MergeTree，  多了一个去重功能不是根据主键去重，而是根据order by的字段去重\n\n去重时机不是实时去重，只会在同一批插入（新版本）或合并的过程去重。最终一致性，不实时一致\n\n去重范围分区内去重，不能跨分区去重\n\nCREATE TABLE t_order_rmt(    id UInt32,    sku_id String,    total_amount Decimal(16,2),    create_time DateTime) engine=ReplacingMergeTree(create_time)partition by toYYYYMM(create_time)parimary key (id)order by (id,sku_id);\n\nReplacingMergeTree(create_time) 中create_time 表示按照order by的去重之后，保留crate_time最大的数据。\n不指定，默认按照插入顺序来，保留最后插入的。\nSummingMergeTree 引擎预聚合求和合并树分区内聚合按照order by\nCREATE TABLE t_order_smt(    id UInt32,    sku_id String,    total_amount Decimal(16,2),    create_time DateTime) engine=SummingMergeTree(total_amount)partition by toYYYYMM(create_time)primary key (id)order by (id,sku_id);\n以上求和total_amount，是按照gruop by id,sku_id来聚合求和的，  重要的是order_by，如果不指定total_amaount,那么会按照order by求和所有的数字列。\ninsert into t_order_smt values (1,&#x27;sku001&#x27;,100.50,&#x27;2023-01-01 10:00:00&#x27;),(1,&#x27;sku001&#x27;,200.75,&#x27;2023-01-01 15:30:00&#x27;),(3,&#x27;sku003&#x27;,50.20,&#x27;2023-03-20 09:15:00&#x27;),(4,&#x27;sku004&#x27;,150.00,&#x27;2023-05-10 14:45:00&#x27;),(5,&#x27;sku005&#x27;,300.99,&#x27;2023-05-10 11:20:00&#x27;),(6,&#x27;sku006&#x27;,75.55,&#x27;2023-06-30 16:50:00&#x27;);OPTIMIZE TABLE t_order_smt final;\n\n结果，两条被聚合了：\n且crate_time 是最前面的一条（第一行），而不是最后一条（最大）\n只有在同一批次插入时（新版本），或分片合并时才会聚合\n设计聚合表时，最好设置聚合的列，否则序号之类的都会被聚合，没有意义\n查询的时候，还是需要在sql里面写sum(),因为可能还没来得及聚合，体会下预聚合\nClickhouse SQL操作insert和mysql一样从表到表，支持insert into xxx select\nupdate和deleteOLAP的，不叫更新，叫做可变查询， 是ALTER的一种不支持事务\n\n物理删除alter table t_order_mt delete where id = 1;\n逻辑删除alter table t_order_mt update status = 1 where id = 1;\n恢复alter table t_order_mt update status = 0 where id = 1;\n\n这种删除操作比较重，分了两步，新增分区并发旧分区打上逻辑失效标记，直到合并的时候才会释放磁盘空间，一般这种功能不给用户，由管理员完成\n实践中， 可以加_sign标记，_version版本号机制以新增的方式替代删除，缺点就是数据膨胀\n查询操作支持子查询支持CTE Common Table Expression 公用表表达式with子句\n语法支持join，但是实际上避免join，json操作无法使用缓存，所以即使是join ch也会查询两次，且占用内存，且分布式的话内存爆炸\n窗口函数，目前官网表示正在试验阶段\n各种函数查看官网\n多维分析函数不支持自定义函数\nGROUP BY 增加了with rollup上卷 , with cube多维分析,  with totals总计  来计算小计和总计比如按照a,b维度分析，上卷：分析 group by a,b分析 group by a分析 group by ()\n多维分析：分析 group by a,b分析 group by a分析 group by b分析 group by ()\n总计分析：分析 group by a,b分析 group by ()\nalter table t_order_mt delete where 1=1;insert into t_order_mt values (101,&#x27;sku_001&#x27;,1000.00, &#x27;2024-01-01 12:00:00&#x27;),(102,&#x27;sku_002&#x27;,2000.00, &#x27;2024-01-02 12:00:00&#x27;),(103,&#x27;sku_003&#x27;,3000.00, &#x27;2024-01-02 12:00:00&#x27;),(104,&#x27;sku_004&#x27;,4000.00, &#x27;2024-01-03 12:00:00&#x27;),(105,&#x27;sku_005&#x27;,5000.00, &#x27;2024-01-03 12:00:00&#x27;),(106,&#x27;sku_006&#x27;,6000.00, &#x27;2024-01-03 12:00:00&#x27;);select id, sku_id, sum(total_amount) from t_order_mt group by id,sku_id with rollup;\n\nalter操作和mysql基本一致新增字段\nalter table tableName add column newcolname String after col1;\n修改字段类型\nalter table tableName modify column colname String;\n删除字段\nalter table tableName drop column colname;\n\n导出数据clickhouse-client --query &quot;select * from t_order_mt where create_time=&#x27;2024-01-01 12:00:00&#x27;&quot; --output_format_csv_separator=&quot;\\t&quot; --output_format_csv_with_names --output_format_csv_with_names_and_types &gt; /tmp/t_order_mt.csv\n更多格式参考官网\n副本\n互为副本， 平权\n注意点：副本的表引擎都要加Replicated例如：\n\nReplicatedMergeTree\nReplicatedReplacingMergeTree\nReplicatedSummingMergeTree其他可参考官方文档\n\n分片集群clickhouse支持数据分片到不同机器上存储\n\n集群写入流程（3分片2副本共6个节点）\n\ns1 s2 s3 三个分片，r1 r2 每个分片两个副本\n参数  internal_replication   内部副本参数false得话，非内部同步，如图黄色true得话，内部同步（生产一般要打开true），如图绿色\n\n集群读取流程\n\n\n优先选择errors_count小得副本\nerrors_count相同，有随机，顺序，随机（优先第一顺位）host名称近似等四种选择方式\n\n\n集群建表创建本地表：CREATE TABLE st_order_mt on cluster gmall_cluster (    id UInt32,    sku_id String,    total_amount Decimal(16,2),    create_time DateTime)engine=RepilicatedMergeTree(&#x27;/clickhouse/tables/&#123;shard&#125;/st_order_1109&#x27;, &#x27;&#123;replica&#125;&#x27;)patition by toYYYYMMDD(create_time)primary key(id)order by(id,sku_id);\n参数含义：集群名， 集群配置里面得宏定义\n\n本地表得意思是集群每个节点都创建一个表\n创建分布式表：\nCREATE TABLE st_order_mt_all on cluster gmall_cluster (    id UInt32,    sku_id String,    total_amount Decimal(16,2),    create_time DateTime)engine=Distributed(gmall_cluster,default,st_order_mt,hiveHash(sku_id));\n参数含义：集群名称，库名，本地表名，分片键统领本地表，有点视图得意思\n\n插入数据插得是分布式表，实际上存在每一张表INSERT INTO st_order_mt_all values()\n\nClickhouse执行计划官方推荐独立部署，128G硬盘 100G内存，32线程内核\n参考官文档：https://clickhouse.com/docs/zh/sql-reference/statements/explain\n语法\nEXPLAIN [AST | SYNTAX | QUERY TREE | PLAN | PIPELINE | ESTIMATE | TABLE OVERRIDE] [setting = value, ...]    [      SELECT ... |      tableFunction(...) [COLUMNS (...)] [ORDER BY ...] [PARTITION BY ...] [PRIMARY KEY] [SAMPLE BY ...] [TTL ...]    ]    [FORMAT ...]\nEXPLAIN 类型:\n\nAST — 抽象语法树。\nSYNTAX — 经 AST 级别优化后的查询文本。\nQUERY TREE — 经查询树级别优化后的查询树。\nPLAN — 查询执行计划。 （默认） \nPIPELINE — 查询执行流水线。\n\n以上实例说明该查询走了预处理文件，也就是直接读的文件资源，速度比较快。\n以上示例，对查询做了优化\n以上对三元嵌套查询做了优化\n以上对函数查询做了优化\n提升查询性能总结\n选择合适的表引擎， mergeTree是ch常见的引擎，但不意味着mergeTree适合所有的场景。\n\n建表时不要使用Nullable，因为官方已经指出了，且这种类型几乎总是拖累性能，所以建议使用一个特殊字符来表空，比如-1， 字符串null等等\n\n合适的划分分区和索引，分区是必须的，官方建议按天分区，建议自己分区的话单分区不超过100w行数据。\n\n数据变更不要太频繁，避免产生过多的临时分区，加大合并的负担，如果要修改建议少次数，大批量的更新修改。官方建议1秒左右发起一起写入操作，单词操作写入数据量保持2w-5w ，具体根据服务器性能定\n\n使用prewhere代替where，因为where是获取整行然后再过滤，而prewhere直接过滤列\n\n指定列和分区，避免select *， 减少内存消耗\n\n避免构建虚拟列，减少内存消耗比如：select income,age income&#x2F;age as incrate from xxx正例：避免虚拟列，直接写select income,age from xxx\n\n用in代替joinjoin会加载到内存，再计算消耗内存\n\n如果非要join，尽量把大表写在前面，小表写后面，这跟mysql 建议小表驱动大表相反。\n\n\n生产常见问题\n数据一致性问题\n\n\nch强调最终一致性，事务控制天生比较弱，主要源自于合并的机制，新分区合并到旧分区\n\n分布式表尽量避免使用，要用就用副本表， 分布式表网络重试问题丢消息，副本同步机制，再次启动还会同步，更稳定可靠\n\n数据合并时间不确定，不一致问题要注意，  可以定期夜间optimize final， 如果非要一致性，就用_sign _version 来标记自己用乐观锁机制保持一致性（sign标记是否删除，version标记新增）\n\n\n\n多副本表，尽量固定写入节点。因为ch 副本是同权的，  如果随机写，一个挂了依然写成功了，但是数据不对齐了，固定写入一个节点，这样挂了立马可以知道\n\nzookeeper数据丢失导致副本无法启动。\n\n\n","categories":["总结笔记"],"tags":["clickhouse","OLAP","OLTP","列式存储","数据仓库"]},{"title":"兰亭集序","url":"/2018_08_11_lanting/","content":"原文【作者】王羲之 【朝代】魏晋\n永和九年，岁在癸丑，暮春之初，会于会稽山阴之兰亭，修禊事也。群贤毕至，少长咸集。此地有崇山峻岭，茂林修竹，又有清流激湍，映带左右，引以为流觞曲水，列坐其次。虽无丝竹管弦之盛，一觞一咏，亦足以畅叙幽情。\n是日也，天朗气清，惠风和畅。仰观宇宙之大，俯察品类之盛，所以游目骋怀，足以极视听之娱，信可乐也。\n夫人之相与，俯仰一世。或取诸怀抱，悟言一室之内；或因寄所托，放浪形骸之外。虽趣舍万殊，静躁不同，当其欣于所遇，暂得于己，快然自足，不知老之将至；及其所之既倦，情随事迁，感慨系之矣。向之所欣，俯仰之间，已为陈迹，犹不能不以之兴怀，况修短随化，终期于尽！古人云：“死生亦大矣。”岂不痛哉！\n每览昔人兴感之由，若合一契，未尝不临文嗟悼，不能喻之于怀。固知一死生为虚诞，齐彭殇为妄作。后之视今，亦犹今之视昔，悲夫！故列叙时人，录其所述，虽世殊事异，所以兴怀，其致一也。后之览者，亦将有感于斯文。\n\n在讲一个什么事情3月河边聚会，边聊边吃饭喝酒，边做活动，快乐的很，然后乐极生悲啦！\n人与人相互交往，很快便度过一生。\n有的人从自己的情趣思想中体悟出一些东西，在室内跟朋友面对面地交谈快乐。\n有的人通过寄情于自己所爱的形体之外的事物,在外面放浪快乐。\n自己体悟也快乐，醉心于外部事物，一内一外的快乐，有万种不同，心中暂时有快乐，就会觉得满足，不知道已经年近将老。（一内一外的快乐）\n等到你所追求的疲倦了，你的感情也随着这些事情改变了，你就会产生某种感慨。\n之前感到欣喜的事，很短时间内，就变成了痕迹。 看到这些痕迹不由自主的感慨着，心绪波动着。\n何况人的生命长短随着造化不同，最终都要走向死亡。\n古人说死亡生存都是大事，不让人哀痛么！\n每次看从前人兴发感想的原因，好像跟我一样，总难免要在读前人文章时叹息哀伤，不能明白于心。（人会有情感波动，原因都是一样）\n庄子把死亡生存看作一样，把长寿和短寿看作一样，我王羲之认为这是非常疯狂荒诞狂妄的想法，人是做不到的。（因为人是情感的动物，时间和情感，死亡活着和情感）\n后人看我就和我看古人一样，难过呀！\n所以我要列出和我一起的人，记录他们叙述的事。虽然事情不一样，但是高兴和难过的情感是一样的。\n后边人读到这儿，也会有感受吧！\n想强调什么人是感情动物，事情时代时空不一样，感情却是一样的。 高兴，悲伤，亲情，友情，爱情…\n","categories":["哲学思考"],"tags":["兰亭集序","感情"]},{"title":"陶渊明汇总","url":"/2018_09_01_taoyuanm/","content":"陶渊明（约365年—427年），字元亮，（又一说名潜，字渊明）号五柳先生，私谥“靖节”，东晋末期南朝宋初期诗人、文学家、辞赋家、散文家。汉族，东晋浔阳柴桑人（今江西九江）。曾做过几年小官，后因厌烦官场辞官回家，从此隐居，田园生活是陶渊明诗的主要题材。\n归去来兮辞·并序原文余家贫，耕植不足以自给。幼稚盈室，瓶无储粟，生生所资，未见其术。亲故多劝余为长吏，脱然有怀，求之靡途。会有四方之事，诸侯以惠爱为德，家叔以余贫苦，遂见用于小邑。于时风波未静，心惮远役，彭泽去家百里，公田之利，足以为酒。故便求之。及少日，眷然有归欤之情。何则？质性自然，非矫厉所得。饥冻虽切，违己交病。尝从人事，皆口腹自役。于是怅然慷慨，深愧平生之志。犹望一稔，当敛裳宵逝。寻程氏妹丧于武昌，情在骏奔，自免去职。仲秋至冬，在官八十余日。因事顺心，命篇曰《归去来兮》。乙巳岁十一月也。\n归去来兮，田园将芜胡不归？既自以心为形役，奚惆怅而独悲？悟已往之不谏，知来者之可追。实迷途其未远，觉今是而昨非。舟遥遥以轻飏，风飘飘而吹衣。问征夫以前路，恨晨光之熹微。\n乃瞻衡宇，载欣载奔。僮仆欢迎，稚子候门。三径就荒，松菊犹存。携幼入室，有酒盈樽。引壶觞以自酌，眄庭柯以怡颜。倚南窗以寄傲，审容膝之易安。园日涉以成趣，门虽设而常关。策扶老以流憩，时矫首而遐观。云无心以出岫，鸟倦飞而知还。景翳翳以将入，抚孤松而盘桓。\n归去来兮，请息交以绝游。世与我而相违，复驾言兮焉求？悦亲戚之情话，乐琴书以消忧。农人告余以春及，将有事于西畴。或命巾车，或棹孤舟。既窈窕以寻壑，亦崎岖而经丘。木欣欣以向荣，泉涓涓而始流。善万物之得时，感吾生之行休。\n已矣乎！寓形宇内复几时？曷不委心任去留？胡为乎遑遑欲何之？富贵非吾愿，帝乡不可期。怀良辰以孤往，或植杖而耘耔。登东皋以舒啸，临清流而赋诗。聊乘化以归尽，乐夫天命复奚疑！\n讲的什么事情不想当官的陶渊明辞官回家，以及回家后的所思所想O(∩_∩)O陶渊明的内耗，做自己想做的事，还是内耗，可见做自己想做的事不能避免内耗(●ˇ∀ˇ●)。\n家里穷的揭不开锅，亲戚劝我当官（当官还要劝，有点小背景），我不情愿的当了官，但是我喜欢田园， 刚好嫁到程家的妹妹在武昌去世了，一方面确实很思念这个妹妹，另一方面乘着去妹妹追悼会这个由头，在做了80多天官的时候，我辞官回家了，写了这个小日记，名字就叫回去吧！《归去来兮辞》\n田地荒芜了为啥不回去？内心被形体裹挟役使，还要内耗思绪焦灼？过去的就算了，未来还能补救，还好错的不是很远，现在知道现在是对的，之前是错的…\n终于回来了，心中开心故奔跑过去。家人来迎接太开心了。回来后每天到院子里走走，开心啊！拄着拐出去走走也开心，抬头看天，看云，看鸟，看落日， 不知不觉由开心转为彷徨。\n终于回来了，我要和世俗绝交，出去有啥好的，在家和亲戚胡侃聊天开心，弹弹琴看看书就不觉忧愁，村里老农说春天来了，我就去西边种地。种完地，我去划船，坐车，蜿蜒曲折，高高低低，青草多好看，水流多好看，一片生机，而我老了，快要结束一生了。\n算了吧，活在世上还能有多久，为啥不能放心下来任其自然生死。为啥心神不定，想要到哪去？我不求荣华富贵，修仙也是没啥希望的，乘着大好春光，顺其自然走完一生，还有啥犹豫疑惑的呢！\n五柳先生传原文先生不知何许人也，亦不详其姓字，宅边有五柳树，因以为号焉。闲静少言，不慕荣利。好读书，不求甚解；每有会意，便欣然忘食。性嗜酒，家贫不能常得。亲旧知其如此，或置酒而招之；造饮辄尽，期在必醉。既醉而退，曾不吝情去留。环堵萧然，不蔽风日；短褐穿结，箪瓢屡空，晏如也。常著文章自娱，颇示己志。忘怀得失，以此自终。\n赞曰：黔娄之妻有言：“不戚戚于贫贱，不汲汲于富贵。”其言兹若人之俦乎？衔觞赋诗，以乐其志，无怀氏之民欤？葛天氏之民欤？\n讲的什么事情据说这是陶渊明的自传，存在争议。看起来就是一个流浪汉，邋邋遢遢，混吃混喝，活那在哪。\n不知道五柳先生是什么地方的人，也不清楚他的姓字。因为住宅旁边有五棵柳树，就把这个作为号了。他安安静静，很少说话，也不羡慕荣华利禄。他喜欢读书，不在一字一句的解释上过分深究；每当对书中的内容有所领会的时候，就会高兴得连饭也忘了吃。他生性喜爱喝酒，家里穷经常没有酒喝。亲戚朋友知道他这种境况，有时摆了酒席叫他去喝。他去喝酒就喝个尽兴，希望一定喝醉；喝醉了就回家，竟然说走就走。简陋的居室里空空荡荡，遮挡不住严寒和烈日，粗布短衣上打满了补丁，盛饭的篮子和饮水的水瓢里经常是空的，可是他还是安然自得。常常写文章来自娱自乐，也稍微透露出他的志趣。他从不把得失放在心上，从此过完自己的一生。赞语说：黔娄的妻子曾经说过：“不为贫贱而忧愁，不热衷于发财做官。这话大概说的是五柳先生这一类的人吧？一边喝酒一边作诗，因为自己抱定的志向而感到无比的快乐。不知道他是无怀氏时代的人呢？还是葛天氏时代的人呢？\n桃花源记原文晋太元中，武陵人捕鱼为业。缘溪行，忘路之远近。忽逢桃花林，夹岸数百步，中无杂树，芳草鲜美，落英缤纷。渔人甚异之，复前行，欲穷其林。\n林尽水源，便得一山，山有小口，仿佛若有光。便舍船，从口入。初极狭，才通人。复行数十步，豁然开朗。土地平旷，屋舍俨然，有良田、美池、桑竹之属。阡陌交通，鸡犬相闻。其中往来种作，男女衣着，悉如外人。黄发垂髫，并怡然自乐。\n见渔人，乃大惊，问所从来。具答之。便要还家，设酒杀鸡作食。村中闻有此人，咸来问讯。自云先世避秦时乱，率妻子邑人来此绝境，不复出焉，遂与外人间隔。问今是何世，乃不知有汉，无论魏晋。此人一一为具言所闻，皆叹惋。余人各复延至其家，皆出酒食。停数日，辞去。此中人语云：“不足为外人道也。”\n既出，得其船，便扶向路，处处志之。及郡下，诣太守，说如此。太守即遣人随其往，寻向所志，遂迷，不复得路。\n南阳刘子骥，高尚士也，闻之，欣然规往。未果，寻病终，后遂无问津者。\n讲的什么事情东晋太元年间，武陵郡有个人以打渔为生。他顺着溪水行船，忘记了路程的远近。忽然遇到一片桃花林，生长在溪水的两岸，长达几百步，中间没有别的树，花草鲜嫩美丽，落花纷纷的散在地上。渔人对此（眼前的景色）感到十分诧异，继续往前行船，想走到林子的尽头。桃林的尽头就是溪水的发源地，于是便出现一座山，山上有个小洞口，洞里仿佛有点光亮。于是他下了船，从洞口进去了。\n起初洞口很狭窄，仅容一人通过。又走了几十步，突然变得开阔明亮了。（呈现在他眼前的是）一片平坦宽广的土地，一排排整齐的房舍。还有肥沃的田地、美丽的池沼，桑树竹林之类的。田间小路交错相通，鸡鸣狗叫到处可以听到。人们在田野里来来往往耕种劳作，男女的穿戴，跟桃花源以外的世人完全一样。老人和小孩们个个都安适愉快，自得其乐。村里的人看到渔人，感到非常惊讶，问他是从哪儿来的。渔人详细地做了回答。村里有人就邀请他到自己家里去（做客），设酒杀鸡做饭来款待他。村里的人听说来了这么一个人，就都来打听消息。他们自己说他们的祖先为了躲避秦时的战乱，领着妻子儿女和乡邻来到这个与人世隔绝的地方，不再出去，因而跟外面的人断绝了来往。他们问渔人现在是什么朝代，他们竟然不知道有过汉朝，更不必说魏晋两朝了。渔人把自己知道的事一一详尽地告诉了他们，听完以后，他们都感叹惋惜。其余的人各自又把渔人请到自己家中，都拿出酒饭来款待他。\n渔人停留了几天，向村里人告辞离开。村里的人对他说：“我们这个地方不值得对外面的人说啊。”\n渔人出来以后，找到了他的船，就顺着旧路回去，处处都做了标记。到了郡城，到太守那里去说，报告了这番经历。太守立即派人跟着他去，寻找以前所做的标记，结果迷失了方向，再也找不到通往桃花源的路了。\n南阳人刘子骥，是个志向高洁的隐士，听到这件事后，高兴地计划前往。但没有实现，不久因病去世了。此后就再也没有问桃花源路的人了。\n","categories":["哲学思考"],"tags":["感情","田园","颓废","内耗"]},{"title":"go-swagger-api应用","url":"/2024_05_17_go_swagger/","content":"该项目包含Swagger 2.0的golang实现 (又名OpenAPI 2.0)。 它提供了与swagger规范一起工作的工具。\nswagger 是RESTful API的简单而强大的实现。\ngithub：https://github.com/go-swagger/go-swagger\n文档：https://goswagger.io/go-swagger/\n聊聊Swagger拥有全球最大的API工具生态系统，几乎所有现代编程语言和部署环境中都支持Swagger。\n使用支持Swagger的API，您可以获得交互式文档、生成客户端SDK和发现服务。\nSwagger帮助Apigee，Getty Images，Intuit，LivingSocial，McKesson，Microsoft，Morningstar和PayPal等公司使用RESTful api构建最佳服务。现在在2.0版本中，Swagger比以往任何时候都更加易用。它是完全开源的软件。\n特点go-swagger为go社区带来了一整套功能齐全、高性能的API组件，可与Swagger API配合使用: 开发服务端、客户端和数据模型。\n\n按照swagger规范生成服务端代码\n按照swagger规范生成客户端代码\n按照swagger规范 (alpha阶段) 生成CLI (命令行工具)\n支持jsonschema和swagger提供的大多数功能，包括多态\n从带注释的go代码生成swagger规范\n使用swagger规范的其他工具\n出色的自定义功能，具有自带的扩展和可自定义的模板\n\n我们对代码生成的理念是：生成惯用的，快速的go代码，它与golint，go vet等兼容得很好。\n安装从源安装按章支持命令，docker，源安装，这里只记录下我用源码安装的操作。\n&gt; go versiongo version go1.23.1 windows/amd64&gt; go install github.com/go-swagger/go-swagger/cmd/swagger@latest...&gt; swagger.exe versionversion: v0.31.0# 查看环境变量&gt; go env# 找到 GOPATHGOPATH=C:\\Users\\hxd\\go# 进入目录，可以看到 swagger.exe 可执行文件，已经安装到gopath了# 后续这个二进制可执行文件可以拷贝到项目中使用，统一版本避免版本问题\nswagger工具说明：\nUsage:  swagger [OPTIONS] &lt;command&gt;Swagger tries to support you as best as possible when building APIs.It aims to represent the contract of your API with a language agnostic description of your application in json or yaml.Application Options:  -q, --quiet                  silence logs      --log-output=LOG-FILE    redirect logs to fileHelp Options:  -h, --help                   Show this help messageAvailable commands:  diff      diff swagger documents  expand    expand $ref fields in a swagger spec  flatten   flattens a swagger document  generate  generate go code  init      initialize a spec document  mixin     merge swagger documents  serve     serve spec and docs  validate  validate the swagger document  version   print the version\n具体怎么用可以参考官网。\n生成服务端参考：https://goswagger.io/go-swagger/generate/server/\n# 生成描述文件swagger.exe init spec# 初始化go modgo mod init mlss-cc-service# git初始化git init\n如图，我新建了一个项目，按照这个目录来构建应用\n把swagger.exe放到项目里，编写好swagger.yaml  执行swagger生成代码\n.\\pkg\\restapi\\swagger\\swagger.exe generate server -m ../models -f .\\pkg\\restapi\\swagger\\swagger.yaml -t .\\pkg\\restapi\n\n新建启动文件，启动：\nD:\\goworkspace\\mlss-cc-service&gt; cd .\\cmd\\mlss-cc-go\\     PS D:\\goworkspace\\mlss-cc-service\\cmd\\mlss-cc-go&gt; ls    目录: D:\\goworkspace\\mlss-cc-service\\cmd\\mlss-cc-goMode                 LastWriteTime         Length Name----                 -------------         ------ -----a----         2024/3/26     16:34           1356 main.goPS D:\\goworkspace\\mlss-cc-service\\cmd\\mlss-cc-go&gt; go run .\\main.goServing open API swagger at http://127.0.0.1:56434\n已经开源到仓库https://gitee.com/deepter/mlss-cc-service\nhttps://gitee.com/deepter/mlss-cc-a\n","categories":["应用笔记"],"tags":["go","swagger"]},{"title":"一条长江，半部文明史","url":"/2025_03_01_the_long_river/","content":"\n长江是世界第三大长河，且长江是中国独有\n\n\n\n\n第一：尼罗河跨越了9个国家第二：亚马逊河跨越了8个国家第三：长江中国独有\n\n\n\n\n我住长江头，君住长江尾。\n\n\n\n\n我在青海，你在上海，我在唐古拉山喝着沱沱河的冰川水，你在上海感叹滚滚长江东逝水\n\n\n\n\n日日思君不见君\n\n\n\n\n是见不着，亚洲第一长河呢，6300多公里，要说双向奔赴的话，要从青海的唐古拉山，跨过青藏高原的横断山脉，来到云贵高原冲到四川盆地江汉平原，最后才到长江中下游平原\n\n\n\n\n\n\n沱沱河的下一站是通天河，是西游记八十一难的最后一难 “通天河遇鼋yuán湿经书” \n\n\n\n\n\n\n通天河周围区域是长江黄河和澜沧江的源头。三江汇合的地方。这个地就是三江源\n\n\n\n\n\n\n\n澜沧江就是湄公河，往下走就有个罕见的奇观，”怒江澜沧江金沙江三江并流在云南”从北往南并着走了170多公里\n\n\n\n\n\n三江并流和长江啥关系？长江在这一段就叫金沙江，金沙江有个拍照特别出片的”云南迪庆雨崩村”云南的玉龙雪山和虎跳峡离的不算远，100公里，但是垂直高度差3.7公里，水流冲击强虎跳峡也称长江第一大峡谷 长江非常凶险的地方之一\n\n任何大的直流和长江交会的地方都会孕育一个非常有特色的城市\n\n\n\n\n\n\n四川攀枝花，中国唯一一座以花命名的城市，长江和第一支流雅砻江交会的地方宜宾， 长江和岷江交会的地方， 宜宾后面的部分才叫真正叫长江，宜宾也叫万里长江第一城。岷江2300年前的超级水利工程都江堰。重庆，嘉陵江交汇点是重庆的朝天门，古城门涪陵，乌江交汇点重庆奉节白帝城，李白的朝辞白帝彩云间。奉节还有个出名的地方，”小寨天坑”世界上最大的天坑，直径622，深度622。 坐进观天很好看，杜甫的无边落木萧萧下，不尽长江滚滚来。三峡大坝，瞿塘峡巫峡西陵峡，十元人名币就是这里取景巴东三峡巫峡长，猿鸣三声泪沾裳。最长的是西陵峡，三峡大坝后面就是屈原故里，秭归穿过三峡大坝就是湖北宜昌，宜昌西边高东边低，武汉南京没高山了。荆州，楚国故都，屈原离骚在这里留下的曾经沧海难为水，除却巫山不是云，巫山是荆州的巫山，不是重庆的巫山。是写夫妻感情的，不是写景的。洞庭湖，中国第二大淡水湖，湖北湖南划分点。刘禹锡 湖光秋月两相和，潭面无风镜未磨，遥望洞庭山水色，白银盘李一青螺。洞庭湖边就是岳阳楼，范仲淹 不以物喜，不以己悲，居庙堂之高则忧其民，处江湖之远则忧其君。江南三大名楼，全在长江上，岳阳楼，黄鹤楼，滕王阁李白在黄鹤楼送孟浩然去扬州，西辞黄鹤楼，烟花三月下扬州庐山九江，李白，飞流直下三千尺，疑是银河落九天。琵琶行，白居易被贬江州，江州就是庐山九江。同是天涯沦落人，相逢何必曾相识。他还在这儿写了 人间四月芳菲尽，山寺桃花始盛开。\n\n\n\n\n\n\n陶渊明就是九江人，采菊东篱下悠然见南山。山气日夕佳，飞鸟相与还。南山就是庐山。\n\n\n\n\n\n\n横看成立侧成峰，远近高低各不同，不识庐山真面目，只缘身在此山中，以上是苏轼的题西林壁,九江和上饶中间有个地方江西万年县仙人洞，这里发现了距今1万年的栽培稻植硅石，世界上最早的栽培稻遗存之一\n\n\n\n\n\n\n鄱阳湖，刘伯温夜观天象帮朱元璋拿下鄱阳湖，完胜陈友谅\n\n\n\n\n\n\n滕王阁，王勃，落霞与孤鹜齐飞，秋水共长天一色\n\n\n\n\n\n\n安徽，长江把它分成了南北皖中皖南。\n\n\n\n\n\n\n南京，六朝古都，金陵。7000年文明史，2600年建城史，500年建都史，永乐大典在这儿写的\n\n\n\n\n\n\n长三角，扬州镇江无锡苏州，然后路过南通，就到魔都上海\n\n\n\n","categories":["地理风情"],"tags":["长江"]},{"title":"平话博弈论","url":"/2024_05_30_game_theory/","content":"一起来看看博弈论是个什么方法，什么理论，解决什么问题。\n\n\n\n目录  · · · · · ·前言第一章 博弈三要素与囚徒困境民营书店的价格大战我怎样被博弈论吸引如此不公平，取胜概率却相等囚徒困境与博弈三要素从囚徒困境说严格优势策略均衡价格大战和双赢对局为什么主要讨论非合作博弈公共品供给的囚徒困境政治家的囚徒困境基数支付和序数支付美苏争霸的囚徒困境第二章 情侣博弈和协调博弈情侣博弈和纳什均衡情侣博弈的其他例子相对优势策略下划线法视觉友好的对角排列情侣博弈表达的对称性嗜好理性人一定自私自利吗？不该一律贬斥自利行为情侣的拥挤博弈默契是协调的一种方式劣势策略消去法的讨论第三章 简单博弈模型的应用智猪博弈和搭便车行为为什么大股东挑起监督经理的重任猎人博弈和帕累托优势斗鸡博弈和航行规则银行挤兑的成因和预防数据不同，结果各异囚徒困境两败俱伤的隐含条件禁鸣喇叭与交通顺畅串通作弊和风险优势营造克己奉公的制度环境“最惠客待遇”对谁有利风险优势的判定说?风险优势的从属地位风险厌恶的统计和理论第四章 混合策略与均衡筛选扑克牌对色游戏混合策略和纳什定理寻找纳什均衡的反应函数法再说混合策略纳什均衡扑克牌讹诈游戏慕尼黑谈判模拟聚点均衡聚点均衡作为共识均衡聚点均衡的制度设置相关均衡商品品牌的“地域连坐”效应品牌地域连坐的博弈分析抗共谋均衡盯着不散伙的共谋德国世界杯警方的优势策略第五章 零和博弈与霍特林模?零和博弈与非零和博弈均衡的观察与验证纳什均衡与杂货铺定位西方两党政治的稳定性和欺骗性动机和实现不是一回事摊贩为什么都往市场门口挤？学校门口等出租车的争先行为多人博弈的霍特林模型对抗性排序经济学家的对称性偏好第六章 动态博弈和子博弈精炼均衡抓钱游戏你死我活，还是你好我好编排故事，加深理解博弈结果依赖制度设置树型博弈策略组合的粗线表示确定树博弈的纳什均衡树型博弈的子博弈子博弈精炼纳什均衡求解动态博弈的倒推法博弈论向自己出难题实验经济学和行为经济学索引\n\n第一章 博弈三要素与囚徒困境博弈三要素:\n","categories":["读书笔记"],"tags":["博弈论"]}]